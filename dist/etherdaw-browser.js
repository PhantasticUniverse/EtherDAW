var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __commonJS = (cb, mod) => function __require() {
  return mod || (0, cb[__getOwnPropNames(cb)[0]])((mod = { exports: {} }).exports, mod), mod.exports;
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));

// node_modules/@babel/runtime/helpers/arrayWithHoles.js
var require_arrayWithHoles = __commonJS({
  "node_modules/@babel/runtime/helpers/arrayWithHoles.js"(exports, module) {
    function _arrayWithHoles(r) {
      if (Array.isArray(r)) return r;
    }
    module.exports = _arrayWithHoles, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/iterableToArrayLimit.js
var require_iterableToArrayLimit = __commonJS({
  "node_modules/@babel/runtime/helpers/iterableToArrayLimit.js"(exports, module) {
    function _iterableToArrayLimit(r, l) {
      var t = null == r ? null : "undefined" != typeof Symbol && r[Symbol.iterator] || r["@@iterator"];
      if (null != t) {
        var e, n, i, u, a = [], f = true, o = false;
        try {
          if (i = (t = t.call(r)).next, 0 === l) {
            if (Object(t) !== t) return;
            f = false;
          } else for (; !(f = (e = i.call(t)).done) && (a.push(e.value), a.length !== l); f = true) ;
        } catch (r2) {
          o = true, n = r2;
        } finally {
          try {
            if (!f && null != t["return"] && (u = t["return"](), Object(u) !== u)) return;
          } finally {
            if (o) throw n;
          }
        }
        return a;
      }
    }
    module.exports = _iterableToArrayLimit, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/arrayLikeToArray.js
var require_arrayLikeToArray = __commonJS({
  "node_modules/@babel/runtime/helpers/arrayLikeToArray.js"(exports, module) {
    function _arrayLikeToArray(r, a) {
      (null == a || a > r.length) && (a = r.length);
      for (var e = 0, n = Array(a); e < a; e++) n[e] = r[e];
      return n;
    }
    module.exports = _arrayLikeToArray, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/unsupportedIterableToArray.js
var require_unsupportedIterableToArray = __commonJS({
  "node_modules/@babel/runtime/helpers/unsupportedIterableToArray.js"(exports, module) {
    var arrayLikeToArray = require_arrayLikeToArray();
    function _unsupportedIterableToArray(r, a) {
      if (r) {
        if ("string" == typeof r) return arrayLikeToArray(r, a);
        var t = {}.toString.call(r).slice(8, -1);
        return "Object" === t && r.constructor && (t = r.constructor.name), "Map" === t || "Set" === t ? Array.from(r) : "Arguments" === t || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t) ? arrayLikeToArray(r, a) : void 0;
      }
    }
    module.exports = _unsupportedIterableToArray, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/nonIterableRest.js
var require_nonIterableRest = __commonJS({
  "node_modules/@babel/runtime/helpers/nonIterableRest.js"(exports, module) {
    function _nonIterableRest() {
      throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.");
    }
    module.exports = _nonIterableRest, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/slicedToArray.js
var require_slicedToArray = __commonJS({
  "node_modules/@babel/runtime/helpers/slicedToArray.js"(exports, module) {
    var arrayWithHoles = require_arrayWithHoles();
    var iterableToArrayLimit = require_iterableToArrayLimit();
    var unsupportedIterableToArray = require_unsupportedIterableToArray();
    var nonIterableRest = require_nonIterableRest();
    function _slicedToArray(r, e) {
      return arrayWithHoles(r) || iterableToArrayLimit(r, e) || unsupportedIterableToArray(r, e) || nonIterableRest();
    }
    module.exports = _slicedToArray, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/classCallCheck.js
var require_classCallCheck = __commonJS({
  "node_modules/@babel/runtime/helpers/classCallCheck.js"(exports, module) {
    function _classCallCheck(a, n) {
      if (!(a instanceof n)) throw new TypeError("Cannot call a class as a function");
    }
    module.exports = _classCallCheck, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/typeof.js
var require_typeof = __commonJS({
  "node_modules/@babel/runtime/helpers/typeof.js"(exports, module) {
    function _typeof(o) {
      "@babel/helpers - typeof";
      return module.exports = _typeof = "function" == typeof Symbol && "symbol" == typeof Symbol.iterator ? function(o2) {
        return typeof o2;
      } : function(o2) {
        return o2 && "function" == typeof Symbol && o2.constructor === Symbol && o2 !== Symbol.prototype ? "symbol" : typeof o2;
      }, module.exports.__esModule = true, module.exports["default"] = module.exports, _typeof(o);
    }
    module.exports = _typeof, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/toPrimitive.js
var require_toPrimitive = __commonJS({
  "node_modules/@babel/runtime/helpers/toPrimitive.js"(exports, module) {
    var _typeof = require_typeof()["default"];
    function toPrimitive(t, r) {
      if ("object" != _typeof(t) || !t) return t;
      var e = t[Symbol.toPrimitive];
      if (void 0 !== e) {
        var i = e.call(t, r || "default");
        if ("object" != _typeof(i)) return i;
        throw new TypeError("@@toPrimitive must return a primitive value.");
      }
      return ("string" === r ? String : Number)(t);
    }
    module.exports = toPrimitive, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/toPropertyKey.js
var require_toPropertyKey = __commonJS({
  "node_modules/@babel/runtime/helpers/toPropertyKey.js"(exports, module) {
    var _typeof = require_typeof()["default"];
    var toPrimitive = require_toPrimitive();
    function toPropertyKey(t) {
      var i = toPrimitive(t, "string");
      return "symbol" == _typeof(i) ? i : i + "";
    }
    module.exports = toPropertyKey, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/@babel/runtime/helpers/createClass.js
var require_createClass = __commonJS({
  "node_modules/@babel/runtime/helpers/createClass.js"(exports, module) {
    var toPropertyKey = require_toPropertyKey();
    function _defineProperties(e, r) {
      for (var t = 0; t < r.length; t++) {
        var o = r[t];
        o.enumerable = o.enumerable || false, o.configurable = true, "value" in o && (o.writable = true), Object.defineProperty(e, toPropertyKey(o.key), o);
      }
    }
    function _createClass(e, r, t) {
      return r && _defineProperties(e.prototype, r), t && _defineProperties(e, t), Object.defineProperty(e, "prototype", {
        writable: false
      }), e;
    }
    module.exports = _createClass, module.exports.__esModule = true, module.exports["default"] = module.exports;
  }
});

// node_modules/automation-events/build/es5/bundle.js
var require_bundle = __commonJS({
  "node_modules/automation-events/build/es5/bundle.js"(exports, module) {
    (function(global, factory) {
      typeof exports === "object" && typeof module !== "undefined" ? factory(exports, require_slicedToArray(), require_classCallCheck(), require_createClass()) : typeof define === "function" && define.amd ? define(["exports", "@babel/runtime/helpers/slicedToArray", "@babel/runtime/helpers/classCallCheck", "@babel/runtime/helpers/createClass"], factory) : (global = typeof globalThis !== "undefined" ? globalThis : global || self, factory(global.automationEvents = {}, global._slicedToArray, global._classCallCheck, global._createClass));
    })(exports, (function(exports2, _slicedToArray, _classCallCheck, _createClass) {
      "use strict";
      var createExtendedExponentialRampToValueAutomationEvent = function createExtendedExponentialRampToValueAutomationEvent2(value, endTime, insertTime) {
        return {
          endTime,
          insertTime,
          type: "exponentialRampToValue",
          value
        };
      };
      var createExtendedLinearRampToValueAutomationEvent = function createExtendedLinearRampToValueAutomationEvent2(value, endTime, insertTime) {
        return {
          endTime,
          insertTime,
          type: "linearRampToValue",
          value
        };
      };
      var createSetValueAutomationEvent2 = function createSetValueAutomationEvent3(value, startTime) {
        return {
          startTime,
          type: "setValue",
          value
        };
      };
      var createSetValueCurveAutomationEvent2 = function createSetValueCurveAutomationEvent3(values, startTime, duration) {
        return {
          duration,
          startTime,
          type: "setValueCurve",
          values
        };
      };
      var getTargetValueAtTime = function getTargetValueAtTime2(time, valueAtStartTime, _ref) {
        var startTime = _ref.startTime, target = _ref.target, timeConstant = _ref.timeConstant;
        return target + (valueAtStartTime - target) * Math.exp((startTime - time) / timeConstant);
      };
      var isExponentialRampToValueAutomationEvent = function isExponentialRampToValueAutomationEvent2(automationEvent) {
        return automationEvent.type === "exponentialRampToValue";
      };
      var isLinearRampToValueAutomationEvent = function isLinearRampToValueAutomationEvent2(automationEvent) {
        return automationEvent.type === "linearRampToValue";
      };
      var isAnyRampToValueAutomationEvent = function isAnyRampToValueAutomationEvent2(automationEvent) {
        return isExponentialRampToValueAutomationEvent(automationEvent) || isLinearRampToValueAutomationEvent(automationEvent);
      };
      var isSetValueAutomationEvent = function isSetValueAutomationEvent2(automationEvent) {
        return automationEvent.type === "setValue";
      };
      var isSetValueCurveAutomationEvent = function isSetValueCurveAutomationEvent2(automationEvent) {
        return automationEvent.type === "setValueCurve";
      };
      var _getValueOfAutomationEventAtIndexAtTime = function getValueOfAutomationEventAtIndexAtTime(automationEvents, index, time, defaultValue) {
        var automationEvent = automationEvents[index];
        return automationEvent === void 0 ? defaultValue : isAnyRampToValueAutomationEvent(automationEvent) || isSetValueAutomationEvent(automationEvent) ? automationEvent.value : isSetValueCurveAutomationEvent(automationEvent) ? automationEvent.values[automationEvent.values.length - 1] : getTargetValueAtTime(time, _getValueOfAutomationEventAtIndexAtTime(automationEvents, index - 1, automationEvent.startTime, defaultValue), automationEvent);
      };
      var getEndTimeAndValueOfPreviousAutomationEvent = function getEndTimeAndValueOfPreviousAutomationEvent2(automationEvents, index, currentAutomationEvent, nextAutomationEvent, defaultValue) {
        return currentAutomationEvent === void 0 ? [nextAutomationEvent.insertTime, defaultValue] : isAnyRampToValueAutomationEvent(currentAutomationEvent) ? [currentAutomationEvent.endTime, currentAutomationEvent.value] : isSetValueAutomationEvent(currentAutomationEvent) ? [currentAutomationEvent.startTime, currentAutomationEvent.value] : isSetValueCurveAutomationEvent(currentAutomationEvent) ? [currentAutomationEvent.startTime + currentAutomationEvent.duration, currentAutomationEvent.values[currentAutomationEvent.values.length - 1]] : [currentAutomationEvent.startTime, _getValueOfAutomationEventAtIndexAtTime(automationEvents, index - 1, currentAutomationEvent.startTime, defaultValue)];
      };
      var isCancelAndHoldAutomationEvent = function isCancelAndHoldAutomationEvent2(automationEvent) {
        return automationEvent.type === "cancelAndHold";
      };
      var isCancelScheduledValuesAutomationEvent = function isCancelScheduledValuesAutomationEvent2(automationEvent) {
        return automationEvent.type === "cancelScheduledValues";
      };
      var getEventTime = function getEventTime2(automationEvent) {
        if (isCancelAndHoldAutomationEvent(automationEvent) || isCancelScheduledValuesAutomationEvent(automationEvent)) {
          return automationEvent.cancelTime;
        }
        if (isExponentialRampToValueAutomationEvent(automationEvent) || isLinearRampToValueAutomationEvent(automationEvent)) {
          return automationEvent.endTime;
        }
        return automationEvent.startTime;
      };
      var getExponentialRampValueAtTime = function getExponentialRampValueAtTime2(time, startTime, valueAtStartTime, _ref) {
        var endTime = _ref.endTime, value = _ref.value;
        if (valueAtStartTime === value) {
          return value;
        }
        if (0 < valueAtStartTime && 0 < value || valueAtStartTime < 0 && value < 0) {
          return valueAtStartTime * Math.pow(value / valueAtStartTime, (time - startTime) / (endTime - startTime));
        }
        return 0;
      };
      var getLinearRampValueAtTime = function getLinearRampValueAtTime2(time, startTime, valueAtStartTime, _ref) {
        var endTime = _ref.endTime, value = _ref.value;
        return valueAtStartTime + (time - startTime) / (endTime - startTime) * (value - valueAtStartTime);
      };
      var interpolateValue = function interpolateValue2(values, theoreticIndex) {
        var lowerIndex = Math.floor(theoreticIndex);
        var upperIndex = Math.ceil(theoreticIndex);
        if (lowerIndex === upperIndex) {
          return values[lowerIndex];
        }
        return (1 - (theoreticIndex - lowerIndex)) * values[lowerIndex] + (1 - (upperIndex - theoreticIndex)) * values[upperIndex];
      };
      var getValueCurveValueAtTime = function getValueCurveValueAtTime2(time, _ref) {
        var duration = _ref.duration, startTime = _ref.startTime, values = _ref.values;
        var theoreticIndex = (time - startTime) / duration * (values.length - 1);
        return interpolateValue(values, theoreticIndex);
      };
      var isSetTargetAutomationEvent = function isSetTargetAutomationEvent2(automationEvent) {
        return automationEvent.type === "setTarget";
      };
      var AutomationEventList2 = /* @__PURE__ */ (function() {
        function AutomationEventList3(defaultValue) {
          _classCallCheck(this, AutomationEventList3);
          this._automationEvents = [];
          this._currenTime = 0;
          this._defaultValue = defaultValue;
        }
        return _createClass(AutomationEventList3, [{
          key: Symbol.iterator,
          value: function value() {
            return this._automationEvents[Symbol.iterator]();
          }
        }, {
          key: "add",
          value: function add(automationEvent) {
            var eventTime = getEventTime(automationEvent);
            if (isCancelAndHoldAutomationEvent(automationEvent) || isCancelScheduledValuesAutomationEvent(automationEvent)) {
              var index = this._automationEvents.findIndex(function(currentAutomationEvent) {
                if (isCancelScheduledValuesAutomationEvent(automationEvent) && isSetValueCurveAutomationEvent(currentAutomationEvent)) {
                  return currentAutomationEvent.startTime + currentAutomationEvent.duration >= eventTime;
                }
                return getEventTime(currentAutomationEvent) >= eventTime;
              });
              var removedAutomationEvent = this._automationEvents[index];
              if (index !== -1) {
                this._automationEvents = this._automationEvents.slice(0, index);
              }
              if (isCancelAndHoldAutomationEvent(automationEvent)) {
                var lastAutomationEvent = this._automationEvents[this._automationEvents.length - 1];
                if (removedAutomationEvent !== void 0 && isAnyRampToValueAutomationEvent(removedAutomationEvent)) {
                  if (lastAutomationEvent !== void 0 && isSetTargetAutomationEvent(lastAutomationEvent)) {
                    throw new Error("The internal list is malformed.");
                  }
                  var startTime = lastAutomationEvent === void 0 ? removedAutomationEvent.insertTime : isSetValueCurveAutomationEvent(lastAutomationEvent) ? lastAutomationEvent.startTime + lastAutomationEvent.duration : getEventTime(lastAutomationEvent);
                  var startValue = lastAutomationEvent === void 0 ? this._defaultValue : isSetValueCurveAutomationEvent(lastAutomationEvent) ? lastAutomationEvent.values[lastAutomationEvent.values.length - 1] : lastAutomationEvent.value;
                  var value = isExponentialRampToValueAutomationEvent(removedAutomationEvent) ? getExponentialRampValueAtTime(eventTime, startTime, startValue, removedAutomationEvent) : getLinearRampValueAtTime(eventTime, startTime, startValue, removedAutomationEvent);
                  var truncatedAutomationEvent = isExponentialRampToValueAutomationEvent(removedAutomationEvent) ? createExtendedExponentialRampToValueAutomationEvent(value, eventTime, this._currenTime) : createExtendedLinearRampToValueAutomationEvent(value, eventTime, this._currenTime);
                  this._automationEvents.push(truncatedAutomationEvent);
                }
                if (lastAutomationEvent !== void 0 && isSetTargetAutomationEvent(lastAutomationEvent)) {
                  this._automationEvents.push(createSetValueAutomationEvent2(this.getValue(eventTime), eventTime));
                }
                if (lastAutomationEvent !== void 0 && isSetValueCurveAutomationEvent(lastAutomationEvent) && lastAutomationEvent.startTime + lastAutomationEvent.duration > eventTime) {
                  var duration = eventTime - lastAutomationEvent.startTime;
                  var ratio = (lastAutomationEvent.values.length - 1) / lastAutomationEvent.duration;
                  var length = Math.max(2, 1 + Math.ceil(duration * ratio));
                  var fraction = duration / (length - 1) * ratio;
                  var values = lastAutomationEvent.values.slice(0, length);
                  if (fraction < 1) {
                    for (var i = 1; i < length; i += 1) {
                      var factor = fraction * i % 1;
                      values[i] = lastAutomationEvent.values[i - 1] * (1 - factor) + lastAutomationEvent.values[i] * factor;
                    }
                  }
                  this._automationEvents[this._automationEvents.length - 1] = createSetValueCurveAutomationEvent2(values, lastAutomationEvent.startTime, duration);
                }
              }
            } else {
              var _index = this._automationEvents.findIndex(function(currentAutomationEvent) {
                return getEventTime(currentAutomationEvent) > eventTime;
              });
              var previousAutomationEvent = _index === -1 ? this._automationEvents[this._automationEvents.length - 1] : this._automationEvents[_index - 1];
              if (previousAutomationEvent !== void 0 && isSetValueCurveAutomationEvent(previousAutomationEvent) && getEventTime(previousAutomationEvent) + previousAutomationEvent.duration > eventTime) {
                return false;
              }
              var persistentAutomationEvent = isExponentialRampToValueAutomationEvent(automationEvent) ? createExtendedExponentialRampToValueAutomationEvent(automationEvent.value, automationEvent.endTime, this._currenTime) : isLinearRampToValueAutomationEvent(automationEvent) ? createExtendedLinearRampToValueAutomationEvent(automationEvent.value, eventTime, this._currenTime) : automationEvent;
              if (_index === -1) {
                this._automationEvents.push(persistentAutomationEvent);
              } else {
                if (isSetValueCurveAutomationEvent(automationEvent) && eventTime + automationEvent.duration > getEventTime(this._automationEvents[_index])) {
                  return false;
                }
                this._automationEvents.splice(_index, 0, persistentAutomationEvent);
              }
            }
            return true;
          }
        }, {
          key: "flush",
          value: function flush(time) {
            var index = this._automationEvents.findIndex(function(currentAutomationEvent) {
              return getEventTime(currentAutomationEvent) > time;
            });
            if (index > 1) {
              var remainingAutomationEvents = this._automationEvents.slice(index - 1);
              var firstRemainingAutomationEvent = remainingAutomationEvents[0];
              if (isSetTargetAutomationEvent(firstRemainingAutomationEvent)) {
                remainingAutomationEvents.unshift(createSetValueAutomationEvent2(_getValueOfAutomationEventAtIndexAtTime(this._automationEvents, index - 2, firstRemainingAutomationEvent.startTime, this._defaultValue), firstRemainingAutomationEvent.startTime));
              }
              this._automationEvents = remainingAutomationEvents;
            }
          }
        }, {
          key: "getValue",
          value: function getValue(time) {
            if (this._automationEvents.length === 0) {
              return this._defaultValue;
            }
            var indexOfNextEvent = this._automationEvents.findIndex(function(automationEvent) {
              return getEventTime(automationEvent) > time;
            });
            var nextAutomationEvent = this._automationEvents[indexOfNextEvent];
            var indexOfCurrentEvent = (indexOfNextEvent === -1 ? this._automationEvents.length : indexOfNextEvent) - 1;
            var currentAutomationEvent = this._automationEvents[indexOfCurrentEvent];
            if (currentAutomationEvent !== void 0 && isSetTargetAutomationEvent(currentAutomationEvent) && (nextAutomationEvent === void 0 || !isAnyRampToValueAutomationEvent(nextAutomationEvent) || nextAutomationEvent.insertTime > time)) {
              return getTargetValueAtTime(time, _getValueOfAutomationEventAtIndexAtTime(this._automationEvents, indexOfCurrentEvent - 1, currentAutomationEvent.startTime, this._defaultValue), currentAutomationEvent);
            }
            if (currentAutomationEvent !== void 0 && isSetValueAutomationEvent(currentAutomationEvent) && (nextAutomationEvent === void 0 || !isAnyRampToValueAutomationEvent(nextAutomationEvent))) {
              return currentAutomationEvent.value;
            }
            if (currentAutomationEvent !== void 0 && isSetValueCurveAutomationEvent(currentAutomationEvent) && (nextAutomationEvent === void 0 || !isAnyRampToValueAutomationEvent(nextAutomationEvent) || currentAutomationEvent.startTime + currentAutomationEvent.duration > time)) {
              if (time < currentAutomationEvent.startTime + currentAutomationEvent.duration) {
                return getValueCurveValueAtTime(time, currentAutomationEvent);
              }
              return currentAutomationEvent.values[currentAutomationEvent.values.length - 1];
            }
            if (currentAutomationEvent !== void 0 && isAnyRampToValueAutomationEvent(currentAutomationEvent) && (nextAutomationEvent === void 0 || !isAnyRampToValueAutomationEvent(nextAutomationEvent))) {
              return currentAutomationEvent.value;
            }
            if (nextAutomationEvent !== void 0 && isExponentialRampToValueAutomationEvent(nextAutomationEvent)) {
              var _getEndTimeAndValueOf = getEndTimeAndValueOfPreviousAutomationEvent(this._automationEvents, indexOfCurrentEvent, currentAutomationEvent, nextAutomationEvent, this._defaultValue), _getEndTimeAndValueOf2 = _slicedToArray(_getEndTimeAndValueOf, 2), startTime = _getEndTimeAndValueOf2[0], value = _getEndTimeAndValueOf2[1];
              return getExponentialRampValueAtTime(time, startTime, value, nextAutomationEvent);
            }
            if (nextAutomationEvent !== void 0 && isLinearRampToValueAutomationEvent(nextAutomationEvent)) {
              var _getEndTimeAndValueOf3 = getEndTimeAndValueOfPreviousAutomationEvent(this._automationEvents, indexOfCurrentEvent, currentAutomationEvent, nextAutomationEvent, this._defaultValue), _getEndTimeAndValueOf4 = _slicedToArray(_getEndTimeAndValueOf3, 2), _startTime = _getEndTimeAndValueOf4[0], _value = _getEndTimeAndValueOf4[1];
              return getLinearRampValueAtTime(time, _startTime, _value, nextAutomationEvent);
            }
            return this._defaultValue;
          }
        }]);
      })();
      var createCancelAndHoldAutomationEvent2 = function createCancelAndHoldAutomationEvent3(cancelTime) {
        return {
          cancelTime,
          type: "cancelAndHold"
        };
      };
      var createCancelScheduledValuesAutomationEvent2 = function createCancelScheduledValuesAutomationEvent3(cancelTime) {
        return {
          cancelTime,
          type: "cancelScheduledValues"
        };
      };
      var createExponentialRampToValueAutomationEvent2 = function createExponentialRampToValueAutomationEvent3(value, endTime) {
        return {
          endTime,
          type: "exponentialRampToValue",
          value
        };
      };
      var createLinearRampToValueAutomationEvent2 = function createLinearRampToValueAutomationEvent3(value, endTime) {
        return {
          endTime,
          type: "linearRampToValue",
          value
        };
      };
      var createSetTargetAutomationEvent2 = function createSetTargetAutomationEvent3(target, startTime, timeConstant) {
        return {
          startTime,
          target,
          timeConstant,
          type: "setTarget"
        };
      };
      exports2.AutomationEventList = AutomationEventList2;
      exports2.createCancelAndHoldAutomationEvent = createCancelAndHoldAutomationEvent2;
      exports2.createCancelScheduledValuesAutomationEvent = createCancelScheduledValuesAutomationEvent2;
      exports2.createExponentialRampToValueAutomationEvent = createExponentialRampToValueAutomationEvent2;
      exports2.createLinearRampToValueAutomationEvent = createLinearRampToValueAutomationEvent2;
      exports2.createSetTargetAutomationEvent = createSetTargetAutomationEvent2;
      exports2.createSetValueAutomationEvent = createSetValueAutomationEvent2;
      exports2.createSetValueCurveAutomationEvent = createSetValueCurveAutomationEvent2;
    }));
  }
});

// src/config/constants.ts
var DURATIONS = {
  w: 4,
  // whole
  h: 2,
  // half
  q: 1,
  // quarter
  "8": 0.5,
  // eighth
  "16": 0.25,
  // sixteenth
  "32": 0.125,
  // thirty-second
  // Alternative notations
  "2": 2,
  // alternative half notation
  "4": 1
  // alternative quarter notation
};
var DOTTED_MULTIPLIER = 1.5;
var MIDI = {
  MIDDLE_C: 60,
  // C4 = MIDI note 60
  SEMITONES_PER_OCTAVE: 12,
  DEFAULT_VELOCITY: 100,
  // 0-127 scale
  MAX_VELOCITY: 127,
  VELOCITY_SCALE: 127
  // Internal 0-1 to MIDI 0-127
};
var AUDIO = {
  SAMPLE_RATE: 44100,
  BIT_DEPTH: 16,
  CHANNELS: 2
  // Stereo
};
var VELOCITY_ENVELOPE = {
  MIN_VELOCITY: 0.3,
  // Floor for diminuendo
  MAX_VELOCITY: 1,
  // Ceiling for crescendo
  SWELL_PEAK: 1.2,
  // Overshoot for swell (clamped to 1.0)
  ACCENT_BOOST: 0.2,
  // Added to accented notes
  DEFAULT_VELOCITY: 0.8
  // Default track velocity
};
var ARTICULATION = {
  staccato: { gate: 0.3, velocityBoost: 0 },
  // * short note
  legato: { gate: 1.1, velocityBoost: 0 },
  // ~ slightly longer
  accent: { gate: 1, velocityBoost: 0.2 },
  // > louder
  marcato: { gate: 0.3, velocityBoost: 0.2 },
  // ^ accent + staccato
  normal: { gate: 1, velocityBoost: 0 }
  // default
};
var DYNAMICS = {
  ppp: 0.1,
  // pianississimo - extremely soft
  pp: 0.2,
  // pianissimo - very soft
  p: 0.35,
  // piano - soft
  mp: 0.5,
  // mezzo-piano - medium soft
  mf: 0.65,
  // mezzo-forte - medium loud
  f: 0.8,
  // forte - loud
  ff: 0.95,
  // fortissimo - very loud
  fff: 1
  // fortississimo - extremely loud
};
var EFFECT_DEFAULTS = {
  reverb: {
    decay: 2,
    wet: 0.3,
    preDelay: 0.01
  },
  delay: {
    time: "8n",
    feedback: 0.3,
    wet: 0.25
  },
  filter: {
    frequency: 1e3,
    type: "lowpass",
    Q: 1
  },
  chorus: {
    frequency: 1.5,
    delayTime: 3.5,
    depth: 0.7,
    wet: 0.3
  },
  distortion: {
    distortion: 0.4,
    wet: 0.5
  },
  bitcrusher: {
    bits: 8,
    wet: 0.5
  },
  phaser: {
    frequency: 0.5,
    octaves: 3,
    baseFrequency: 350,
    wet: 0.5
  },
  vibrato: {
    frequency: 5,
    depth: 0.1,
    wet: 0.5
  },
  compressor: {
    threshold: -24,
    ratio: 4,
    attack: 3e-3,
    release: 0.25
  },
  eq: {
    low: 0,
    mid: 0,
    high: 0
  }
};
var ENVELOPE_PRESETS = {
  piano: { attack: 5e-3, decay: 0.2, sustain: 0.3, release: 0.8 },
  organ: { attack: 0.01, decay: 0.1, sustain: 0.9, release: 0.1 },
  pad: { attack: 0.5, decay: 0.3, sustain: 0.8, release: 1.5 },
  pluck: { attack: 1e-3, decay: 0.3, sustain: 0.1, release: 0.5 },
  bass: { attack: 0.01, decay: 0.2, sustain: 0.7, release: 0.3 },
  lead: { attack: 0.02, decay: 0.1, sustain: 0.8, release: 0.4 },
  bell: { attack: 1e-3, decay: 2, sustain: 0, release: 2 },
  strings: { attack: 0.3, decay: 0.2, sustain: 0.8, release: 0.5 }
};
var DEFAULT_SETTINGS = {
  tempo: 120,
  key: "C major",
  timeSignature: "4/4",
  swing: 0
};
var HUMANIZE = {
  MAX_TIMING_VARIANCE_BEATS: 0.05,
  // 5% of beat
  MAX_VELOCITY_VARIANCE: 0.1,
  // +/-10%
  MAX_DURATION_VARIANCE: 0.05
  // +/-5%
};
var PORTAMENTO = {
  DEFAULT_TIME: 0.05,
  // 50ms glide
  MAX_TIME: 0.5
  // 500ms max
};
var ARPEGGIATOR = {
  DEFAULT_GATE: 0.8,
  // Note length ratio
  DEFAULT_OCTAVES: 1
  // Octave span
};
var DRUM_SEQUENCER = {
  DEFAULT_STEP_DURATION: "16",
  // 16th notes
  ACCENT_VELOCITY: 1,
  DEFAULT_VELOCITY: 0.8
};
var NOTE_VALUES = {
  "C": 0,
  "D": 2,
  "E": 4,
  "F": 5,
  "G": 7,
  "A": 9,
  "B": 11
};
var NOTE_NAMES = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"];
var SCALE_INTERVALS = {
  "major": [0, 2, 4, 5, 7, 9, 11],
  "minor": [0, 2, 3, 5, 7, 8, 10],
  "dorian": [0, 2, 3, 5, 7, 9, 10],
  "phrygian": [0, 1, 3, 5, 7, 8, 10],
  "lydian": [0, 2, 4, 6, 7, 9, 11],
  "mixolydian": [0, 2, 4, 5, 7, 9, 10],
  "aeolian": [0, 2, 3, 5, 7, 8, 10],
  "locrian": [0, 1, 3, 5, 6, 8, 10]
};
var GROOVE_TEMPLATES = {
  straight: {
    name: "Straight",
    description: "Quantized, metronomic timing",
    timingOffsets: [0, 0, 0, 0],
    velocityMultipliers: [1, 0.8, 0.9, 0.8]
  },
  shuffle: {
    name: "Shuffle",
    description: "Classic swing feel with delayed offbeats",
    timingOffsets: [0, 0.08, 0, 0.08],
    velocityMultipliers: [1, 0.7, 0.9, 0.7]
  },
  funk: {
    name: "Funk",
    description: "Tight, syncopated funk groove",
    timingOffsets: [0, -0.02, 0.02, -0.01],
    velocityMultipliers: [1, 0.9, 0.85, 0.95]
  },
  laid_back: {
    name: "Laid Back",
    description: "Slightly behind the beat, relaxed feel",
    timingOffsets: [0.03, 0.03, 0.03, 0.03],
    velocityMultipliers: [1, 0.85, 0.9, 0.85]
  },
  pushed: {
    name: "Pushed",
    description: "Slightly ahead of the beat, energetic",
    timingOffsets: [-0.02, -0.02, -0.02, -0.02],
    velocityMultipliers: [1, 0.9, 0.95, 0.9]
  },
  hip_hop: {
    name: "Hip Hop",
    description: "Boom-bap influenced with swing on 2 and 4",
    timingOffsets: [0, 0.05, 0, 0.07],
    velocityMultipliers: [1, 0.75, 0.9, 0.8]
  },
  // NEW v0.8 groove templates
  dilla: {
    name: "Dilla",
    description: "J Dilla-inspired drunk/loose timing",
    timingOffsets: [0, 0.06, -0.02, 0.09],
    velocityMultipliers: [1, 0.7, 0.85, 0.65]
  },
  reggae: {
    name: "Reggae",
    description: "One-drop emphasis with delayed backbeat",
    timingOffsets: [0, 0.04, 0, 0.06],
    velocityMultipliers: [0.7, 0.9, 1, 0.8]
  },
  dnb: {
    name: "Drum and Bass",
    description: "Fast breakbeat with ghost note dynamics",
    timingOffsets: [0, -0.01, 0.01, -0.01],
    velocityMultipliers: [1, 0.6, 0.85, 0.55]
  },
  trap: {
    name: "Trap",
    description: "Modern trap with hi-hat roll dynamics",
    timingOffsets: [0, 0.02, 0, 0.03],
    velocityMultipliers: [1, 0.65, 0.9, 0.6]
  },
  gospel: {
    name: "Gospel",
    description: "Church feel with strong backbeat",
    timingOffsets: [0, 0.04, 0, 0.05],
    velocityMultipliers: [0.9, 1, 0.85, 0.95]
  },
  new_orleans: {
    name: "New Orleans",
    description: "Second line parade feel",
    timingOffsets: [0, 0.07, 0.02, 0.05],
    velocityMultipliers: [1, 0.8, 0.9, 0.85]
  },
  bossa: {
    name: "Bossa Nova",
    description: "Brazilian bossa nova subtle swing",
    timingOffsets: [0, 0.03, 0, 0.04],
    velocityMultipliers: [1, 0.75, 0.85, 0.8]
  },
  afrobeat: {
    name: "Afrobeat",
    description: "Fela-inspired African polyrhythm feel",
    timingOffsets: [0, 0.02, 0.04, 0.01],
    velocityMultipliers: [1, 0.85, 0.9, 0.8]
  }
};
var EXPRESSION_PRESETS = {
  mechanical: {
    name: "Mechanical",
    description: "Quantized, robotic - no humanization",
    humanize: 0,
    groove: "straight",
    velocityVariance: 0
  },
  tight: {
    name: "Tight",
    description: "Clean, professional studio performance",
    humanize: 0.01,
    groove: "straight",
    velocityVariance: 0.02
  },
  natural: {
    name: "Natural",
    description: "Human but controlled, slight variations",
    humanize: 0.03,
    groove: "straight",
    velocityVariance: 0.05
  },
  romantic: {
    name: "Romantic",
    description: "Expressive, rubato-like, laid back feel",
    humanize: 0.04,
    groove: "laid_back",
    velocityVariance: 0.08
  },
  jazzy: {
    name: "Jazzy",
    description: "Loose, swung, Dilla-inspired groove",
    humanize: 0.03,
    groove: "dilla",
    velocityVariance: 0.1
  },
  funk: {
    name: "Funk",
    description: "Tight pocket with funky timing",
    humanize: 0.02,
    groove: "funk",
    velocityVariance: 0.06
  },
  gospel: {
    name: "Gospel",
    description: "Church feel with strong backbeat",
    humanize: 0.03,
    groove: "gospel",
    velocityVariance: 0.08
  },
  aggressive: {
    name: "Aggressive",
    description: "Forward, driving, slightly ahead of beat",
    humanize: 0.01,
    groove: "pushed",
    velocityVariance: 0.04
  }
};

// src/schema/types.ts
var DURATION_MAP = DURATIONS;

// src/errors/messages.ts
function levenshtein(a, b) {
  const matrix = [];
  for (let i = 0; i <= b.length; i++) {
    matrix[i] = [i];
  }
  for (let j = 0; j <= a.length; j++) {
    matrix[0][j] = j;
  }
  for (let i = 1; i <= b.length; i++) {
    for (let j = 1; j <= a.length; j++) {
      if (b.charAt(i - 1) === a.charAt(j - 1)) {
        matrix[i][j] = matrix[i - 1][j - 1];
      } else {
        matrix[i][j] = Math.min(
          matrix[i - 1][j - 1] + 1,
          matrix[i][j - 1] + 1,
          matrix[i - 1][j] + 1
        );
      }
    }
  }
  return matrix[b.length][a.length];
}
function findSimilar(input, candidates, maxDistance = 3, maxResults = 3) {
  return candidates.map((c) => ({ c, d: levenshtein(input.toLowerCase(), c.toLowerCase()) })).filter(({ d }) => d <= maxDistance).sort((a, b) => a.d - b.d).slice(0, maxResults).map(({ c }) => c);
}
function formatSuggestion(similar) {
  if (similar.length === 0) return void 0;
  if (similar.length === 1) return `Did you mean '${similar[0]}'?`;
  return `Did you mean: ${similar.map((s) => `'${s}'`).join(", ")}?`;
}
var DRUM_NAMES = [
  "kick",
  "bd",
  "bass",
  "snare",
  "sd",
  "hihat",
  "hh",
  "ch",
  "oh",
  "tom1",
  "tom2",
  "tom3",
  "crash",
  "ride",
  "bell",
  "rim",
  "clap",
  "perc",
  "shaker",
  "tambourine",
  "cowbell",
  "conga_hi",
  "conga_lo",
  "bongo_hi",
  "bongo_lo",
  "timbale_hi",
  "timbale_lo",
  "agogo_hi",
  "agogo_lo"
];
var errors = {
  /**
   * Invalid note duration
   */
  invalidDuration: (note, duration) => ({
    code: "E001",
    message: `Invalid note '${note}' - unknown duration '${duration}'`,
    help: `Valid durations: w (whole), h (half), q (quarter), 8 (eighth), 16 (sixteenth), 32 (thirty-second), t (triplet modifier)`,
    docs: "docs/ETHERSCORE_FORMAT.md#durations"
  }),
  /**
   * Invalid note pitch
   */
  invalidPitch: (note, pitch) => ({
    code: "E002",
    message: `Invalid note '${note}' - invalid pitch '${pitch}'`,
    help: `Pitch must be a letter A-G, optional # or b, and octave number (0-9). Examples: C4, F#5, Bb3`,
    docs: "docs/ETHERSCORE_FORMAT.md#notes"
  }),
  /**
   * Invalid note syntax
   */
  invalidNoteSyntax: (note) => ({
    code: "E003",
    message: `Invalid note syntax: '${note}'`,
    help: `Note format: <pitch>:<duration> or just <pitch>. Examples: C4:q (quarter note C4), E5:h (half note E5), r:q (quarter rest)`,
    docs: "docs/ETHERSCORE_FORMAT.md#notes"
  }),
  /**
   * Unknown preset
   */
  unknownPreset: (preset, instrument, availablePresets) => {
    const similar = findSimilar(preset, availablePresets);
    return {
      code: "E004",
      message: `Unknown preset '${preset}' for instrument '${instrument}'`,
      help: similar.length > 0 ? formatSuggestion(similar) : `Run 'npx tsx src/cli.ts list presets' to see available presets`,
      docs: "docs/PRESETS.md"
    };
  },
  /**
   * Invalid chord name
   */
  invalidChord: (chord) => ({
    code: "E005",
    message: `Invalid chord: '${chord}'`,
    help: `Chord format: <root>[quality][:<duration>]. Examples: C (C major), Am7 (A minor 7th), Dm:h (D minor half note). Qualities: m, maj7, m7, 7, dim, aug, sus2, sus4`,
    docs: "docs/ETHERSCORE_FORMAT.md#chords"
  }),
  /**
   * Unknown drum name
   */
  unknownDrum: (drum, kit) => {
    const similar = findSimilar(drum, DRUM_NAMES);
    return {
      code: "E006",
      message: `Unknown drum '${drum}' in kit '${kit}'`,
      help: similar.length > 0 ? formatSuggestion(similar) : `Available drums: kick, snare, hihat, tom1, tom2, crash, ride, rim, clap`,
      docs: "docs/ETHERSCORE_FORMAT.md#drums"
    };
  },
  /**
   * Pattern not found
   */
  patternNotFound: (pattern, availablePatterns) => {
    const similar = findSimilar(pattern, availablePatterns);
    return {
      code: "E007",
      message: `Pattern '${pattern}' not found`,
      help: similar.length > 0 ? formatSuggestion(similar) : `Define pattern '${pattern}' in the patterns object`,
      docs: "docs/ETHERSCORE_FORMAT.md#patterns"
    };
  },
  /**
   * Section not found
   */
  sectionNotFound: (section, availableSections) => {
    const similar = findSimilar(section, availableSections);
    return {
      code: "E008",
      message: `Section '${section}' in arrangement not found`,
      help: similar.length > 0 ? formatSuggestion(similar) : `Define section '${section}' in the sections object`,
      docs: "docs/ETHERSCORE_FORMAT.md#sections"
    };
  },
  /**
   * Missing required field
   */
  missingRequired: (field, context2) => ({
    code: "E009",
    message: `Missing required field '${field}' in ${context2}`,
    help: `Every ${context2.split(" ")[0]} must have a '${field}' property`,
    docs: "docs/ETHERSCORE_FORMAT.md"
  }),
  /**
   * Invalid time signature
   */
  invalidTimeSignature: (timeSignature) => ({
    code: "E010",
    message: `Invalid time signature: '${timeSignature}'`,
    help: `Time signature format: <numerator>/<denominator>. Examples: 4/4, 3/4, 6/8. Denominator must be 2, 4, 8, or 16.`,
    docs: "docs/ETHERSCORE_FORMAT.md#settings"
  }),
  /**
   * Invalid velocity
   */
  invalidVelocity: (velocity) => ({
    code: "E011",
    message: `Invalid velocity: ${velocity}`,
    help: `Velocity must be between 0 and 1. Example: 0.7 for moderate velocity, 0.9 for loud`
  }),
  /**
   * Invalid tempo
   */
  invalidTempo: (tempo) => ({
    code: "E012",
    message: `Invalid tempo: ${tempo} BPM`,
    help: `Tempo should typically be between 20 and 300 BPM`
  }),
  /**
   * Instrument not defined
   */
  instrumentNotDefined: (instrument, section) => ({
    code: "E013",
    message: `Track '${instrument}' in section '${section}' has no matching instrument definition`,
    help: `Add an instrument definition: { "instruments": { "${instrument}": { "preset": "..." } } }`,
    docs: "docs/ETHERSCORE_FORMAT.md#instruments"
  }),
  /**
   * Empty arrangement
   */
  emptyArrangement: () => ({
    code: "E014",
    message: `Arrangement is empty`,
    help: `Add section names to the arrangement array: { "arrangement": ["intro", "verse", "chorus"] }`,
    docs: "docs/ETHERSCORE_FORMAT.md#arrangement"
  }),
  /**
   * Invalid octave
   */
  invalidOctave: (octave) => ({
    code: "E015",
    message: `Invalid octave: ${octave}`,
    help: `Octave should be between -2 and 10. Typical range is 2-6 for most instruments.`
  }),
  /**
   * Invalid humanize value
   */
  invalidHumanize: (humanize) => ({
    code: "E016",
    message: `Invalid humanize value: ${humanize}`,
    help: `Humanize should be between 0 and 0.1. Typical values: 0.01-0.03 for subtle variation, 0.05+ for loose feel.`
  }),
  /**
   * Invalid swing value
   */
  invalidSwing: (swing) => ({
    code: "E017",
    message: `Invalid swing value: ${swing}`,
    help: `Swing should be between 0 and 1. Typical values: 0.1-0.3 for light swing, 0.5-0.7 for heavy swing.`
  }),
  /**
   * Bracket chord syntax error
   */
  invalidBracketChord: (input) => ({
    code: "E018",
    message: `Invalid bracket chord syntax: '${input}'`,
    help: `Bracket chord format: [pitch1,pitch2,...]:<duration>. Example: [C4,E4,G4]:h for a half-note C major triad`,
    docs: "docs/ETHERSCORE_FORMAT.md#bracket-chords"
  }),
  /**
   * Invalid effect type
   */
  invalidEffect: (effect, availableEffects) => {
    const similar = findSimilar(effect, availableEffects);
    return {
      code: "E019",
      message: `Unknown effect type: '${effect}'`,
      help: similar.length > 0 ? formatSuggestion(similar) : `Available effects: reverb, delay, chorus, distortion, filter, compressor, eq`,
      docs: "docs/ETHERSCORE_FORMAT.md#effects"
    };
  },
  /**
   * Invalid drum kit
   */
  invalidDrumKit: (kit, availableKits) => {
    const similar = findSimilar(kit, availableKits);
    return {
      code: "E020",
      message: `Unknown drum kit: '${kit}'`,
      help: similar.length > 0 ? formatSuggestion(similar) : `Available kits: 808, 909, acoustic, world`
    };
  }
};
function formatError(error) {
  const lines = [`${error.code}: ${error.message}`];
  if (error.help) {
    lines.push(`  ${error.help}`);
  }
  if (error.docs) {
    lines.push(`  See: ${error.docs}`);
  }
  return lines.join("\n");
}
function createError(info) {
  const error = new Error(formatError(info));
  error.code = info.code;
  error.help = info.help;
  error.docs = info.docs;
  return error;
}

// src/parser/note-parser.ts
var NOTE_REGEX = /^([A-Ga-g])([#b]?)(-?\d)?:(\d+|[whq])(\.?)(?:t(\d+))?(?:([*>^])|(~>)|(~))?(?:\.(fall|doit|scoop|bend)(?:\+(\d+))?)?(?:\.(tr|mord|turn))?(?:@((?:0|1)?\.?\d+|ppp|pp|p|mp|mf|f|ff|fff))?(?:([+-]\d+)ms)?(?:\?((?:0|1)?\.?\d+))?(~>)?(:ped)?$/;
var REST_REGEX = /^r:(\d+|[whq])(\.?)$/i;
function checkSyntaxOrderHints(noteStr) {
  const velocityBeforeArticulation = /@((?:0|1)?\.?\d+|ppp|pp|p|mp|mf|f|ff|fff)([*~>^])/.exec(noteStr);
  if (velocityBeforeArticulation) {
    const [, velocity, articulation] = velocityBeforeArticulation;
    const articulationName = articulation === "~" ? "legato" : articulation === "*" ? "staccato" : articulation === ">" ? "accent" : articulation === "^" ? "marcato" : "articulation";
    const corrected = noteStr.replace("@" + velocity + articulation, articulation + "@" + velocity);
    throw new Error(
      'Syntax order error in "' + noteStr + '": ' + articulationName + " (" + articulation + ") must come BEFORE velocity (@" + velocity + ").\nTry: " + corrected
    );
  }
  const missingColon = /^([A-Ga-g][#b]?\d?)([whq]|\d+)/.exec(noteStr);
  if (missingColon && !noteStr.includes(":")) {
    const [, pitch, duration] = missingColon;
    throw new Error(
      'Missing colon in "' + noteStr + '": Note format requires a colon between pitch and duration.\nTry: ' + pitch + ":" + duration
    );
  }
  const portamentoAfterVelocity = /@((?:0|1)?\.?\d+|ppp|pp|p|mp|mf|f|ff|fff)(~>)/.exec(noteStr);
  if (portamentoAfterVelocity) {
    const [, velocity] = portamentoAfterVelocity;
    const corrected = noteStr.replace("@" + velocity + "~>", "~>@" + velocity);
    throw new Error(
      'Syntax order error in "' + noteStr + '": Portamento (~>) should come BEFORE velocity (@' + velocity + ").\nTry: " + corrected
    );
  }
}
function parseNote(noteStr) {
  const trimmed = noteStr.trim();
  const match = trimmed.match(NOTE_REGEX);
  if (!match) {
    checkSyntaxOrderHints(trimmed);
    throw createError(errors.invalidNoteSyntax(noteStr));
  }
  const [
    ,
    noteNameRaw,
    // 1: Note name
    accidentalRaw,
    // 2: Accidental
    octaveStr,
    // 3: Octave
    durationCode,
    // 4: Duration
    dotted,
    // 5: Dot
    tupletRaw,
    // 6: Tuplet ratio
    articulationRaw,
    // 7: Articulation (*>^)
    portamentoRaw,
    // 8: Portamento (~>) - before modifiers
    legatoRaw,
    // 9: Legato (~)
    jazzArtRaw,
    // 10: Jazz articulation (fall, doit, scoop, bend)
    bendAmountRaw,
    // 11: Bend amount for .bend+N
    ornamentRaw,
    // 12: Ornament (tr, mord, turn)
    velocityRaw,
    // 13: Velocity
    timingRaw,
    // 14: Timing offset
    probabilityRaw,
    // 15: Probability
    portamentoTrailing,
    // 16: Portamento (~>) - after modifiers (alternate position)
    pedalRaw
    // 17: Sustain pedal (:ped)
  ] = match;
  const noteName = noteNameRaw.toUpperCase();
  const accidental = accidentalRaw || "";
  const octave = octaveStr ? parseInt(octaveStr, 10) : 4;
  const isDotted = dotted === ".";
  const tupletRatio = tupletRaw ? parseInt(tupletRaw, 10) : void 0;
  let articulation = "";
  if (articulationRaw) {
    articulation = articulationRaw;
  } else if (legatoRaw) {
    articulation = "~";
  }
  const portamento = portamentoRaw === "~>" || portamentoTrailing === "~>";
  const jazzArticulation = jazzArtRaw;
  const bendAmount = bendAmountRaw ? parseInt(bendAmountRaw, 10) : void 0;
  const ornament = ornamentRaw;
  let velocity;
  let dynamics;
  if (velocityRaw) {
    if (velocityRaw in DYNAMICS) {
      dynamics = velocityRaw;
      velocity = DYNAMICS[dynamics];
    } else {
      velocity = parseFloat(velocityRaw);
    }
  }
  const timingOffset = timingRaw ? parseInt(timingRaw, 10) : void 0;
  const probability = probabilityRaw ? parseFloat(probabilityRaw) : void 0;
  if (velocity !== void 0 && (velocity < 0 || velocity > 1)) {
    throw createError(errors.invalidVelocity(velocity));
  }
  if (probability !== void 0 && (probability < 0 || probability > 1)) {
    throw new Error(`Invalid probability ${probability} in "${noteStr}". Must be 0.0-1.0`);
  }
  if (tupletRatio !== void 0 && (tupletRatio < 2 || tupletRatio > 9)) {
    throw new Error(`Invalid tuplet ratio ${tupletRatio} in "${noteStr}". Must be 2-9 (e.g., t3 for triplet, t5 for quintuplet)`);
  }
  if (bendAmount !== void 0 && (bendAmount < 1 || bendAmount > 12)) {
    throw new Error(`Invalid bend amount ${bendAmount} in "${noteStr}". Must be 1-12 semitones`);
  }
  const baseDuration = DURATION_MAP[durationCode];
  if (baseDuration === void 0) {
    throw createError(errors.invalidDuration(noteStr, durationCode));
  }
  let durationBeats = isDotted ? baseDuration * DOTTED_MULTIPLIER : baseDuration;
  if (tupletRatio) {
    const tupletBase = tupletRatio % 2 === 0 ? tupletRatio / 2 : Math.floor(tupletRatio / 2) + 1;
    durationBeats = durationBeats * tupletBase / tupletRatio;
  }
  const pitch = `${noteName}${accidental}${octave}`;
  const result = {
    pitch,
    noteName,
    accidental,
    octave,
    duration: durationCode,
    durationBeats,
    dotted: isDotted,
    articulation
  };
  if (velocity !== void 0) result.velocity = velocity;
  if (probability !== void 0) result.probability = probability;
  if (timingOffset !== void 0) result.timingOffset = timingOffset;
  if (portamento) result.portamento = true;
  if (tupletRatio !== void 0) result.tupletRatio = tupletRatio;
  if (jazzArticulation) result.jazzArticulation = jazzArticulation;
  if (bendAmount !== void 0) result.bendAmount = bendAmount;
  if (ornament) result.ornament = ornament;
  if (dynamics) result.dynamics = dynamics;
  if (pedalRaw === ":ped") result.pedal = true;
  return result;
}
function getArticulationModifiers(articulation) {
  switch (articulation) {
    case "*":
      return { ...ARTICULATION.staccato };
    case "~":
      return { ...ARTICULATION.legato };
    case ">":
      return { ...ARTICULATION.accent };
    case "^":
      return { ...ARTICULATION.marcato };
    default:
      return { ...ARTICULATION.normal };
  }
}
function parseRest(restStr) {
  const match = restStr.trim().match(REST_REGEX);
  if (!match) {
    throw new Error(`Invalid rest format: "${restStr}". Expected format: r:{duration} (e.g., "r:q", "r:h")`);
  }
  const [, durationCode, dotted] = match;
  const baseDuration = DURATION_MAP[durationCode];
  if (baseDuration === void 0) {
    throw new Error(`Invalid duration code: "${durationCode}"`);
  }
  return dotted === "." ? baseDuration * DOTTED_MULTIPLIER : baseDuration;
}
function isRest(str) {
  const lower = str.trim().toLowerCase();
  return lower.startsWith("r:");
}
function parseDuration(durationStr, dotted = false) {
  const baseDuration = DURATION_MAP[durationStr];
  if (baseDuration === void 0) {
    throw new Error(`Invalid duration: "${durationStr}". Valid durations: ${Object.keys(DURATION_MAP).join(", ")}`);
  }
  return dotted ? baseDuration * DOTTED_MULTIPLIER : baseDuration;
}
function beatsToSeconds(beats, tempo) {
  return beats / tempo * 60;
}
function pitchToMidi(pitch) {
  const match = pitch.match(/^([A-G])([#b]?)(-?\d+)$/);
  if (!match) {
    throw new Error(`Invalid pitch: "${pitch}"`);
  }
  const [, noteName, accidental, octaveStr] = match;
  const octave = parseInt(octaveStr, 10);
  let noteValue = NOTE_VALUES[noteName];
  if (accidental === "#") noteValue += 1;
  if (accidental === "b") noteValue -= 1;
  return (octave + 1) * MIDI.SEMITONES_PER_OCTAVE + noteValue;
}
function midiToPitch(midi) {
  const octave = Math.floor(midi / MIDI.SEMITONES_PER_OCTAVE) - 1;
  const noteIndex = midi % MIDI.SEMITONES_PER_OCTAVE;
  return `${NOTE_NAMES[noteIndex]}${octave}`;
}
function transposePitch(pitch, semitones) {
  const midi = pitchToMidi(pitch);
  return midiToPitch(midi + semitones);
}
function parseNotes(noteStrings) {
  return noteStrings.map(parseNote);
}
function isCompactNotation(str) {
  return str.trim().includes(" ") || str.includes("|");
}
function parseCompactNotes(compactStr) {
  const withoutBars = compactStr.replace(/\|/g, " ");
  return withoutBars.split(/\s+/).map((s) => s.trim()).filter((s) => s.length > 0);
}
function expandNoteStrings(noteStrings) {
  return noteStrings.flatMap((str) => {
    if (isCompactNotation(str)) {
      return parseCompactNotes(str);
    }
    return [str];
  });
}
var BRACKET_CHORD_REGEX = /^\[([A-Ga-g][#b]?\d+(?:,[A-Ga-g][#b]?\d+)+)\]:(\d+|[whq])(\.?)(?:@((?:0|1)?\.?\d+|ppp|pp|p|mp|mf|f|ff|fff))?$/;
function isBracketChord(str) {
  return BRACKET_CHORD_REGEX.test(str.trim());
}
function parseBracketChord(chordStr) {
  const match = chordStr.trim().match(BRACKET_CHORD_REGEX);
  if (!match) {
    throw new Error(`Invalid bracket chord format: "${chordStr}". Expected format: [pitch1,pitch2,...]:duration[@velocity] (e.g., "[C4,E4,G4]:q", "[A3,C4]:h@0.6")`);
  }
  const [, pitchesRaw, durationCode, dotted, velocityRaw] = match;
  const pitches = pitchesRaw.split(",").map((p) => {
    const normalized = p.trim().replace(/^([a-g])/, (_, note) => note.toUpperCase());
    if (!/^[A-G][#b]?\d+$/.test(normalized)) {
      throw new Error(`Invalid pitch "${p}" in bracket chord "${chordStr}"`);
    }
    return normalized;
  });
  const baseDuration = DURATION_MAP[durationCode];
  if (baseDuration === void 0) {
    throw new Error(`Invalid duration code: "${durationCode}" in bracket chord "${chordStr}"`);
  }
  const isDotted = dotted === ".";
  const durationBeats = isDotted ? baseDuration * DOTTED_MULTIPLIER : baseDuration;
  let velocity;
  if (velocityRaw) {
    if (velocityRaw in DYNAMICS) {
      velocity = DYNAMICS[velocityRaw];
    } else {
      velocity = parseFloat(velocityRaw);
      if (velocity < 0 || velocity > 1) {
        throw new Error(`Invalid velocity ${velocity} in "${chordStr}". Must be 0.0-1.0`);
      }
    }
  }
  return {
    pitches,
    duration: durationCode,
    durationBeats,
    dotted: isDotted,
    velocity
  };
}

// src/parser/chord-parser.ts
var CHORD_REGEX = /^([A-G][#b]?)((?:minmaj|mM|m\/M|maj|min|m|M|dim|aug|o||\+|lyd|dom|quartal|quintal|so_what|mu|power|shell)?(?:7sus[24]?|9sus[24]?|11sus[24]?|13sus[24]?|sus[24]?)?(?:\d+)?(?:add\d+)?(?:alt)?(?:[b#]\d+)*)(?:@(\w+))?(?:\/([A-G][#b]?))?:(\d+|[whq])(\.?)([*~>^]?)$/;
var CHORD_INTERVALS = {
  // ============================================
  // TRIADS (3 notes)
  // ============================================
  "": [0, 4, 7],
  // major (default)
  "maj": [0, 4, 7],
  // major
  "M": [0, 4, 7],
  // major
  "m": [0, 3, 7],
  // minor
  "min": [0, 3, 7],
  // minor
  "dim": [0, 3, 6],
  // diminished
  "aug": [0, 4, 8],
  // augmented
  "+": [0, 4, 8],
  // augmented (alt notation)
  "sus2": [0, 2, 7],
  // suspended 2nd
  "sus4": [0, 5, 7],
  // suspended 4th
  "sus": [0, 5, 7],
  // suspended 4th (default)
  // ============================================
  // SEVENTH CHORDS (4 notes)
  // ============================================
  // Major family
  "maj7": [0, 4, 7, 11],
  // major 7th
  "M7": [0, 4, 7, 11],
  // major 7th
  // Dominant family
  "7": [0, 4, 7, 10],
  // dominant 7th
  "dom7": [0, 4, 7, 10],
  // dominant 7th
  // Minor family
  "m7": [0, 3, 7, 10],
  // minor 7th
  "min7": [0, 3, 7, 10],
  // minor 7th
  // Diminished family
  "dim7": [0, 3, 6, 9],
  // diminished 7th
  "o7": [0, 3, 6, 9],
  // diminished 7th (alt)
  "m7b5": [0, 3, 6, 10],
  // half-diminished 7th
  "\xF87": [0, 3, 6, 10],
  // half-diminished 7th (alt)
  // Augmented family
  "aug7": [0, 4, 8, 10],
  // augmented 7th
  "+7": [0, 4, 8, 10],
  // augmented 7th (alt)
  "augmaj7": [0, 4, 8, 11],
  // augmented major 7
  "maj7#5": [0, 4, 8, 11],
  // augmented major 7 (alt)
  "+M7": [0, 4, 8, 11],
  // augmented major 7 (alt)
  // Minor-major family
  "minmaj7": [0, 3, 7, 11],
  // minor-major 7th
  "mM7": [0, 3, 7, 11],
  // minor-major 7th (alt)
  "m/M7": [0, 3, 7, 11],
  // minor-major 7th (alt)
  // Suspended sevenths
  "7sus4": [0, 5, 7, 10],
  "7sus2": [0, 2, 7, 10],
  "7sus": [0, 5, 7, 10],
  // ============================================
  // SIXTH CHORDS
  // ============================================
  "6": [0, 4, 7, 9],
  // major 6th
  "maj6": [0, 4, 7, 9],
  // major 6th
  "m6": [0, 3, 7, 9],
  // minor 6th
  "min6": [0, 3, 7, 9],
  // minor 6th
  "6/9": [0, 4, 7, 9, 14],
  // 6/9 chord
  "m6/9": [0, 3, 7, 9, 14],
  // minor 6/9 chord
  // ============================================
  // EXTENDED CHORDS (9th, 11th, 13th)
  // ============================================
  // Dominant
  "9": [0, 4, 7, 10, 14],
  // dominant 9th
  "dom9": [0, 4, 7, 10, 14],
  // dominant 9th
  "11": [0, 4, 7, 10, 14, 17],
  // dominant 11th
  "dom11": [0, 4, 7, 10, 14, 17],
  "13": [0, 4, 7, 10, 14, 21],
  // dominant 13th (omit 11)
  "dom13": [0, 4, 7, 10, 14, 21],
  // Major
  "maj9": [0, 4, 7, 11, 14],
  // major 9th
  "M9": [0, 4, 7, 11, 14],
  // major 9th
  "maj11": [0, 4, 7, 11, 14, 17],
  // major 11th
  "M11": [0, 4, 7, 11, 14, 17],
  "maj13": [0, 4, 7, 11, 14, 21],
  // major 13th
  "M13": [0, 4, 7, 11, 14, 21],
  // Minor
  "m9": [0, 3, 7, 10, 14],
  // minor 9th
  "min9": [0, 3, 7, 10, 14],
  "m11": [0, 3, 7, 10, 14, 17],
  // minor 11th
  "min11": [0, 3, 7, 10, 14, 17],
  "m13": [0, 3, 7, 10, 14, 21],
  // minor 13th
  "min13": [0, 3, 7, 10, 14, 21],
  // Minor-major
  "mM9": [0, 3, 7, 11, 14],
  // minor-major 9th
  // Suspended extended
  "9sus4": [0, 5, 7, 10, 14],
  "9sus": [0, 5, 7, 10, 14],
  "11sus": [0, 5, 7, 10, 14, 17],
  "13sus4": [0, 5, 7, 10, 14, 21],
  "13sus": [0, 5, 7, 10, 14, 21],
  // ============================================
  // ADD CHORDS
  // ============================================
  "add2": [0, 2, 4, 7],
  // add 2
  "add9": [0, 4, 7, 14],
  // add 9
  "add4": [0, 4, 5, 7],
  // add 4
  "add11": [0, 4, 7, 17],
  // add 11
  "add13": [0, 4, 7, 21],
  // add 13
  "madd2": [0, 2, 3, 7],
  // minor add 2
  "madd9": [0, 3, 7, 14],
  // minor add 9
  "madd4": [0, 3, 5, 7],
  // minor add 4
  "madd11": [0, 3, 7, 17],
  // minor add 11
  // Seventh chords with added tones
  "7add11": [0, 4, 7, 10, 17],
  "7add13": [0, 4, 7, 10, 21],
  "maj7add11": [0, 4, 7, 11, 17],
  "maj7add13": [0, 4, 7, 11, 21],
  "m7add11": [0, 3, 7, 10, 17],
  "m7add13": [0, 3, 7, 10, 21],
  // ============================================
  // LYDIAN CHORDS (#11)
  // ============================================
  "7#11": [0, 4, 7, 10, 18],
  // Lydian dominant
  "lyd7": [0, 4, 7, 10, 18],
  "maj7#11": [0, 4, 7, 11, 18],
  // Lydian major 7
  "lydmaj7": [0, 4, 7, 11, 18],
  "9#11": [0, 4, 7, 10, 14, 18],
  "maj9#11": [0, 4, 7, 11, 14, 18],
  "13#11": [0, 4, 7, 10, 14, 18, 21],
  // ============================================
  // ALTERED DOMINANT CHORDS
  // ============================================
  "7alt": [0, 4, 6, 10, 13, 15],
  // full altered (b5, b9, #9)
  "alt": [0, 4, 6, 10, 13, 15],
  "7b5": [0, 4, 6, 10],
  "7#5": [0, 4, 8, 10],
  "7b9": [0, 4, 7, 10, 13],
  "7#9": [0, 4, 7, 10, 15],
  "7b13": [0, 4, 7, 10, 20],
  // Double alterations
  "7b5b9": [0, 4, 6, 10, 13],
  "7b5#9": [0, 4, 6, 10, 15],
  "7#5b9": [0, 4, 8, 10, 13],
  "7#5#9": [0, 4, 8, 10, 15],
  "7b9b13": [0, 4, 7, 10, 13, 20],
  "7#9b13": [0, 4, 7, 10, 15, 20],
  "7b9#11": [0, 4, 7, 10, 13, 18],
  "7#9#11": [0, 4, 7, 10, 15, 18],
  // 9th/13th alterations
  "9b5": [0, 4, 6, 10, 14],
  "9#5": [0, 4, 8, 10, 14],
  "13b9": [0, 4, 7, 10, 13, 21],
  "13#9": [0, 4, 7, 10, 15, 21],
  "13b5": [0, 4, 6, 10, 14, 21],
  // ============================================
  // QUARTAL & SPECIAL VOICINGS
  // ============================================
  "quartal": [0, 5, 10],
  "quartal4": [0, 5, 10, 15],
  "quintal": [0, 7, 14],
  "so_what": [0, 5, 10, 15, 19],
  // Miles Davis voicing
  "mu": [0, 2, 4, 7],
  // Steely Dan "mu" chord
  // ============================================
  // POWER CHORDS & DYADS
  // ============================================
  "5": [0, 7],
  "power": [0, 7],
  "power8": [0, 7, 12],
  "octave": [0, 12],
  // ============================================
  // SHELL VOICINGS
  // ============================================
  "shell7": [0, 4, 10],
  "shellM7": [0, 4, 11],
  "shellm7": [0, 3, 10]
};
var CHORD_VOICINGS = {
  // Major 7th voicings
  "maj7": {
    "close": [0, 4, 7, 11],
    "drop2": [0, 7, 11, 16],
    "drop3": [0, 11, 16, 19],
    "drop24": [0, 7, 16, 23],
    "shell": [0, 11, 16],
    "open": [-12, 0, 7, 11],
    "spread": [0, 11, 16, 23],
    "quartal": [0, 5, 10, 16],
    "rootless_a": [4, 7, 11, 14],
    "rootless_b": [11, 14, 16, 19]
  },
  "M7": {
    "close": [0, 4, 7, 11],
    "drop2": [0, 7, 11, 16],
    "shell": [0, 11, 16]
  },
  // Minor 7th voicings
  "m7": {
    "close": [0, 3, 7, 10],
    "drop2": [0, 7, 10, 15],
    "drop3": [0, 10, 15, 19],
    "drop24": [0, 7, 15, 22],
    "shell": [0, 10, 15],
    "open": [-12, 0, 7, 15],
    "rootless_a": [3, 7, 10, 14],
    "rootless_b": [10, 14, 15, 19],
    "quartal": [0, 5, 10, 15],
    "so_what": [0, 5, 10, 15, 19]
  },
  "min7": {
    "close": [0, 3, 7, 10],
    "drop2": [0, 7, 10, 15],
    "shell": [0, 10, 15]
  },
  // Dominant 7th voicings
  "7": {
    "close": [0, 4, 7, 10],
    "drop2": [0, 7, 10, 16],
    "drop3": [0, 10, 16, 19],
    "drop24": [0, 7, 16, 22],
    "shell": [0, 10, 16],
    "open": [-12, 0, 7, 16],
    "rootless_a": [4, 7, 10, 14],
    "rootless_b": [10, 14, 16, 19],
    "tritone_sub": [6, 10, 13, 16],
    "freddie": [0, 10, 16]
  },
  "dom7": {
    "close": [0, 4, 7, 10],
    "drop2": [0, 7, 10, 16],
    "shell": [0, 10, 16]
  },
  // Dominant 9th voicings
  "9": {
    "close": [0, 4, 7, 10, 14],
    "drop2": [0, 7, 10, 14, 16],
    "shell": [0, 10, 14, 16],
    "open": [-12, 0, 10, 14, 16],
    "rootless_a": [4, 10, 14, 19],
    "rootless_b": [10, 14, 16, 21],
    "spread": [0, 10, 16, 26]
  },
  "dom9": {
    "close": [0, 4, 7, 10, 14],
    "shell": [0, 10, 14, 16]
  },
  // Major 9th voicings
  "maj9": {
    "close": [0, 4, 7, 11, 14],
    "drop2": [0, 7, 11, 14, 16],
    "shell": [0, 11, 14, 16],
    "open": [-12, 0, 11, 16, 26],
    "rootless": [4, 11, 14, 19]
  },
  "M9": {
    "close": [0, 4, 7, 11, 14],
    "shell": [0, 11, 14, 16]
  },
  // Minor 9th voicings
  "m9": {
    "close": [0, 3, 7, 10, 14],
    "drop2": [0, 7, 10, 14, 15],
    "shell": [0, 10, 14, 15],
    "open": [-12, 0, 10, 14, 15],
    "rootless_a": [3, 10, 14, 19],
    "rootless_b": [10, 14, 15, 21],
    "so_what": [0, 5, 10, 14, 19]
  },
  "min9": {
    "close": [0, 3, 7, 10, 14],
    "shell": [0, 10, 14, 15]
  },
  // Diminished 7th voicings
  "dim7": {
    "close": [0, 3, 6, 9],
    "drop2": [0, 6, 9, 15],
    "spread": [0, 6, 15, 21]
  },
  // Half-diminished voicings
  "m7b5": {
    "close": [0, 3, 6, 10],
    "drop2": [0, 6, 10, 15],
    "shell": [0, 10, 15],
    "rootless": [3, 6, 10, 14]
  },
  // Suspended voicings
  "7sus4": {
    "close": [0, 5, 7, 10],
    "drop2": [0, 7, 10, 17],
    "shell": [0, 10, 17],
    "quartal": [0, 5, 10, 17]
  },
  "9sus4": {
    "close": [0, 5, 7, 10, 14],
    "quartal": [0, 5, 10, 14, 19]
  },
  // 13th voicings
  "13": {
    "close": [0, 4, 7, 10, 14, 21],
    "shell": [0, 10, 16, 21],
    "rootless": [4, 10, 14, 21],
    "gospel": [0, 4, 10, 14, 21]
  },
  // Altered dominant voicings
  "7alt": {
    "close": [0, 4, 6, 10, 13],
    "open": [0, 10, 13, 18],
    "rootless": [4, 6, 10, 13]
  },
  "7#9": {
    "close": [0, 4, 7, 10, 15],
    "drop2": [0, 7, 10, 15, 16],
    "shell": [0, 10, 15, 16]
  },
  "7b9": {
    "close": [0, 4, 7, 10, 13],
    "rootless": [4, 7, 10, 13]
  },
  // Add chord voicings
  "add9": {
    "close": [0, 4, 7, 14],
    "spread": [0, 7, 14, 16]
  },
  // 6th chord voicings
  "6": {
    "close": [0, 4, 7, 9],
    "drop2": [0, 7, 9, 16]
  },
  "m6": {
    "close": [0, 3, 7, 9],
    "drop2": [0, 7, 9, 15]
  },
  // Minor voicings (triads)
  "m": {
    "close": [0, 3, 7],
    "open": [-12, 0, 7, 15]
  },
  "min": {
    "close": [0, 3, 7],
    "open": [-12, 0, 7, 15]
  },
  // Major voicings (triads)
  "maj": {
    "close": [0, 4, 7],
    "open": [-12, 0, 7, 16]
  },
  "": {
    "close": [0, 4, 7],
    "open": [-12, 0, 7, 16]
  }
};
function getVoicingIntervals(quality, voicingName) {
  const qualityVoicings = CHORD_VOICINGS[quality];
  if (!qualityVoicings) {
    return null;
  }
  return qualityVoicings[voicingName] || null;
}
function getChordIntervals(quality) {
  if (CHORD_INTERVALS[quality]) {
    return [...CHORD_INTERVALS[quality]];
  }
  let baseQuality = quality;
  const alterations = [];
  const altRegex = /([b#])(\d+)/g;
  let altMatch;
  while ((altMatch = altRegex.exec(quality)) !== null) {
    const [full, accidental, degree] = altMatch;
    baseQuality = baseQuality.replace(full, "");
    alterations.push({
      degree: parseInt(degree, 10),
      delta: accidental === "#" ? 1 : -1
    });
  }
  const intervals = CHORD_INTERVALS[baseQuality] || CHORD_INTERVALS[""];
  const result = [...intervals];
  for (const alt of alterations) {
    const degreeToSemitone = {
      5: 7,
      9: 14,
      11: 17,
      13: 21
    };
    const baseSemitone = degreeToSemitone[alt.degree];
    if (baseSemitone !== void 0) {
      const index = result.indexOf(baseSemitone);
      if (index >= 0) {
        result[index] += alt.delta;
      } else {
        result.push(baseSemitone + alt.delta);
      }
    }
  }
  return result.sort((a, b) => a - b);
}
function isChordRest(str) {
  return str.trim().startsWith("r:");
}
function parseChordRest(restStr) {
  const match = restStr.trim().match(/^r:(\d+|[whq])(\.?)$/);
  if (!match) {
    throw new Error(`Invalid rest format: "${restStr}"`);
  }
  const [, durationCode, dotted] = match;
  const baseDuration = DURATION_MAP[durationCode];
  if (baseDuration === void 0) {
    throw new Error(`Invalid duration code in rest: "${durationCode}"`);
  }
  return dotted === "." ? baseDuration * 1.5 : baseDuration;
}
function parseChord(chordStr, defaultOctave = 3) {
  if (isChordRest(chordStr)) {
    const durationBeats2 = parseChordRest(chordStr);
    return {
      root: "r",
      quality: "",
      bass: void 0,
      duration: chordStr.split(":")[1].replace(".", ""),
      durationBeats: durationBeats2,
      notes: [],
      // Empty notes array for rest
      articulation: ""
    };
  }
  const match = chordStr.trim().match(CHORD_REGEX);
  if (!match) {
    throw new Error(`Invalid chord format: "${chordStr}". Expected format: {root}{quality}[@voicing]:{duration}[articulation] (e.g., "Cmaj7:w", "Dm:h", "Am7:q*", "Am9@drop2:w")`);
  }
  const [, root, quality, voicing, bass, durationCode, dotted, articulationRaw] = match;
  const isDotted = dotted === ".";
  const articulation = articulationRaw || "";
  const baseDuration = DURATION_MAP[durationCode];
  if (baseDuration === void 0) {
    throw new Error(`Invalid duration code: "${durationCode}"`);
  }
  const durationBeats = isDotted ? baseDuration * 1.5 : baseDuration;
  const rootPitch = `${root}${defaultOctave}`;
  const rootMidi = pitchToMidi(rootPitch);
  let notes;
  if (voicing) {
    const voicingIntervals = getVoicingIntervals(quality || "", voicing);
    if (voicingIntervals) {
      notes = voicingIntervals.map((interval) => midiToPitch(rootMidi + interval));
    } else {
      console.warn(`Voicing "${voicing}" not found for quality "${quality || "maj"}", using standard voicing`);
      const intervals = getChordIntervals(quality || "");
      notes = intervals.map((interval) => midiToPitch(rootMidi + interval));
    }
  } else {
    const intervals = getChordIntervals(quality || "");
    notes = intervals.map((interval) => midiToPitch(rootMidi + interval));
  }
  if (bass) {
    const bassPitch = `${bass}${defaultOctave - 1}`;
    notes.unshift(bassPitch);
  }
  return {
    root,
    quality: quality || "maj",
    bass,
    duration: durationCode,
    durationBeats,
    notes,
    articulation
  };
}
function parseChords(chordStrings, defaultOctave = 3) {
  return chordStrings.map((str) => parseChord(str, defaultOctave));
}
function getChordNotes(chordSymbol, octave = 3) {
  const parsed = parseChord(`${chordSymbol}:q`, octave);
  return parsed.notes;
}
function getSupportedChordQualities() {
  return Object.keys(CHORD_INTERVALS);
}

// src/theory/euclidean.ts
function euclidean(hits, steps) {
  if (hits === 0) return new Array(steps).fill(false);
  if (hits >= steps) return new Array(steps).fill(true);
  let pattern = [];
  for (let i = 0; i < hits; i++) {
    pattern.push([1]);
  }
  for (let i = 0; i < steps - hits; i++) {
    pattern.push([0]);
  }
  function distributeRemainders(groups) {
    const numGroups = groups.length;
    let onesCount = 0;
    let zerosCount = 0;
    for (const group of groups) {
      if (group[0] === 1) onesCount++;
      else zerosCount++;
    }
    if (zerosCount <= 1 || onesCount <= 1) {
      return groups;
    }
    const newGroups = [];
    const minCount = Math.min(onesCount, zerosCount);
    for (let i = 0; i < minCount; i++) {
      const oneIdx = groups.findIndex((g) => g[0] === 1);
      const zeroIdx = groups.findIndex((g) => g[0] === 0);
      if (oneIdx !== -1 && zeroIdx !== -1) {
        newGroups.push([...groups[oneIdx], ...groups[zeroIdx]]);
        groups.splice(Math.max(oneIdx, zeroIdx), 1);
        groups.splice(Math.min(oneIdx, zeroIdx), 1);
      }
    }
    newGroups.push(...groups);
    return distributeRemainders(newGroups);
  }
  pattern = distributeRemainders(pattern);
  const result = [];
  for (const group of pattern) {
    for (const val of group) {
      result.push(val === 1);
    }
  }
  return result;
}
function rotatePattern(pattern, rotation) {
  if (pattern.length === 0 || rotation === 0) return [...pattern];
  const normalizedRotation = (rotation % pattern.length + pattern.length) % pattern.length;
  return [
    ...pattern.slice(pattern.length - normalizedRotation),
    ...pattern.slice(0, pattern.length - normalizedRotation)
  ];
}
function generateEuclidean(hits, steps, rotation = 0) {
  const base = euclidean(hits, steps);
  return rotatePattern(base, rotation);
}
function patternToSteps(pattern) {
  const steps = [];
  for (let i = 0; i < pattern.length; i++) {
    if (pattern[i]) steps.push(i);
  }
  return steps;
}

// src/theory/scales.ts
var SCALE_INTERVALS2 = {
  // Diatonic modes
  "major": [0, 2, 4, 5, 7, 9, 11],
  "ionian": [0, 2, 4, 5, 7, 9, 11],
  "dorian": [0, 2, 3, 5, 7, 9, 10],
  "phrygian": [0, 1, 3, 5, 7, 8, 10],
  "lydian": [0, 2, 4, 6, 7, 9, 11],
  "mixolydian": [0, 2, 4, 5, 7, 9, 10],
  "minor": [0, 2, 3, 5, 7, 8, 10],
  "aeolian": [0, 2, 3, 5, 7, 8, 10],
  "locrian": [0, 1, 3, 5, 6, 8, 10],
  // Harmonic and melodic minor
  "harmonic_minor": [0, 2, 3, 5, 7, 8, 11],
  "melodic_minor": [0, 2, 3, 5, 7, 9, 11],
  // Pentatonic scales
  "pentatonic_major": [0, 2, 4, 7, 9],
  "pentatonic_minor": [0, 3, 5, 7, 10],
  // Blues scales
  "blues": [0, 3, 5, 6, 7, 10],
  "blues_major": [0, 2, 3, 4, 7, 9],
  // Other common scales
  "whole_tone": [0, 2, 4, 6, 8, 10],
  "diminished": [0, 2, 3, 5, 6, 8, 9, 11],
  "diminished_half_whole": [0, 1, 3, 4, 6, 7, 9, 10],
  "chromatic": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
  // Jazz scales
  "bebop_dominant": [0, 2, 4, 5, 7, 9, 10, 11],
  "bebop_major": [0, 2, 4, 5, 7, 8, 9, 11],
  "altered": [0, 1, 3, 4, 6, 8, 10]
};
var SCALE_ALIASES = {
  "nat_minor": "minor",
  "natural_minor": "minor",
  "min": "minor",
  "maj": "major",
  "pent": "pentatonic_major",
  "pent_major": "pentatonic_major",
  "pent_minor": "pentatonic_minor",
  "harm_minor": "harmonic_minor",
  "mel_minor": "melodic_minor"
};
function getScaleIntervals(scaleName) {
  const normalized = scaleName.toLowerCase().replace(/\s+/g, "_");
  const name = SCALE_ALIASES[normalized] || normalized;
  const intervals = SCALE_INTERVALS2[name];
  if (!intervals) {
    throw new Error(`Unknown scale: "${scaleName}". Available scales: ${Object.keys(SCALE_INTERVALS2).join(", ")}`);
  }
  return [...intervals];
}
function getScaleNotes(root, scaleName, octave = 4) {
  const intervals = getScaleIntervals(scaleName);
  const rootPitch = `${root}${octave}`;
  const rootMidi = pitchToMidi(rootPitch);
  return intervals.map((interval) => midiToPitch(rootMidi + interval));
}
function getScaleDegree(root, scaleName, degree, octave = 4) {
  const intervals = getScaleIntervals(scaleName);
  const scaleLength = intervals.length;
  const octaveOffset = Math.floor((degree - 1) / scaleLength);
  const normalizedDegree = (degree - 1) % scaleLength;
  const interval = intervals[normalizedDegree];
  const rootPitch = `${root}${octave}`;
  const rootMidi = pitchToMidi(rootPitch);
  return midiToPitch(rootMidi + interval + octaveOffset * 12);
}
function isNoteInScale(note, root, scaleName) {
  const noteMidi = pitchToMidi(note) % 12;
  const rootMidi = pitchToMidi(`${root}4`) % 12;
  const intervals = getScaleIntervals(scaleName);
  const relativeInterval = (noteMidi - rootMidi + 12) % 12;
  return intervals.includes(relativeInterval);
}
function snapToScale(note, root, scaleName) {
  const noteMidi = pitchToMidi(note);
  const noteClass = noteMidi % 12;
  const octave = Math.floor(noteMidi / 12);
  const rootMidi = pitchToMidi(`${root}4`) % 12;
  const intervals = getScaleIntervals(scaleName);
  let minDistance = Infinity;
  let closestInterval = 0;
  for (const interval of intervals) {
    const scaleNoteClass = (rootMidi + interval) % 12;
    const distance = Math.min(
      Math.abs(noteClass - scaleNoteClass),
      12 - Math.abs(noteClass - scaleNoteClass)
    );
    if (distance < minDistance) {
      minDistance = distance;
      closestInterval = interval;
    }
  }
  const targetNoteClass = (rootMidi + closestInterval) % 12;
  return midiToPitch(octave * 12 + targetNoteClass);
}
function getRelativeKey(root, mode) {
  const rootMidi = pitchToMidi(`${root}4`) % 12;
  if (mode === "minor") {
    const majorRoot = midiToPitch(48 + (rootMidi + 3) % 12);
    return { root: majorRoot.replace(/\d+$/, ""), mode: "major" };
  } else {
    const minorRoot = midiToPitch(48 + (rootMidi - 3 + 12) % 12);
    return { root: minorRoot.replace(/\d+$/, ""), mode: "minor" };
  }
}
function getParallelKey(root, mode) {
  return { root, mode: mode === "major" ? "minor" : "major" };
}
function parseKey(keyString) {
  const match = keyString.match(/^([A-G][#b]?)\s*(.*)$/i);
  if (!match) {
    throw new Error(`Invalid key: "${keyString}"`);
  }
  const [, root, modeRaw] = match;
  const mode = modeRaw.trim().toLowerCase() || "major";
  const modeNormalized = SCALE_ALIASES[mode] || mode;
  return { root, mode: modeNormalized };
}
function getAvailableScales() {
  return Object.keys(SCALE_INTERVALS2);
}

// src/theory/transformations.ts
var BEATS_TO_DURATION = {
  4: "w",
  2: "h",
  1: "q",
  0.5: "8",
  0.25: "16",
  0.125: "32"
};
function beatsToDuration(beats) {
  if (BEATS_TO_DURATION[beats]) {
    return BEATS_TO_DURATION[beats];
  }
  const undottedBeats = beats / 1.5;
  if (BEATS_TO_DURATION[undottedBeats]) {
    return BEATS_TO_DURATION[undottedBeats] + ".";
  }
  const sortedBeats = Object.keys(BEATS_TO_DURATION).map(Number).sort((a, b) => b - a);
  for (const dur of sortedBeats) {
    if (beats >= dur) {
      return BEATS_TO_DURATION[dur];
    }
  }
  return "32";
}
function parseNoteComponents(noteStr) {
  const trimmed = noteStr.trim();
  if (trimmed.startsWith("r:")) {
    const durationPart2 = trimmed.slice(2);
    const dotted2 = durationPart2.endsWith(".");
    const duration2 = dotted2 ? durationPart2.slice(0, -1) : durationPart2;
    return { pitch: null, duration: duration2, dotted: dotted2, isRest: true };
  }
  const colonIndex = trimmed.lastIndexOf(":");
  if (colonIndex === -1) {
    throw new Error(`Invalid note format: "${noteStr}"`);
  }
  const pitch = trimmed.slice(0, colonIndex);
  const durationPart = trimmed.slice(colonIndex + 1);
  const dotted = durationPart.endsWith(".");
  const duration = dotted ? durationPart.slice(0, -1) : durationPart;
  return { pitch, duration, dotted, isRest: false };
}
function reconstructNote(pitch, duration, dotted, isRest3) {
  const durationStr = dotted ? duration + "." : duration;
  if (isRest3) {
    return `r:${durationStr}`;
  }
  return `${pitch}:${durationStr}`;
}
function invertPattern(pattern, axisPitch) {
  let axis;
  if (axisPitch) {
    axis = pitchToMidi(axisPitch);
  } else {
    const firstNote = pattern.find((n) => !n.startsWith("r:"));
    if (!firstNote) {
      return [...pattern];
    }
    const { pitch } = parseNoteComponents(firstNote);
    axis = pitchToMidi(pitch);
  }
  return pattern.map((noteStr) => {
    const { pitch, duration, dotted, isRest: isRestNote } = parseNoteComponents(noteStr);
    if (isRestNote) {
      return noteStr;
    }
    const midi = pitchToMidi(pitch);
    const interval = midi - axis;
    const invertedMidi = axis - interval;
    const invertedPitch = midiToPitch(invertedMidi);
    return reconstructNote(invertedPitch, duration, dotted, false);
  });
}
function retrogradePattern(pattern) {
  return [...pattern].reverse();
}
function augmentPattern(pattern, factor) {
  return pattern.map((noteStr) => {
    const { pitch, duration, dotted, isRest: isRestNote } = parseNoteComponents(noteStr);
    const baseDuration = DURATION_MAP[duration];
    if (baseDuration === void 0) {
      throw new Error(`Unknown duration: "${duration}"`);
    }
    const originalBeats = dotted ? baseDuration * 1.5 : baseDuration;
    const newBeats = originalBeats * factor;
    const newDuration = beatsToDuration(newBeats);
    const isDotted = newDuration.endsWith(".");
    const cleanDuration = isDotted ? newDuration.slice(0, -1) : newDuration;
    return reconstructNote(pitch, cleanDuration, isDotted, isRestNote);
  });
}
function transposePattern(pattern, semitones) {
  return pattern.map((noteStr) => {
    const { pitch, duration, dotted, isRest: isRestNote } = parseNoteComponents(noteStr);
    if (isRestNote) {
      return noteStr;
    }
    const midi = pitchToMidi(pitch);
    const transposedMidi = midi + semitones;
    const transposedPitch = midiToPitch(transposedMidi);
    return reconstructNote(transposedPitch, duration, dotted, false);
  });
}
function extractHead(pattern, count) {
  return pattern.slice(0, count);
}
function extractTail(pattern, count) {
  return pattern.slice(-count);
}
function shiftOctave(pattern, octaves) {
  return transposePattern(pattern, octaves * 12);
}
function rotatePattern2(pattern, steps) {
  if (pattern.length === 0 || steps === 0) {
    return [...pattern];
  }
  const n = pattern.length;
  const normalizedSteps = (steps % n + n) % n;
  return [
    ...pattern.slice(normalizedSteps),
    ...pattern.slice(0, normalizedSteps)
  ];
}

// src/generative/markov-presets.ts
function generateUniform(states) {
  const transitions = {};
  const prob = 1 / states.length;
  for (const state of states) {
    transitions[state] = {};
    for (const target of states) {
      transitions[state][target] = prob;
    }
  }
  return transitions;
}
function generateNeighborWeighted(states) {
  const transitions = {};
  for (let i = 0; i < states.length; i++) {
    const state = states[i];
    transitions[state] = {};
    let totalWeight = 0;
    const weights = [];
    for (let j = 0; j < states.length; j++) {
      const distance = Math.abs(i - j);
      const weight = 1 / (distance + 1);
      weights.push(weight);
      totalWeight += weight;
    }
    for (let j = 0; j < states.length; j++) {
      transitions[state][states[j]] = weights[j] / totalWeight;
    }
  }
  return transitions;
}
function generateWalkingBass(states) {
  const transitions = {};
  const hasApproach = states.includes("approach");
  const hasRest = states.includes("rest");
  const hasRoot = states.includes("1");
  for (const state of states) {
    transitions[state] = {};
    if (state === "1") {
      const others = states.filter((s) => s !== "1" && s !== "rest");
      const baseProbEach = 0.85 / others.length;
      for (const target of states) {
        if (target === "1") {
          transitions[state][target] = 0.1;
        } else if (target === "5") {
          transitions[state][target] = baseProbEach + 0.05;
        } else if (target === "rest") {
          transitions[state][target] = 0.05;
        } else {
          transitions[state][target] = baseProbEach;
        }
      }
    } else if (state === "approach") {
      for (const target of states) {
        if (target === "1") {
          transitions[state][target] = 0.6;
        } else if (target === "5") {
          transitions[state][target] = 0.3;
        } else {
          transitions[state][target] = 0.1 / (states.length - 2);
        }
      }
    } else if (state === "rest") {
      for (const target of states) {
        if (target === "1") {
          transitions[state][target] = 0.5;
        } else if (target === "rest") {
          transitions[state][target] = 0.1;
        } else {
          transitions[state][target] = 0.4 / (states.length - 2);
        }
      }
    } else {
      for (const target of states) {
        if (target === "1") {
          transitions[state][target] = 0.35;
        } else if (target === "approach" && hasApproach) {
          transitions[state][target] = 0.2;
        } else if (target === state) {
          transitions[state][target] = 0.05;
        } else {
          const remaining = hasApproach ? 0.4 : 0.6;
          const otherCount = states.filter((s) => s !== "1" && s !== "approach" && s !== state).length;
          transitions[state][target] = remaining / Math.max(1, otherCount);
        }
      }
    }
  }
  return normalizeTransitions(transitions);
}
function generateMelodyStepwise(states) {
  const transitions = {};
  for (let i = 0; i < states.length; i++) {
    const state = states[i];
    transitions[state] = {};
    for (let j = 0; j < states.length; j++) {
      const target = states[j];
      const distance = Math.abs(i - j);
      if (distance === 0) {
        transitions[state][target] = 0.05;
      } else if (distance === 1) {
        transitions[state][target] = 0.35;
      } else if (distance === 2) {
        transitions[state][target] = 0.15;
      } else {
        transitions[state][target] = 0.05 / distance;
      }
    }
  }
  return normalizeTransitions(transitions);
}
function generateRootHeavy(states) {
  const transitions = {};
  const hasRoot = states.includes("1");
  for (const state of states) {
    transitions[state] = {};
    if (state === "1") {
      for (const target of states) {
        if (target === "1") {
          transitions[state][target] = 0.3;
        } else if (target === "5") {
          transitions[state][target] = 0.25;
        } else {
          transitions[state][target] = 0.45 / (states.length - 2);
        }
      }
    } else {
      for (const target of states) {
        if (target === "1") {
          transitions[state][target] = 0.5;
        } else if (target === state) {
          transitions[state][target] = 0.1;
        } else {
          transitions[state][target] = 0.4 / (states.length - 2);
        }
      }
    }
  }
  return normalizeTransitions(transitions);
}
function normalizeTransitions(transitions) {
  const normalized = {};
  for (const [state, probs] of Object.entries(transitions)) {
    normalized[state] = {};
    const sum = Object.values(probs).reduce((a, b) => a + b, 0);
    if (sum === 0) {
      const count = Object.keys(probs).length;
      for (const target of Object.keys(probs)) {
        normalized[state][target] = 1 / count;
      }
    } else {
      for (const [target, prob] of Object.entries(probs)) {
        normalized[state][target] = prob / sum;
      }
    }
  }
  return normalized;
}
var PRESET_GENERATORS = {
  "uniform": generateUniform,
  "neighbor_weighted": generateNeighborWeighted,
  "walking_bass": generateWalkingBass,
  "melody_stepwise": generateMelodyStepwise,
  "root_heavy": generateRootHeavy
};
function getPresetTransitions(preset, states) {
  const generator = PRESET_GENERATORS[preset];
  if (!generator) {
    console.warn(`Unknown Markov preset: ${preset}, using uniform`);
    return generateUniform(states);
  }
  return generator(states);
}
function isValidPreset(preset) {
  return preset in PRESET_GENERATORS;
}

// src/generative/markov.ts
var CHORD_SCALE_MAP = {
  // Major family
  "maj": "major",
  "maj7": "major",
  "maj9": "major",
  "maj6": "major",
  "6": "major",
  "6/9": "major",
  "add9": "major",
  // Minor family
  "m": "minor",
  "min": "minor",
  "m7": "dorian",
  // Minor 7 often uses dorian for brighter sound
  "min7": "dorian",
  "m9": "dorian",
  "m6": "dorian",
  "m11": "dorian",
  // Dominant family
  "7": "mixolydian",
  "dom7": "mixolydian",
  "9": "mixolydian",
  "11": "mixolydian",
  "13": "mixolydian",
  "7sus4": "mixolydian",
  "7#9": "mixolydian",
  // Hendrix chord
  "7b9": "phrygian",
  // Altered sound
  // Half-diminished
  "m7b5": "locrian",
  "half-dim": "locrian",
  // Diminished
  "dim": "locrian",
  "dim7": "locrian",
  // Sus chords
  "sus2": "major",
  "sus4": "major",
  // Augmented
  "aug": "major",
  // Whole tone implied but use major for simplicity
  "+": "major"
};
var SeededRandom = class {
  state;
  constructor(seed) {
    this.state = seed !== void 0 ? seed : Math.floor(Math.random() * 2147483647);
  }
  next() {
    let t = this.state += 1831565813;
    t = Math.imul(t ^ t >>> 15, t | 1);
    t ^= t + Math.imul(t ^ t >>> 7, t | 61);
    return ((t ^ t >>> 14) >>> 0) / 4294967296;
  }
};
function parseKey2(keyStr) {
  const match = keyStr.match(/^([A-G][#b]?)\s*(major|minor|maj|min|m|M|dorian|phrygian|lydian|mixolydian|aeolian|locrian)?$/i);
  if (!match) {
    return { root: "C", mode: "major" };
  }
  const [, root, mode] = match;
  const normalizedMode = normalizeMode(mode || "major");
  return { root, mode: normalizedMode };
}
function normalizeMode(mode) {
  const modeMap = {
    "major": "major",
    "maj": "major",
    "M": "major",
    "minor": "minor",
    "min": "minor",
    "m": "minor",
    "dorian": "dorian",
    "phrygian": "phrygian",
    "lydian": "lydian",
    "mixolydian": "mixolydian",
    "aeolian": "aeolian",
    "locrian": "locrian"
  };
  return modeMap[mode?.toLowerCase() ?? "major"] || "major";
}
function parseChordSymbol(chord) {
  const match = chord.match(/^([A-G][#b]?)(.*)$/);
  if (!match) {
    return { root: "C", quality: "maj" };
  }
  const [, root, quality] = match;
  return { root, quality: quality || "maj" };
}
function getChordScale(chordSymbol) {
  const { root, quality } = parseChordSymbol(chordSymbol);
  const mode = CHORD_SCALE_MAP[quality] || "major";
  return { root, mode };
}
function isNoteInScale2(midiNote, scaleRoot, mode) {
  const intervals = SCALE_INTERVALS[mode] || SCALE_INTERVALS["major"];
  const rootMatch = scaleRoot.match(/^([A-G])([#b]?)$/);
  if (!rootMatch) return true;
  let rootValue = NOTE_VALUES[rootMatch[1]];
  if (rootMatch[2] === "#") rootValue += 1;
  if (rootMatch[2] === "b") rootValue -= 1;
  rootValue = (rootValue % 12 + 12) % 12;
  const noteValue = (midiNote % 12 + 12) % 12;
  const intervalFromRoot = (noteValue - rootValue + 12) % 12;
  return intervals.includes(intervalFromRoot);
}
function constrainToScale(pitch, key) {
  const { root, mode } = parseKey2(key);
  const intervals = SCALE_INTERVALS[mode] || SCALE_INTERVALS["major"];
  const pitchMatch = pitch.match(/^([A-G][#b]?)(-?\d+)$/);
  if (!pitchMatch) return pitch;
  const [, noteName, octaveStr] = pitchMatch;
  const midi = noteNameToMidi(noteName, parseInt(octaveStr, 10));
  if (isNoteInScale2(midi, root, mode)) {
    return pitch;
  }
  if (isNoteInScale2(midi + 1, root, mode)) {
    return midiToPitch2(midi + 1);
  }
  if (isNoteInScale2(midi - 1, root, mode)) {
    return midiToPitch2(midi - 1);
  }
  if (isNoteInScale2(midi + 2, root, mode)) {
    return midiToPitch2(midi + 2);
  }
  if (isNoteInScale2(midi - 2, root, mode)) {
    return midiToPitch2(midi - 2);
  }
  return pitch;
}
function noteNameToMidi(noteName, octave) {
  const match = noteName.match(/^([A-G])([#b]?)$/);
  if (!match) throw new Error(`Invalid note name: ${noteName}`);
  const [, note, accidental] = match;
  let value = NOTE_VALUES[note];
  if (accidental === "#") value += 1;
  if (accidental === "b") value -= 1;
  return (octave + 1) * MIDI.SEMITONES_PER_OCTAVE + value;
}
function midiToPitch2(midi) {
  const octave = Math.floor(midi / MIDI.SEMITONES_PER_OCTAVE) - 1;
  const noteIndex = (midi % MIDI.SEMITONES_PER_OCTAVE + MIDI.SEMITONES_PER_OCTAVE) % MIDI.SEMITONES_PER_OCTAVE;
  return `${NOTE_NAMES[noteIndex]}${octave}`;
}
function resolveState(state, key, octave, nextPitch) {
  if (state === "rest") {
    return null;
  }
  if (state === "approach") {
    if (!nextPitch) {
      return null;
    }
    const match = nextPitch.match(/^([A-G][#b]?)(-?\d+)$/);
    if (!match) return null;
    const midi = noteNameToMidi(match[1], parseInt(match[2], 10));
    return midiToPitch2(midi - 1);
  }
  const pitchMatch = state.match(/^([A-G][#b]?)(\d+)$/);
  if (pitchMatch) {
    return state;
  }
  const degreeMatch = state.match(/^([#b]?)(\d+)$/);
  if (degreeMatch) {
    const [, accidental, degreeStr] = degreeMatch;
    const degree = parseInt(degreeStr, 10);
    const { root, mode } = parseKey2(key);
    const intervals = SCALE_INTERVALS[mode] || SCALE_INTERVALS["major"];
    const octaveAdd = Math.floor((degree - 1) / 7);
    const normalizedDegree = (degree - 1) % 7 + 1;
    let interval = intervals[normalizedDegree - 1];
    if (accidental === "#") interval += 1;
    if (accidental === "b") interval -= 1;
    const rootMidi = noteNameToMidi(root, octave);
    const noteMidi = rootMidi + interval + octaveAdd * MIDI.SEMITONES_PER_OCTAVE;
    return midiToPitch2(noteMidi);
  }
  console.warn(`Unknown Markov state: ${state}`);
  return null;
}
function selectNextState(currentState, transitions, rng) {
  const stateTransitions = transitions[currentState];
  if (!stateTransitions) {
    return currentState;
  }
  const r = rng.next();
  let cumulative = 0;
  for (const [nextState, probability] of Object.entries(stateTransitions)) {
    cumulative += probability;
    if (r < cumulative) {
      return nextState;
    }
  }
  return Object.keys(stateTransitions)[0] || currentState;
}
function parseDuration3(durationStr) {
  const DURATIONS2 = {
    w: 4,
    h: 2,
    q: 1,
    "8": 0.5,
    "16": 0.25,
    "32": 0.125,
    "2": 2,
    "4": 1
  };
  const isDotted = durationStr.endsWith(".");
  const code = isDotted ? durationStr.slice(0, -1) : durationStr;
  const base = DURATIONS2[code];
  if (base === void 0) {
    throw new Error(`Invalid duration: ${durationStr}`);
  }
  return isDotted ? base * 1.5 : base;
}
function generateMarkovPattern(config, options) {
  const {
    states,
    transitions: explicitTransitions,
    preset,
    initialState,
    steps,
    duration,
    octave = 3,
    seed,
    constrainToScale: shouldConstrainToScale = false,
    // v0.8
    chordScale
    // v0.8
  } = config;
  let effectiveKey = options.key || "C major";
  if (chordScale) {
    const { root, mode } = getChordScale(chordScale);
    effectiveKey = `${root} ${mode}`;
  }
  const key = effectiveKey;
  const rng = new SeededRandom(seed);
  if (!states || states.length === 0) {
    console.warn("Markov config has no states");
    return [];
  }
  let transitions;
  if (explicitTransitions) {
    transitions = explicitTransitions;
  } else if (preset && isValidPreset(preset)) {
    transitions = getPresetTransitions(preset, states);
  } else {
    console.warn("Markov config has no transitions or valid preset, using uniform distribution");
    transitions = getPresetTransitions("uniform", states);
  }
  let currentState = initialState || states[0];
  const stateSequence = [currentState];
  for (let i = 1; i < steps; i++) {
    currentState = selectNextState(currentState, transitions, rng);
    stateSequence.push(currentState);
  }
  const notes = [];
  let currentBeat = 0;
  const durations = Array.isArray(duration) ? duration : [duration];
  for (let i = 0; i < stateSequence.length; i++) {
    const state = stateSequence[i];
    const durationStr = durations[i % durations.length];
    const durationBeats = parseDuration3(durationStr);
    const nextState = i < stateSequence.length - 1 ? stateSequence[i + 1] : void 0;
    const nextPitch = nextState ? resolveState(nextState, key, octave) || void 0 : void 0;
    let pitch = resolveState(state, key, octave, nextPitch);
    if (pitch !== null && shouldConstrainToScale) {
      pitch = constrainToScale(pitch, key);
    }
    if (pitch !== null) {
      notes.push({
        pitch,
        durationBeats,
        startBeat: currentBeat,
        velocity: 0.7 + rng.next() * 0.2
        // Slight velocity variation
      });
    }
    currentBeat += durationBeats;
  }
  return notes;
}

// src/generative/continuation.ts
function generateContinuation(config, sourceNotes, key) {
  const { technique, steps = 3, interval = -2 } = config;
  if (!sourceNotes || sourceNotes.length === 0) {
    console.warn("Continuation source has no notes");
    return [];
  }
  switch (technique) {
    case "ascending_sequence":
      return generateSequence(sourceNotes, steps, Math.abs(interval));
    case "descending_sequence":
      return generateSequence(sourceNotes, steps, -Math.abs(interval));
    case "extension":
      return generateExtension(sourceNotes, steps);
    case "fragmentation":
      return generateFragmentation(sourceNotes, steps);
    case "development":
      return generateDevelopment(sourceNotes, steps, interval);
    default:
      console.warn(`Unknown continuation technique: ${technique}`);
      return [...sourceNotes];
  }
}
function generateSequence(motif, repetitions, intervalPerStep) {
  const result = [];
  for (let i = 0; i < repetitions; i++) {
    const transposed = transposePattern(motif, i * intervalPerStep);
    result.push(...transposed);
  }
  return result;
}
function generateExtension(sourceNotes, additionalNotes) {
  if (sourceNotes.length < 2) {
    return [...sourceNotes];
  }
  const intervals = analyzeIntervals(sourceNotes);
  if (intervals.length === 0) {
    return [...sourceNotes];
  }
  const lastInterval = intervals[intervals.length - 1];
  const result = [...sourceNotes];
  let lastNote = sourceNotes[sourceNotes.length - 1];
  for (let i = 0; i < additionalNotes; i++) {
    const intervalToApply = intervals[intervals.length - 1 - i % intervals.length];
    const extended = transposePattern([lastNote], intervalToApply);
    if (extended.length > 0) {
      result.push(extended[0]);
      lastNote = extended[0];
    }
  }
  return result;
}
function generateFragmentation(sourceNotes, repetitions) {
  const result = [];
  let fragmentSize = sourceNotes.length;
  for (let i = 0; i < repetitions; i++) {
    fragmentSize = Math.max(1, Math.floor(fragmentSize * 0.7));
    const fragment = i % 2 === 0 ? extractHead(sourceNotes, fragmentSize) : extractTail(sourceNotes, fragmentSize);
    const transposed = transposePattern(fragment, i * 2);
    result.push(...transposed);
  }
  return result;
}
function generateDevelopment(sourceNotes, steps, baseInterval) {
  const result = [];
  result.push(...sourceNotes);
  const sequence = generateSequence(sourceNotes, 2, baseInterval);
  result.push(...sequence.slice(sourceNotes.length));
  if (steps > 2) {
    const fragmented = generateFragmentation(sourceNotes, steps - 2);
    result.push(...fragmented);
  }
  return result;
}
function analyzeIntervals(notes) {
  const intervals = [];
  for (let i = 1; i < notes.length; i++) {
    const prevMidi = noteToMidi(notes[i - 1]);
    const currMidi = noteToMidi(notes[i]);
    if (prevMidi !== null && currMidi !== null) {
      intervals.push(currMidi - prevMidi);
    }
  }
  return intervals;
}
function noteToMidi(noteStr) {
  if (noteStr.startsWith("r:")) {
    return null;
  }
  const colonIndex = noteStr.indexOf(":");
  if (colonIndex === -1) return null;
  const pitch = noteStr.slice(0, colonIndex);
  const match = pitch.match(/^([A-G])([#b]?)(-?\d+)$/);
  if (!match) return null;
  const [, note, accidental, octaveStr] = match;
  const octave = parseInt(octaveStr, 10);
  const noteValues = {
    "C": 0,
    "D": 2,
    "E": 4,
    "F": 5,
    "G": 7,
    "A": 9,
    "B": 11
  };
  let value = noteValues[note];
  if (accidental === "#") value += 1;
  if (accidental === "b") value -= 1;
  return (octave + 1) * 12 + value;
}

// src/theory/voice-leading.ts
var DEFAULT_RANGES = {
  bass: [28, 48],
  // E1 to C3
  tenor: [36, 55],
  // C2 to G3
  alto: [43, 62],
  // G2 to D4
  soprano: [48, 79]
  // C3 to G5
};
var VOICE_NAMES = ["bass", "tenor", "alto", "soprano"];
var CONSTRAINT_PRESETS = {
  bach: [
    "no_parallel_fifths",
    "no_parallel_octaves",
    "resolve_leading_tones",
    "resolve_sevenths",
    "smooth_motion",
    "contrary_outer_motion",
    "avoid_voice_crossing"
  ],
  jazz: [
    "smooth_motion",
    "avoid_voice_crossing"
  ],
  pop: [
    "smooth_motion"
  ],
  custom: []
};
function pitchToMidi2(pitch) {
  const match = pitch.match(/^([A-G])([#b]?)(-?\d+)$/);
  if (!match) throw new Error(`Invalid pitch: ${pitch}`);
  const [, note, accidental, octaveStr] = match;
  const octave = parseInt(octaveStr, 10);
  const noteValues = {
    "C": 0,
    "D": 2,
    "E": 4,
    "F": 5,
    "G": 7,
    "A": 9,
    "B": 11
  };
  let value = noteValues[note];
  if (accidental === "#") value += 1;
  if (accidental === "b") value -= 1;
  return (octave + 1) * 12 + value;
}
function midiToPitch3(midi) {
  const noteNames = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"];
  const octave = Math.floor(midi / 12) - 1;
  const noteIndex = midi % 12;
  return `${noteNames[noteIndex]}${octave}`;
}
function getPitchClass(pitch) {
  const midi = pitchToMidi2(pitch);
  return midi % 12;
}
function getInterval(midi1, midi2) {
  return Math.abs(midi2 - midi1);
}
function isLeadingTone(pc, keyRoot) {
  return (pc - keyRoot + 12) % 12 === 11;
}
function isChordSeventh(pc, chordRoot) {
  const interval = (pc - chordRoot + 12) % 12;
  return interval === 10 || interval === 11;
}
function getChordRootPitchClass(chordSymbol) {
  const match = chordSymbol.match(/^([A-G])([#b]?)/);
  if (!match) return 0;
  const [, note, accidental] = match;
  const noteValues = {
    "C": 0,
    "D": 2,
    "E": 4,
    "F": 5,
    "G": 7,
    "A": 9,
    "B": 11
  };
  let value = noteValues[note] || 0;
  if (accidental === "#") value = (value + 1) % 12;
  if (accidental === "b") value = (value + 11) % 12;
  return value;
}
function hasUnresolvedLeadingTone(voicing1, voicing2, chord1, chord2) {
  const root1 = getChordRootPitchClass(chord1);
  const root2 = getChordRootPitchClass(chord2);
  const leadingTonePC = (root1 + 11) % 12;
  const tonicPC = root1;
  for (let i = 0; i < voicing1.length; i++) {
    const pc1 = voicing1[i] % 12;
    if (isLeadingTone(pc1, root1)) {
      const pc2 = voicing2[i] % 12;
      const motion = voicing2[i] - voicing1[i];
      if (motion > 0 && motion <= 2 && (pc1 + 1) % 12 === pc2) {
        continue;
      }
      if (motion !== 0 && pc2 !== pc1) {
        return true;
      }
    }
  }
  return false;
}
function hasUnresolvedSeventh(voicing1, voicing2, chord1) {
  if (!chord1.includes("7")) {
    return false;
  }
  const root1 = getChordRootPitchClass(chord1);
  for (let i = 0; i < voicing1.length; i++) {
    const pc1 = voicing1[i] % 12;
    if (isChordSeventh(pc1, root1)) {
      const motion = voicing2[i] - voicing1[i];
      if (motion === 0 || motion < 0 && motion >= -2) {
        continue;
      }
      if (motion > 0) {
        return true;
      }
    }
  }
  return false;
}
function hasParallelFifths(voicing1, voicing2) {
  for (let i = 0; i < voicing1.length; i++) {
    for (let j = i + 1; j < voicing1.length; j++) {
      const interval1 = getInterval(voicing1[i], voicing1[j]) % 12;
      const interval2 = getInterval(voicing2[i], voicing2[j]) % 12;
      if (interval1 === 7 && interval2 === 7) {
        const motion1 = voicing2[i] - voicing1[i];
        const motion2 = voicing2[j] - voicing1[j];
        if (motion1 !== 0 && motion2 !== 0 && Math.sign(motion1) === Math.sign(motion2)) {
          return true;
        }
      }
    }
  }
  return false;
}
function hasParallelOctaves(voicing1, voicing2) {
  for (let i = 0; i < voicing1.length; i++) {
    for (let j = i + 1; j < voicing1.length; j++) {
      const interval1 = getInterval(voicing1[i], voicing1[j]) % 12;
      const interval2 = getInterval(voicing2[i], voicing2[j]) % 12;
      if (interval1 === 0 && interval2 === 0) {
        const motion1 = voicing2[i] - voicing1[i];
        const motion2 = voicing2[j] - voicing1[j];
        if (motion1 !== 0 && motion2 !== 0 && Math.sign(motion1) === Math.sign(motion2)) {
          return true;
        }
      }
    }
  }
  return false;
}
function hasVoiceCrossing(voicing) {
  for (let i = 0; i < voicing.length - 1; i++) {
    if (voicing[i] >= voicing[i + 1]) {
      return true;
    }
  }
  return false;
}
function calculateMotion(voicing1, voicing2) {
  let total = 0;
  for (let i = 0; i < voicing1.length; i++) {
    total += Math.abs(voicing2[i] - voicing1[i]);
  }
  return total;
}
function hasContraryOuterMotion(voicing1, voicing2) {
  if (voicing1.length < 2) return true;
  const bassMotion = voicing2[0] - voicing1[0];
  const sopranoMotion = voicing2[voicing1.length - 1] - voicing1[voicing1.length - 1];
  return bassMotion !== 0 && sopranoMotion !== 0 && Math.sign(bassMotion) !== Math.sign(sopranoMotion);
}
function generatePossibleVoicings(chordNotes, voices, ranges) {
  const pitchClasses = chordNotes.map((n) => getPitchClass(n));
  const voicings = [];
  const voiceOptions = [];
  for (let v = 0; v < voices; v++) {
    const [low, high] = ranges[v];
    const options = [];
    for (let midi = low; midi <= high; midi++) {
      const pc = midi % 12;
      if (pitchClasses.includes(pc)) {
        options.push(midi);
      }
    }
    voiceOptions.push(options);
  }
  const MAX_VOICINGS = 1e3;
  function generate(voiceIndex, current) {
    if (voicings.length >= MAX_VOICINGS) return;
    if (voiceIndex === voices) {
      const usedPCs = new Set(current.map((m) => m % 12));
      const hasAllRequired = pitchClasses.slice(0, Math.min(3, pitchClasses.length)).every((pc) => usedPCs.has(pc));
      if (hasAllRequired) {
        voicings.push([...current]);
      }
      return;
    }
    for (const midi of voiceOptions[voiceIndex]) {
      current.push(midi);
      generate(voiceIndex + 1, current);
      current.pop();
    }
  }
  generate(0, []);
  return voicings;
}
function scoreTransition(prev, next, constraints, prevChord, nextChord) {
  let score = 0;
  let valid = true;
  if (constraints.includes("no_parallel_fifths")) {
    if (hasParallelFifths(prev, next)) {
      valid = false;
    }
  }
  if (constraints.includes("no_parallel_octaves")) {
    if (hasParallelOctaves(prev, next)) {
      valid = false;
    }
  }
  if (constraints.includes("avoid_voice_crossing")) {
    if (hasVoiceCrossing(next)) {
      valid = false;
    }
  }
  if (constraints.includes("contrary_outer_motion")) {
    if (!hasContraryOuterMotion(prev, next)) {
      score -= 10;
    }
  }
  if (constraints.includes("resolve_leading_tones") && prevChord && nextChord) {
    if (hasUnresolvedLeadingTone(prev, next, prevChord, nextChord)) {
      score -= 20;
    }
  }
  if (constraints.includes("resolve_sevenths") && prevChord) {
    if (hasUnresolvedSeventh(prev, next, prevChord)) {
      score -= 15;
    }
  }
  if (constraints.includes("smooth_motion")) {
    const motion = calculateMotion(prev, next);
    score -= motion;
  }
  return { valid, score };
}
function findBestVoicingSequence(chords, voices, constraints, ranges) {
  if (chords.length === 0) return [];
  const allVoicings = chords.map((chord) => {
    const notes = getChordNotes(chord, 3);
    return generatePossibleVoicings(notes, voices, ranges);
  });
  for (let i = 0; i < allVoicings.length; i++) {
    if (allVoicings[i].length === 0) {
      console.warn(`No valid voicings for chord: ${chords[i]}`);
      return null;
    }
  }
  const BEAM_WIDTH = 50;
  let beam = allVoicings[0].map((v) => ({
    sequence: [v],
    score: 0
  }));
  for (let i = 1; i < chords.length; i++) {
    const nextBeam = [];
    for (const state of beam) {
      const lastVoicing = state.sequence[state.sequence.length - 1];
      const prevChord = chords[i - 1];
      const nextChord = chords[i];
      for (const nextVoicing of allVoicings[i]) {
        const { valid, score } = scoreTransition(
          lastVoicing,
          nextVoicing,
          constraints,
          prevChord,
          nextChord
        );
        if (valid) {
          nextBeam.push({
            sequence: [...state.sequence, nextVoicing],
            score: state.score + score
          });
        }
      }
    }
    if (nextBeam.length === 0) {
      console.warn(`No valid voicings satisfy all constraints at chord ${i}`);
      for (const state of beam) {
        const prevChord = chords[i - 1];
        const nextChord = chords[i];
        for (const nextVoicing of allVoicings[i]) {
          const { score } = scoreTransition(
            state.sequence[state.sequence.length - 1],
            nextVoicing,
            constraints,
            prevChord,
            nextChord
          );
          nextBeam.push({
            sequence: [...state.sequence, nextVoicing],
            score: state.score + score - 100
            // Heavy penalty for relaxed constraint
          });
        }
      }
    }
    beam = nextBeam.sort((a, b) => b.score - a.score).slice(0, BEAM_WIDTH);
  }
  if (beam.length === 0) return null;
  const best = beam[0];
  return best.sequence.map((voicing, i) => ({
    chord: chords[i],
    notes: voicing.map((m) => midiToPitch3(m))
  }));
}
function generateVoiceLeading(config) {
  const warnings = [];
  const { progression, voices, constraints, voiceRanges, style = "jazz" } = config;
  let effectiveConstraints = [...constraints || []];
  if (style !== "custom" && CONSTRAINT_PRESETS[style]) {
    effectiveConstraints = [...CONSTRAINT_PRESETS[style], ...constraints || []];
  }
  effectiveConstraints = [...new Set(effectiveConstraints)];
  const ranges = [];
  const voiceNames = voices === 4 ? VOICE_NAMES : VOICE_NAMES.slice(0, voices);
  for (let i = 0; i < voices; i++) {
    const voiceName = voiceNames[i] || `voice${i}`;
    if (voiceRanges && voiceRanges[voiceName]) {
      const [low, high] = voiceRanges[voiceName];
      ranges.push([pitchToMidi2(low), pitchToMidi2(high)]);
    } else if (DEFAULT_RANGES[voiceName]) {
      ranges.push(DEFAULT_RANGES[voiceName]);
    } else {
      ranges.push([36 + i * 12, 60 + i * 12]);
    }
  }
  const voicings = findBestVoicingSequence(progression, voices, effectiveConstraints, ranges);
  if (!voicings) {
    warnings.push("Could not find valid voicing sequence for all chords");
    return {
      voicings: progression.map((chord) => ({
        chord,
        notes: getChordNotes(chord, 3)
      })),
      warnings
    };
  }
  return { voicings, warnings };
}

// src/parser/pattern-expander.ts
function generateVelocityPreset(preset, noteCount, baseVelocity) {
  if (noteCount === 0) return [];
  if (noteCount === 1) return [baseVelocity];
  const velocities = [];
  const minVel = Math.max(0.1, baseVelocity * VELOCITY_ENVELOPE.MIN_VELOCITY);
  const maxVel = Math.min(VELOCITY_ENVELOPE.MAX_VELOCITY, baseVelocity * VELOCITY_ENVELOPE.SWELL_PEAK);
  switch (preset) {
    case "crescendo":
      for (let i = 0; i < noteCount; i++) {
        const t = i / (noteCount - 1);
        velocities.push(minVel + t * (maxVel - minVel));
      }
      break;
    case "diminuendo":
      for (let i = 0; i < noteCount; i++) {
        const t = i / (noteCount - 1);
        velocities.push(maxVel - t * (maxVel - minVel));
      }
      break;
    case "swell":
      for (let i = 0; i < noteCount; i++) {
        const t = i / (noteCount - 1);
        const factor = Math.sin(t * Math.PI);
        velocities.push(minVel + factor * (maxVel - minVel));
      }
      break;
    case "accent_first":
      for (let i = 0; i < noteCount; i++) {
        velocities.push(i === 0 ? maxVel : baseVelocity);
      }
      break;
    case "accent_downbeats":
      for (let i = 0; i < noteCount; i++) {
        velocities.push(i % 2 === 0 ? maxVel : baseVelocity * 0.7);
      }
      break;
    default:
      for (let i = 0; i < noteCount; i++) {
        velocities.push(baseVelocity);
      }
  }
  return velocities;
}
function applyVelocityEnvelope(notes, envelope, baseVelocity) {
  if (notes.length === 0) return;
  let velocities;
  if (typeof envelope.velocity === "string") {
    velocities = generateVelocityPreset(envelope.velocity, notes.length, baseVelocity);
  } else {
    const customVels = envelope.velocity;
    if (customVels.length === 0) return;
    velocities = [];
    for (let i = 0; i < notes.length; i++) {
      const t = notes.length === 1 ? 0 : i / (notes.length - 1);
      const arrayPos = t * (customVels.length - 1);
      const lowerIdx = Math.floor(arrayPos);
      const upperIdx = Math.min(lowerIdx + 1, customVels.length - 1);
      const frac = arrayPos - lowerIdx;
      const interpolated = customVels[lowerIdx] * (1 - frac) + customVels[upperIdx] * frac;
      velocities.push(Math.max(0, Math.min(1, interpolated)));
    }
  }
  for (let i = 0; i < notes.length; i++) {
    notes[i].velocity = velocities[i];
  }
}
function applyTransform(sourceNotes, transform) {
  const { operation, params } = transform;
  switch (operation) {
    case "invert":
      return invertPattern(sourceNotes, params?.axis);
    case "retrograde":
      return retrogradePattern(sourceNotes);
    case "augment":
      return augmentPattern(sourceNotes, params?.factor ?? 2);
    case "diminish":
      return augmentPattern(sourceNotes, params?.factor ?? 0.5);
    case "transpose":
      return transposePattern(sourceNotes, params?.semitones ?? 0);
    case "octave":
      return shiftOctave(sourceNotes, params?.octaves ?? 1);
    case "rotate":
      return rotatePattern2(sourceNotes, params?.steps ?? 1);
    default:
      return sourceNotes;
  }
}
function resolvePattern(pattern, allPatterns) {
  if (!pattern.transform || !allPatterns) {
    return pattern;
  }
  const { source, operation, params } = pattern.transform;
  const sourcePattern = allPatterns[source];
  if (!sourcePattern) {
    console.warn(`Transform source pattern "${source}" not found`);
    return pattern;
  }
  const resolvedSource = resolvePattern(sourcePattern, allPatterns);
  if (resolvedSource.notes) {
    const transformedNotes = applyTransform(resolvedSource.notes, pattern.transform);
    return {
      ...pattern,
      notes: transformedNotes,
      transform: void 0
      // Remove transform since it's been applied
    };
  }
  console.warn(`Transform on pattern with no notes array - transforms only work on notes`);
  return pattern;
}
function resolveInheritance(pattern, allPatterns) {
  if (!pattern.extends || !allPatterns) {
    return pattern;
  }
  const parentPattern = allPatterns[pattern.extends];
  if (!parentPattern) {
    console.warn(`Pattern inheritance: parent pattern "${pattern.extends}" not found`);
    return pattern;
  }
  const resolvedParent = resolveInheritance(parentPattern, allPatterns);
  const merged = { ...resolvedParent };
  if (pattern.overrides) {
    if (pattern.overrides.notes) {
      merged.notes = pattern.overrides.notes;
    }
    if (pattern.overrides.transpose && merged.notes) {
      merged.notes = transposePattern(merged.notes, pattern.overrides.transpose);
    }
    if (pattern.overrides.octave && merged.notes) {
      merged.notes = shiftOctave(merged.notes, pattern.overrides.octave);
    }
  }
  const childOnlyProps = ["envelope", "constrainToScale"];
  for (const prop of childOnlyProps) {
    if (pattern[prop] !== void 0) {
      merged[prop] = pattern[prop];
    }
  }
  delete merged.extends;
  delete merged.overrides;
  return merged;
}
function evaluateConditional(conditional, context2) {
  let leftValue;
  switch (conditional.condition) {
    case "density":
      leftValue = context2.density ?? 0.5;
      break;
    case "probability":
      leftValue = Math.random();
      break;
    case "section_index":
      leftValue = context2.sectionIndex ?? 0;
      break;
    default:
      leftValue = 0;
  }
  let result;
  switch (conditional.operator) {
    case ">":
      result = leftValue > conditional.value;
      break;
    case "<":
      result = leftValue < conditional.value;
      break;
    case ">=":
      result = leftValue >= conditional.value;
      break;
    case "<=":
      result = leftValue <= conditional.value;
      break;
    case "==":
      result = leftValue === conditional.value;
      break;
    case "!=":
      result = leftValue !== conditional.value;
      break;
    default:
      result = false;
  }
  return result ? conditional.then : conditional.else || conditional.then;
}
function expandTuplet(config, octaveOffset, transpose, velocity) {
  const [actual, normal] = config.ratio;
  const notes = [];
  let currentBeat = 0;
  let totalNormalBeats = 0;
  const parsedNotes = [];
  for (const noteStr of config.notes) {
    if (isRest(noteStr)) {
      parsedNotes.push({
        noteStr,
        isRest: true,
        restDuration: parseRest(noteStr)
      });
    } else {
      parsedNotes.push({
        noteStr,
        isRest: false,
        parsed: parseNote(noteStr)
      });
    }
  }
  for (const item of parsedNotes) {
    if (item.isRest) {
      totalNormalBeats += item.restDuration;
    } else {
      totalNormalBeats += item.parsed.durationBeats;
    }
  }
  const scaleFactor = normal / actual;
  for (const item of parsedNotes) {
    if (item.isRest) {
      currentBeat += item.restDuration * scaleFactor;
    } else {
      const parsed = item.parsed;
      const adjustedOctave = parsed.octave + octaveOffset;
      const adjustedPitch = applyTranspose(`${parsed.noteName}${parsed.accidental}${adjustedOctave}`, transpose);
      const articulationMods = getArticulationModifiers(parsed.articulation);
      const baseVel = parsed.velocity !== void 0 ? parsed.velocity : velocity;
      const noteVelocity = Math.min(1, baseVel + articulationMods.velocityBoost);
      const scaledDuration = parsed.durationBeats * scaleFactor;
      const noteDuration = scaledDuration * articulationMods.gate;
      const noteData = {
        pitch: adjustedPitch,
        startBeat: currentBeat,
        durationBeats: noteDuration,
        velocity: noteVelocity
      };
      if (parsed.timingOffset !== void 0) noteData.timingOffset = parsed.timingOffset;
      if (parsed.probability !== void 0) noteData.probability = parsed.probability;
      if (parsed.portamento) noteData.portamento = true;
      notes.push(noteData);
      currentBeat += scaledDuration;
    }
  }
  return {
    notes,
    totalBeats: totalNormalBeats * scaleFactor
  };
}
function expandPattern(pattern, context2) {
  const velocity = context2.velocity ?? VELOCITY_ENVELOPE.DEFAULT_VELOCITY;
  const octaveOffset = context2.octaveOffset ?? 0;
  const transpose = context2.transpose ?? 0;
  if (pattern.conditional && context2.allPatterns) {
    const selectedPatternName = evaluateConditional(pattern.conditional, context2);
    const selectedPattern = context2.allPatterns[selectedPatternName];
    if (selectedPattern) {
      return expandPattern(selectedPattern, context2);
    }
    console.warn(`Conditional pattern target "${selectedPatternName}" not found`);
    return { notes: [], totalBeats: 0 };
  }
  let workingPattern = pattern;
  if (pattern.extends && context2.allPatterns) {
    workingPattern = resolveInheritance(pattern, context2.allPatterns);
  }
  const resolvedPattern = workingPattern.transform ? resolvePattern(workingPattern, context2.allPatterns) : workingPattern;
  let notes = [];
  let currentBeat = 0;
  if (resolvedPattern.notes) {
    const noteInput = typeof resolvedPattern.notes === "string" ? [resolvedPattern.notes] : resolvedPattern.notes;
    const expandedNotes = expandNoteStrings(noteInput);
    for (const noteStr of expandedNotes) {
      if (isRest(noteStr)) {
        currentBeat += parseRest(noteStr);
      } else if (isBracketChord(noteStr)) {
        const bracketChord = parseBracketChord(noteStr);
        const chordVelocity = bracketChord.velocity !== void 0 ? bracketChord.velocity : velocity;
        for (const pitch of bracketChord.pitches) {
          const pitchMatch = pitch.match(/^([A-G][#b]?)(\d+)$/);
          if (pitchMatch) {
            const [, notePart, octaveStr] = pitchMatch;
            const adjustedOctave = parseInt(octaveStr, 10) + octaveOffset;
            const adjustedPitch = applyTranspose(`${notePart}${adjustedOctave}`, transpose);
            notes.push({
              pitch: adjustedPitch,
              startBeat: currentBeat,
              durationBeats: bracketChord.durationBeats,
              velocity: chordVelocity
            });
          }
        }
        currentBeat += bracketChord.durationBeats;
      } else {
        const parsed = parseNote(noteStr);
        const adjustedOctave = parsed.octave + octaveOffset;
        const adjustedPitch = applyTranspose(`${parsed.noteName}${parsed.accidental}${adjustedOctave}`, transpose);
        const articulationMods = getArticulationModifiers(parsed.articulation);
        const baseVel = parsed.velocity !== void 0 ? parsed.velocity : velocity;
        const noteVelocity = Math.min(1, baseVel + articulationMods.velocityBoost);
        const noteDuration = parsed.durationBeats * articulationMods.gate;
        const noteData = {
          pitch: adjustedPitch,
          startBeat: currentBeat,
          durationBeats: noteDuration,
          velocity: noteVelocity
        };
        if (parsed.timingOffset !== void 0) noteData.timingOffset = parsed.timingOffset;
        if (parsed.probability !== void 0) noteData.probability = parsed.probability;
        if (parsed.portamento) noteData.portamento = true;
        if (parsed.jazzArticulation) noteData.jazzArticulation = parsed.jazzArticulation;
        if (parsed.bendAmount !== void 0) noteData.bendAmount = parsed.bendAmount;
        if (parsed.ornament) noteData.ornament = parsed.ornament;
        if (parsed.pedal) noteData.pedal = true;
        notes.push(noteData);
        currentBeat += parsed.durationBeats;
      }
    }
  }
  if (resolvedPattern.chords) {
    for (const chordStr of resolvedPattern.chords) {
      const parsed = parseChord(chordStr);
      const articulationMods = getArticulationModifiers(parsed.articulation);
      const chordVelocity = Math.min(1, velocity + articulationMods.velocityBoost);
      const chordDuration = parsed.durationBeats * articulationMods.gate;
      for (const pitch of parsed.notes) {
        const adjustedPitch = applyTranspose(adjustOctave(pitch, octaveOffset), transpose);
        notes.push({
          pitch: adjustedPitch,
          startBeat: currentBeat,
          durationBeats: chordDuration,
          velocity: chordVelocity
        });
      }
      currentBeat += parsed.durationBeats;
    }
  }
  if (resolvedPattern.degrees && context2.key) {
    const rhythm = resolvedPattern.rhythm || ["q"];
    let rhythmIndex = 0;
    for (const degree of resolvedPattern.degrees) {
      const parsed = parseEnhancedDegree(degree);
      const durationStr = parsed.duration || rhythm[rhythmIndex % rhythm.length];
      const durationBeats = parseDurationString(durationStr);
      if (typeof degree === "number" || !isRest(`r:${degree}`)) {
        const pitch = scaleDegreeToNote(degree, context2.key, 4 + octaveOffset);
        const adjustedPitch = applyTranspose(pitch, transpose);
        notes.push({
          pitch: adjustedPitch,
          startBeat: currentBeat,
          durationBeats,
          velocity
        });
      }
      currentBeat += durationBeats;
      if (!parsed.duration) {
        rhythmIndex++;
      }
    }
  }
  if (resolvedPattern.arpeggio) {
    const expanded = expandArpeggio(resolvedPattern.arpeggio, octaveOffset, transpose, velocity);
    for (const note of expanded.notes) {
      notes.push({
        ...note,
        startBeat: currentBeat + note.startBeat
      });
    }
    currentBeat += expanded.totalBeats;
  }
  if (resolvedPattern.type === "drums" || resolvedPattern.drums) {
    const drumPattern = resolvedPattern.drums || resolvedPattern;
    const expanded = expandDrumPattern(drumPattern, velocity);
    for (const note of expanded.notes) {
      notes.push({
        ...note,
        startBeat: currentBeat + note.startBeat
      });
    }
    currentBeat += expanded.totalBeats;
  }
  if (resolvedPattern.euclidean) {
    const expanded = expandEuclidean(resolvedPattern.euclidean, octaveOffset, transpose, velocity);
    for (const note of expanded.notes) {
      notes.push({
        ...note,
        startBeat: currentBeat + note.startBeat
      });
    }
    currentBeat += expanded.totalBeats;
  }
  if (resolvedPattern.markov) {
    const expanded = expandMarkov(resolvedPattern.markov, context2, velocity);
    for (const note of expanded.notes) {
      notes.push({
        ...note,
        startBeat: currentBeat + note.startBeat
      });
    }
    currentBeat += expanded.totalBeats;
  }
  if (resolvedPattern.continuation && context2.allPatterns) {
    const expanded = expandContinuation(resolvedPattern.continuation, context2, octaveOffset, transpose, velocity);
    for (const note of expanded.notes) {
      notes.push({
        ...note,
        startBeat: currentBeat + note.startBeat
      });
    }
    currentBeat += expanded.totalBeats;
  }
  if (resolvedPattern.voiceLead) {
    const expanded = expandVoiceLead(resolvedPattern.voiceLead, octaveOffset, transpose, velocity);
    for (const note of expanded.notes) {
      notes.push({
        ...note,
        startBeat: currentBeat + note.startBeat
      });
    }
    currentBeat += expanded.totalBeats;
  }
  if (resolvedPattern.tuplet) {
    const expanded = expandTuplet(resolvedPattern.tuplet, octaveOffset, transpose, velocity);
    for (const note of expanded.notes) {
      notes.push({
        ...note,
        startBeat: currentBeat + note.startBeat
      });
    }
    currentBeat += expanded.totalBeats;
  }
  if (resolvedPattern.rest) {
    currentBeat += parseRest(resolvedPattern.rest);
  }
  if (resolvedPattern.envelope) {
    applyVelocityEnvelope(notes, resolvedPattern.envelope, velocity);
  }
  if (resolvedPattern.constrainToScale && context2.key) {
    const { root, mode } = parseKey(context2.key);
    notes = notes.map((note) => {
      if (note.pitch.startsWith("drum:")) {
        return note;
      }
      return {
        ...note,
        pitch: snapToScale(note.pitch, root, mode)
      };
    });
  }
  if (resolvedPattern.pedal === true) {
    notes = notes.map((note) => ({ ...note, pedal: true }));
  }
  if (resolvedPattern.pedalMarks && resolvedPattern.pedalMarks.length > 0) {
    notes = applyPedalMarks(notes, resolvedPattern.pedalMarks);
  }
  return {
    notes,
    totalBeats: currentBeat
  };
}
function applyPedalMarks(notes, pedalMarks) {
  return notes.map((note) => {
    for (const mark of pedalMarks) {
      if (note.startBeat >= mark.start && note.startBeat < mark.end) {
        const extendedDuration = Math.max(
          note.durationBeats,
          mark.end - note.startBeat
        );
        return {
          ...note,
          pedal: true,
          durationBeats: extendedDuration
        };
      }
    }
    return note;
  });
}
function parseDurationString(str) {
  const isDotted = str.endsWith(".");
  const code = isDotted ? str.slice(0, -1) : str;
  const base = DURATIONS[code];
  if (base === void 0) {
    throw new Error(`Invalid duration: ${str}`);
  }
  return isDotted ? base * DOTTED_MULTIPLIER : base;
}
function parseEnhancedDegree(degree) {
  if (typeof degree === "number") {
    return { degreeNum: degree, accidentalOffset: 0, octaveShift: 0 };
  }
  const match = degree.match(/^(\d+)([#b]?)([+-]?)(?::(\d+|[whq])(\.?))?$/);
  if (!match) {
    const legacyMatch = degree.match(/^([#b]?)(\d+)$/);
    if (legacyMatch) {
      return {
        degreeNum: parseInt(legacyMatch[2], 10),
        accidentalOffset: legacyMatch[1] === "#" ? 1 : legacyMatch[1] === "b" ? -1 : 0,
        octaveShift: 0
      };
    }
    throw new Error(`Invalid degree: ${degree}`);
  }
  const [, degNum, accidental, octaveMod, durCode, dotted] = match;
  return {
    degreeNum: parseInt(degNum, 10),
    accidentalOffset: accidental === "#" ? 1 : accidental === "b" ? -1 : 0,
    octaveShift: octaveMod === "+" ? 1 : octaveMod === "-" ? -1 : 0,
    duration: durCode ? durCode + (dotted || "") : void 0
  };
}
function scaleDegreeToNote(degree, key, octave) {
  const keyMatch = key.match(/^([A-G][#b]?)\s*(major|minor|maj|min|m|M|dorian|phrygian|lydian|mixolydian|aeolian|locrian)?$/i);
  if (!keyMatch) {
    throw new Error(`Invalid key: ${key}`);
  }
  const [, root, mode] = keyMatch;
  const normalizedMode = normalizeMode2(mode || "major");
  const intervals = SCALE_INTERVALS[normalizedMode] || SCALE_INTERVALS["major"];
  const parsed = parseEnhancedDegree(degree);
  const { degreeNum, accidentalOffset, octaveShift } = parsed;
  const octaveAdd = Math.floor((degreeNum - 1) / 7);
  const normalizedDegree = (degreeNum - 1) % 7 + 1;
  const interval = intervals[normalizedDegree - 1] + accidentalOffset;
  const rootMidi = noteNameToMidi2(root, octave);
  const noteMidi = rootMidi + interval + octaveAdd * 12 + octaveShift * 12;
  return midiToPitchName(noteMidi);
}
function normalizeMode2(mode) {
  const modeMap = {
    "major": "major",
    "maj": "major",
    "M": "major",
    "minor": "minor",
    "min": "minor",
    "m": "minor",
    "dorian": "dorian",
    "phrygian": "phrygian",
    "lydian": "lydian",
    "mixolydian": "mixolydian",
    "aeolian": "aeolian",
    "locrian": "locrian"
  };
  return modeMap[mode.toLowerCase()] || "major";
}
function noteNameToMidi2(noteName, octave) {
  const match = noteName.match(/^([A-G])([#b]?)$/);
  if (!match) throw new Error(`Invalid note name: ${noteName}`);
  const [, note, accidental] = match;
  let value = NOTE_VALUES[note];
  if (accidental === "#") value += 1;
  if (accidental === "b") value -= 1;
  return (octave + 1) * MIDI.SEMITONES_PER_OCTAVE + value;
}
function midiToPitchName(midi) {
  const octave = Math.floor(midi / MIDI.SEMITONES_PER_OCTAVE) - 1;
  const noteIndex = midi % MIDI.SEMITONES_PER_OCTAVE;
  return `${NOTE_NAMES[noteIndex]}${octave}`;
}
function adjustOctave(pitch, octaveOffset) {
  const match = pitch.match(/^([A-G][#b]?)(-?\d+)$/);
  if (!match) return pitch;
  const [, note, oct] = match;
  return `${note}${parseInt(oct, 10) + octaveOffset}`;
}
function adjustOctaveByMidi(pitch, octaves) {
  const match = pitch.match(/^([A-G][#b]?)(-?\d+)$/);
  if (!match) return pitch;
  const [, note, oct] = match;
  return `${note}${parseInt(oct, 10) + octaves}`;
}
function applyTranspose(pitch, semitones) {
  if (semitones === 0) return pitch;
  const match = pitch.match(/^([A-G][#b]?)(-?\d+)$/);
  if (!match) return pitch;
  const midi = noteNameToMidi2(match[1], parseInt(match[2], 10));
  return midiToPitchName(midi + semitones);
}
function generateArpPattern(chordNotes, mode, octaves, steps) {
  const numNotes = chordNotes.length;
  const totalNotes = numNotes * octaves;
  const indices = [];
  for (let oct = 0; oct < octaves; oct++) {
    for (let i = 0; i < numNotes; i++) {
      indices.push(oct * numNotes + i + 1);
    }
  }
  let pattern = [];
  switch (mode) {
    case "up":
      pattern = [...indices];
      break;
    case "down":
      pattern = [...indices].reverse();
      break;
    case "updown":
      pattern = [...indices, ...indices.slice(1, -1).reverse()];
      break;
    case "downup":
      pattern = [...indices.reverse(), ...indices.slice(1, -1).reverse()];
      break;
    case "random":
      const len = steps || totalNotes;
      for (let i = 0; i < len; i++) {
        pattern.push(indices[Math.floor(Math.random() * indices.length)]);
      }
      break;
  }
  if (steps && mode !== "random") {
    const patternLen = pattern.length;
    if ((mode === "updown" || mode === "downup") && steps > patternLen) {
      const result2 = [];
      const extraSteps = steps - patternLen;
      const peakIndex = mode === "updown" ? totalNotes - 1 : 0;
      const peakNote = indices[peakIndex];
      const insertPoint = totalNotes;
      for (let i = 0; i < patternLen; i++) {
        result2.push(pattern[i]);
        if (i === insertPoint - 1) {
          for (let j = 0; j < extraSteps; j++) {
            result2.push(peakNote);
          }
        }
      }
      return result2.slice(0, steps);
    }
    const result = [];
    for (let i = 0; i < steps; i++) {
      result.push(pattern[i % patternLen]);
    }
    return result;
  }
  return pattern;
}
function expandArpeggio(arpeggio, octaveOffset, transpose, velocity) {
  const { chord, duration, mode, octaves = ARPEGGIATOR.DEFAULT_OCTAVES, gate = ARPEGGIATOR.DEFAULT_GATE, steps } = arpeggio;
  const chordNotes = getChordNotes(chord, 3 + octaveOffset);
  const durationBeats = parseDurationString(duration);
  let arpPattern;
  if (arpeggio.pattern) {
    arpPattern = arpeggio.pattern;
  } else if (mode) {
    arpPattern = generateArpPattern(chordNotes, mode, octaves, steps);
  } else {
    arpPattern = chordNotes.map((_, i) => i + 1);
  }
  const notes = [];
  let currentBeat = 0;
  for (const noteIndex of arpPattern) {
    const octaveAdd = Math.floor((noteIndex - 1) / chordNotes.length) * 12;
    const noteIdx = (noteIndex - 1) % chordNotes.length;
    const basePitch = chordNotes[noteIdx];
    const adjustedPitch = applyTranspose(adjustOctaveByMidi(basePitch, octaveAdd / 12), transpose);
    notes.push({
      pitch: adjustedPitch,
      startBeat: currentBeat,
      durationBeats: durationBeats * gate,
      velocity
    });
    currentBeat += durationBeats;
  }
  return {
    notes,
    totalBeats: currentBeat
  };
}
function parseDrumTime(timeStr) {
  const DRUM_DURATION_MAP = {
    ...DURATIONS,
    "0": 0
  };
  const parts = timeStr.split("+");
  let total = 0;
  for (const part of parts) {
    const trimmed = part.trim();
    if (DRUM_DURATION_MAP[trimmed] !== void 0) {
      total += DRUM_DURATION_MAP[trimmed];
    } else {
      const num = parseFloat(trimmed);
      if (!isNaN(num)) {
        total += num;
      }
    }
  }
  return total;
}
function expandDrumPattern(drums, velocity) {
  const notes = [];
  const kit = drums.kit || "909";
  const stepDuration = parseDurationString(drums.stepDuration || DRUM_SEQUENCER.DEFAULT_STEP_DURATION);
  const DRUM_NAMES2 = ["kick", "snare", "hihat", "openhat", "closedhat", "clap", "rim", "tom_hi", "tom_mid", "tom_lo", "crash", "ride", "cowbell", "shaker", "perc"];
  const directDrumKeys = Object.keys(drums).filter((k) => DRUM_NAMES2.includes(k) && typeof drums[k] === "string");
  if (directDrumKeys.length > 0 && !drums.lines) {
    const lines = {};
    for (const key of directDrumKeys) {
      lines[key] = drums[key];
    }
    drums = { ...drums, lines };
  }
  if (drums.lines) {
    let maxLength = 0;
    for (const [drumName, pattern] of Object.entries(drums.lines)) {
      if (typeof pattern !== "string") continue;
      for (let i = 0; i < pattern.length; i++) {
        const char = pattern[i];
        if (char === "x" || char === "X") {
          notes.push({
            pitch: `drum:${drumName}@${kit}`,
            startBeat: i * stepDuration,
            durationBeats: stepDuration,
            velocity: velocity * DRUM_SEQUENCER.DEFAULT_VELOCITY
          });
        } else if (char === ">") {
          notes.push({
            pitch: `drum:${drumName}@${kit}`,
            startBeat: i * stepDuration,
            durationBeats: stepDuration,
            velocity: DRUM_SEQUENCER.ACCENT_VELOCITY
          });
        }
      }
      if (pattern.length > maxLength) {
        maxLength = pattern.length;
      }
    }
    const totalBeats2 = drums.bars !== void 0 ? drums.bars * 4 : maxLength * stepDuration;
    return { notes, totalBeats: totalBeats2 };
  }
  if (drums.steps) {
    const pattern = drums.steps;
    for (let i = 0; i < pattern.length; i++) {
      const char = pattern[i];
      if (char === "x" || char === "X") {
        notes.push({
          pitch: `drum:kick@${kit}`,
          // Default to kick for steps pattern
          startBeat: i * stepDuration,
          durationBeats: stepDuration,
          velocity: velocity * DRUM_SEQUENCER.DEFAULT_VELOCITY
        });
      } else if (char === ">") {
        notes.push({
          pitch: `drum:kick@${kit}`,
          startBeat: i * stepDuration,
          durationBeats: stepDuration,
          velocity: DRUM_SEQUENCER.ACCENT_VELOCITY
        });
      }
    }
  }
  if (drums.hits) {
    for (const hit of drums.hits) {
      const time = parseDrumTime(hit.time);
      const vel = hit.velocity ?? velocity;
      notes.push({
        pitch: `drum:${hit.drum}@${kit}`,
        startBeat: time,
        durationBeats: stepDuration,
        // Drums don't really use duration, but needed for timing
        velocity: vel
      });
    }
  }
  let totalBeats = 0;
  if (drums.bars !== void 0) {
    totalBeats = drums.bars * 4;
  } else if (drums.steps) {
    totalBeats = drums.steps.length * stepDuration;
  } else if (drums.hits && drums.hits.length > 0) {
    const maxTime = Math.max(...drums.hits.map((h) => parseDrumTime(h.time)));
    totalBeats = Math.max(maxTime + stepDuration, 4);
  }
  return {
    notes,
    totalBeats
  };
}
function expandEuclidean(config, octaveOffset, transpose, velocity) {
  const { hits, steps, rotation = 0, duration, pitch, drum } = config;
  const pattern = generateEuclidean(hits, steps, rotation);
  const stepIndices = patternToSteps(pattern);
  const durationBeats = parseDurationString(duration);
  const notes = [];
  for (const stepIndex of stepIndices) {
    let notePitch;
    if (drum) {
      notePitch = `drum:${drum}@909`;
    } else if (pitch) {
      const adjustedPitch = applyTranspose(adjustOctave(pitch, octaveOffset), transpose);
      notePitch = adjustedPitch;
    } else {
      notePitch = applyTranspose(`C${4 + octaveOffset}`, transpose);
    }
    notes.push({
      pitch: notePitch,
      startBeat: stepIndex * durationBeats,
      durationBeats,
      velocity
    });
  }
  return {
    notes,
    totalBeats: steps * durationBeats
  };
}
function expandMarkov(config, context2, velocity) {
  const key = context2.key || "C major";
  const tempo = context2.tempo;
  const generatedNotes = generateMarkovPattern(config, { key, tempo });
  const notes = generatedNotes.map((n) => ({
    pitch: n.pitch,
    startBeat: n.startBeat,
    durationBeats: n.durationBeats,
    velocity: n.velocity * velocity
  }));
  const totalBeats = generatedNotes.length > 0 ? Math.max(...generatedNotes.map((n) => n.startBeat + n.durationBeats)) : 0;
  return { notes, totalBeats };
}
function expandContinuation(config, context2, octaveOffset, transpose, velocity) {
  const sourcePattern = context2.allPatterns?.[config.source];
  if (!sourcePattern || !sourcePattern.notes) {
    console.warn(`Continuation source pattern "${config.source}" not found or has no notes`);
    return { notes: [], totalBeats: 0 };
  }
  const sourceNotes = sourcePattern.notes;
  const continuedNotes = generateContinuation(config, sourceNotes, context2.key);
  const notes = [];
  let currentBeat = 0;
  for (const noteStr of continuedNotes) {
    if (isRest(noteStr)) {
      currentBeat += parseRest(noteStr);
    } else {
      const parsed = parseNote(noteStr);
      const adjustedOctave = parsed.octave + octaveOffset;
      const adjustedPitch = applyTransposeInternal(`${parsed.noteName}${parsed.accidental}${adjustedOctave}`, transpose);
      const articulationMods = getArticulationModifiers(parsed.articulation);
      const noteVelocity = Math.min(1, velocity + articulationMods.velocityBoost);
      const noteDuration = parsed.durationBeats * articulationMods.gate;
      notes.push({
        pitch: adjustedPitch,
        startBeat: currentBeat,
        durationBeats: noteDuration,
        velocity: noteVelocity
      });
      currentBeat += parsed.durationBeats;
    }
  }
  return { notes, totalBeats: currentBeat };
}
function applyTransposeInternal(pitch, semitones) {
  if (semitones === 0) return pitch;
  const match = pitch.match(/^([A-G][#b]?)(-?\d+)$/);
  if (!match) return pitch;
  const midi = noteNameToMidi2(match[1], parseInt(match[2], 10));
  return midiToPitchName(midi + semitones);
}
function expandVoiceLead(config, octaveOffset, transpose, velocity) {
  const result = generateVoiceLeading(config);
  const notes = [];
  let currentBeat = 0;
  const chordDuration = 4;
  for (const voicing of result.voicings) {
    for (const pitch of voicing.notes) {
      const match = pitch.match(/^([A-G][#b]?)(-?\d+)$/);
      if (!match) continue;
      const adjustedOctave = parseInt(match[2], 10) + octaveOffset;
      const adjustedPitch = applyTransposeInternal(`${match[1]}${adjustedOctave}`, transpose);
      notes.push({
        pitch: adjustedPitch,
        startBeat: currentBeat,
        durationBeats: chordDuration,
        velocity
      });
    }
    currentBeat += chordDuration;
  }
  return { notes, totalBeats: currentBeat };
}

// src/parser/json-preprocessor.ts
function stripComments(obj) {
  if (obj === null || typeof obj !== "object") {
    return obj;
  }
  if (Array.isArray(obj)) {
    return obj.map(
      (item) => typeof item === "object" && item !== null ? stripComments(item) : item
    );
  }
  const result = {};
  for (const [key, value] of Object.entries(obj)) {
    if (key.startsWith("//")) {
      continue;
    }
    if (typeof value === "object" && value !== null) {
      result[key] = stripComments(value);
    } else {
      result[key] = value;
    }
  }
  return result;
}
function parseWithComments(json) {
  const parsed = JSON.parse(json);
  return stripComments(parsed);
}
function isCommentKey(key) {
  return key.startsWith("//");
}

// src/theory/chords.ts
var CHORD_INTERVALS2 = {
  // ============================================
  // TRIADS (3 notes)
  // ============================================
  "maj": [0, 4, 7],
  "min": [0, 3, 7],
  "m": [0, 3, 7],
  "dim": [0, 3, 6],
  "aug": [0, 4, 8],
  "sus2": [0, 2, 7],
  "sus4": [0, 5, 7],
  "sus": [0, 5, 7],
  // ============================================
  // SEVENTH CHORDS (4 notes)
  // ============================================
  // Major family
  "maj7": [0, 4, 7, 11],
  "M7": [0, 4, 7, 11],
  // Dominant family
  "7": [0, 4, 7, 10],
  "dom7": [0, 4, 7, 10],
  // Minor family
  "min7": [0, 3, 7, 10],
  "m7": [0, 3, 7, 10],
  // Diminished family
  "dim7": [0, 3, 6, 9],
  "o7": [0, 3, 6, 9],
  "m7b5": [0, 3, 6, 10],
  // Half-diminished
  "\xF87": [0, 3, 6, 10],
  // Half-diminished (alternate notation)
  // Augmented family
  "aug7": [0, 4, 8, 10],
  "+7": [0, 4, 8, 10],
  "augmaj7": [0, 4, 8, 11],
  // Augmented major 7
  "maj7#5": [0, 4, 8, 11],
  // Same as augmaj7
  "+M7": [0, 4, 8, 11],
  // Minor-major family
  "minmaj7": [0, 3, 7, 11],
  "mM7": [0, 3, 7, 11],
  "m/M7": [0, 3, 7, 11],
  // Suspended sevenths
  "7sus4": [0, 5, 7, 10],
  "7sus2": [0, 2, 7, 10],
  "7sus": [0, 5, 7, 10],
  // Default sus = sus4
  // ============================================
  // SIXTH CHORDS
  // ============================================
  "6": [0, 4, 7, 9],
  "maj6": [0, 4, 7, 9],
  "m6": [0, 3, 7, 9],
  "min6": [0, 3, 7, 9],
  "6/9": [0, 4, 7, 9, 14],
  "m6/9": [0, 3, 7, 9, 14],
  // ============================================
  // EXTENDED CHORDS (9th, 11th, 13th)
  // ============================================
  // Dominant 9ths
  "9": [0, 4, 7, 10, 14],
  "dom9": [0, 4, 7, 10, 14],
  // Major 9ths
  "maj9": [0, 4, 7, 11, 14],
  "M9": [0, 4, 7, 11, 14],
  // Minor 9ths
  "min9": [0, 3, 7, 10, 14],
  "m9": [0, 3, 7, 10, 14],
  // Minor-major 9th
  "mM9": [0, 3, 7, 11, 14],
  // Suspended 9ths
  "9sus4": [0, 5, 7, 10, 14],
  "9sus": [0, 5, 7, 10, 14],
  // Dominant 11ths
  "11": [0, 4, 7, 10, 14, 17],
  "dom11": [0, 4, 7, 10, 14, 17],
  // Major 11ths
  "maj11": [0, 4, 7, 11, 14, 17],
  "M11": [0, 4, 7, 11, 14, 17],
  // Minor 11ths
  "min11": [0, 3, 7, 10, 14, 17],
  "m11": [0, 3, 7, 10, 14, 17],
  // Suspended 11 (common in funk/soul)
  "11sus": [0, 5, 7, 10, 14, 17],
  // Dominant 13ths
  "13": [0, 4, 7, 10, 14, 17, 21],
  "dom13": [0, 4, 7, 10, 14, 17, 21],
  // Major 13ths
  "maj13": [0, 4, 7, 11, 14, 17, 21],
  "M13": [0, 4, 7, 11, 14, 17, 21],
  // Minor 13ths
  "min13": [0, 3, 7, 10, 14, 17, 21],
  "m13": [0, 3, 7, 10, 14, 17, 21],
  // Suspended 13ths
  "13sus4": [0, 5, 7, 10, 14, 17, 21],
  "13sus": [0, 5, 7, 10, 14, 17, 21],
  // ============================================
  // ADD CHORDS (triads with single extension)
  // ============================================
  "add2": [0, 2, 4, 7],
  // Same as add9 but 9th in lower octave
  "add9": [0, 4, 7, 14],
  // Major triad + 9th
  "add4": [0, 4, 5, 7],
  // Major + 4
  "add11": [0, 4, 7, 17],
  // Major triad + 11th
  "add13": [0, 4, 7, 21],
  // Major triad + 13th
  "madd2": [0, 2, 3, 7],
  // Minor add2
  "madd9": [0, 3, 7, 14],
  // Minor add9
  "madd4": [0, 3, 5, 7],
  // Minor add4
  "madd11": [0, 3, 7, 17],
  // Minor add11
  // ============================================
  // LYDIAN CHORDS (#11 - raised 4th)
  // ============================================
  "7#11": [0, 4, 7, 10, 18],
  // Lydian dominant
  "lyd7": [0, 4, 7, 10, 18],
  // Alias
  "maj7#11": [0, 4, 7, 11, 18],
  // Lydian major 7
  "lydmaj7": [0, 4, 7, 11, 18],
  // Alias
  "9#11": [0, 4, 7, 10, 14, 18],
  // Dominant 9 #11
  "maj9#11": [0, 4, 7, 11, 14, 18],
  // Major 9 #11
  "13#11": [0, 4, 7, 10, 14, 18, 21],
  // Dominant 13 #11
  // ============================================
  // ALTERED DOMINANT CHORDS
  // ============================================
  // Basic alterations
  "7b5": [0, 4, 6, 10],
  "7#5": [0, 4, 8, 10],
  "7b9": [0, 4, 7, 10, 13],
  "7#9": [0, 4, 7, 10, 15],
  "7b13": [0, 4, 7, 10, 20],
  // Flat 13 (same as #5 + octave)
  // Double alterations
  "7b5b9": [0, 4, 6, 10, 13],
  "7b5#9": [0, 4, 6, 10, 15],
  "7#5b9": [0, 4, 8, 10, 13],
  "7#5#9": [0, 4, 8, 10, 15],
  "7b9b13": [0, 4, 7, 10, 13, 20],
  "7#9b13": [0, 4, 7, 10, 15, 20],
  "7b9#11": [0, 4, 7, 10, 13, 18],
  "7#9#11": [0, 4, 7, 10, 15, 18],
  // Fully altered (b9, #9, b5/#11, b13)
  "7alt": [0, 4, 6, 10, 13, 15],
  // Root, 3, b5, b7, b9, #9
  "alt": [0, 4, 6, 10, 13, 15],
  // Alias
  // 9th alterations
  "9b5": [0, 4, 6, 10, 14],
  "9#5": [0, 4, 8, 10, 14],
  // 13th alterations
  "13b9": [0, 4, 7, 10, 13, 17, 21],
  "13#9": [0, 4, 7, 10, 15, 17, 21],
  "13b5": [0, 4, 6, 10, 14, 17, 21],
  "13#5": [0, 4, 8, 10, 14, 17, 21],
  // ============================================
  // QUARTAL & QUINTAL CHORDS (4ths/5ths stacking)
  // ============================================
  "quartal": [0, 5, 10],
  // Stacked 4ths
  "quartal4": [0, 5, 10, 15],
  // 4 notes quartal
  "quintal": [0, 7, 14],
  // Stacked 5ths
  "quintal4": [0, 7, 14, 21],
  // 4 notes quintal
  "so_what": [0, 5, 10, 15, 19],
  // Miles Davis "So What" voicing (4ths + maj 3rd)
  // ============================================
  // CLUSTER & SECUNDAL CHORDS
  // ============================================
  "cluster3": [0, 1, 2],
  // 3-note chromatic cluster
  "cluster4": [0, 1, 2, 3],
  // 4-note chromatic cluster
  "mu": [0, 2, 4, 7],
  // Steely Dan "mu" chord (add2 voicing)
  // ============================================
  // POLYCHORDS (slash chord notation handles most)
  // ============================================
  // D/C style slash chords handled by parser
  // These are specific polychord voicings
  "majover": [0, 4, 7, 12, 16, 19],
  // Major triad over root + octave major triad
  // ============================================
  // SPECIAL VOICINGS
  // ============================================
  // James Bond chord (augmented with major 7)
  "bond": [0, 4, 8, 11, 14],
  // Augmented major 9 (maj7#5add9)
  // Phrygian sound
  "phryg": [0, 1, 5, 7],
  // Root, b2, 4, 5
  // ============================================
  // POWER CHORDS & DYADS
  // ============================================
  "5": [0, 7],
  "power": [0, 7],
  "power8": [0, 7, 12],
  // Power chord with octave
  "octave": [0, 12],
  // Pure octave
  "unison": [0],
  // Single note
  // ============================================
  // JAZZ SHELL VOICINGS (minimal voicings for comping)
  // ============================================
  // These are also available through @shell voicing modifier
  "shell7": [0, 4, 10],
  // Root, 3rd, b7 (dominant shell)
  "shellM7": [0, 4, 11],
  // Root, 3rd, maj7 (major shell)
  "shellm7": [0, 3, 10]
  // Root, b3, b7 (minor shell)
};
var VOICINGS = {
  // Major 7th voicings
  "maj7": {
    "close": { name: "Close", intervals: [0, 4, 7, 11], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 11, 16], description: "Second voice dropped an octave" },
    "drop3": { name: "Drop 3", intervals: [0, 11, 16, 19], description: "Third voice dropped an octave" },
    "drop24": { name: "Drop 2&4", intervals: [0, 7, 16, 23], description: "Open jazz guitar voicing" },
    "shell": { name: "Shell", intervals: [0, 11, 16], description: "Root, 7th, 3rd (no 5th)" },
    "open": { name: "Open", intervals: [-12, 0, 7, 11], description: "Wide spread with bass note" },
    "spread": { name: "Spread", intervals: [0, 11, 16, 23], description: "Very open, orchestral feel" },
    "quartal": { name: "Quartal", intervals: [0, 5, 10, 16], description: "Modern quartal voicing" },
    "rootless_a": { name: "Rootless A", intervals: [4, 7, 11, 14], description: "Bill Evans A position" },
    "rootless_b": { name: "Rootless B", intervals: [11, 14, 16, 19], description: "Bill Evans B position" }
  },
  "M7": {
    "close": { name: "Close", intervals: [0, 4, 7, 11], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 11, 16], description: "Second voice dropped an octave" },
    "shell": { name: "Shell", intervals: [0, 11, 16], description: "Root, 7th, 3rd (no 5th)" }
  },
  // Minor 7th voicings
  "m7": {
    "close": { name: "Close", intervals: [0, 3, 7, 10], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 10, 15], description: "Second voice dropped an octave" },
    "drop3": { name: "Drop 3", intervals: [0, 10, 15, 19], description: "Third voice dropped" },
    "drop24": { name: "Drop 2&4", intervals: [0, 7, 15, 22], description: "Open voicing" },
    "shell": { name: "Shell", intervals: [0, 10, 15], description: "Root, 7th, 3rd" },
    "open": { name: "Open", intervals: [-12, 0, 7, 15], description: "Wide spread" },
    "rootless_a": { name: "Rootless A", intervals: [3, 7, 10, 14], description: "Bill Evans A position" },
    "rootless_b": { name: "Rootless B", intervals: [10, 14, 15, 19], description: "Bill Evans B position" },
    "quartal": { name: "Quartal", intervals: [0, 5, 10, 15], description: "Modern quartal" },
    "so_what": { name: "So What", intervals: [0, 5, 10, 15, 19], description: "Miles Davis voicing" }
  },
  "min7": {
    "close": { name: "Close", intervals: [0, 3, 7, 10], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 10, 15], description: "Second voice dropped" },
    "shell": { name: "Shell", intervals: [0, 10, 15], description: "Minimal voicing" }
  },
  // Dominant 7th voicings
  "7": {
    "close": { name: "Close", intervals: [0, 4, 7, 10], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 10, 16], description: "Second voice dropped an octave" },
    "drop3": { name: "Drop 3", intervals: [0, 10, 16, 19], description: "Third voice dropped" },
    "drop24": { name: "Drop 2&4", intervals: [0, 7, 16, 22], description: "Open guitar voicing" },
    "shell": { name: "Shell", intervals: [0, 10, 16], description: "Root, 7th, 3rd (no 5th)" },
    "open": { name: "Open", intervals: [-12, 0, 7, 16], description: "Wide spread" },
    "rootless_a": { name: "Rootless A", intervals: [4, 7, 10, 14], description: "Bill Evans A position" },
    "rootless_b": { name: "Rootless B", intervals: [10, 14, 16, 19], description: "Bill Evans B position" },
    "tritone_sub": { name: "Tritone Sub", intervals: [6, 10, 13, 16], description: "Tritone substitution" },
    "freddie": { name: "Freddie Green", intervals: [0, 10, 16], description: "Count Basie style" }
  },
  "dom7": {
    "close": { name: "Close", intervals: [0, 4, 7, 10], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 10, 16], description: "Second voice dropped" },
    "shell": { name: "Shell", intervals: [0, 10, 16], description: "Minimal voicing" }
  },
  // Dominant 9th voicings
  "9": {
    "close": { name: "Close", intervals: [0, 4, 7, 10, 14], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 10, 14, 16], description: "Open voicing" },
    "shell": { name: "Shell", intervals: [0, 10, 14, 16], description: "No 5th" },
    "rootless_a": { name: "Rootless A", intervals: [4, 10, 14, 19], description: "Rootless with 3, b7, 9, 13" },
    "rootless_b": { name: "Rootless B", intervals: [10, 14, 16, 21], description: "Rootless B position" },
    "spread": { name: "Spread", intervals: [0, 10, 16, 26], description: "Wide spread" }
  },
  "dom9": {
    "close": { name: "Close", intervals: [0, 4, 7, 10, 14], description: "Standard" },
    "shell": { name: "Shell", intervals: [0, 10, 14, 16], description: "No 5th" }
  },
  // Major 9th voicings
  "maj9": {
    "close": { name: "Close", intervals: [0, 4, 7, 11, 14], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 11, 14, 16], description: "Open voicing" },
    "shell": { name: "Shell", intervals: [0, 11, 14, 16], description: "No 5th" },
    "open": { name: "Open", intervals: [-12, 0, 11, 16, 26], description: "Very wide" },
    "rootless": { name: "Rootless", intervals: [4, 11, 14, 19], description: "No root" }
  },
  "M9": {
    "close": { name: "Close", intervals: [0, 4, 7, 11, 14], description: "Standard" },
    "shell": { name: "Shell", intervals: [0, 11, 14, 16], description: "No 5th" }
  },
  // Minor 9th voicings
  "m9": {
    "close": { name: "Close", intervals: [0, 3, 7, 10, 14], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 10, 14, 15], description: "Open voicing" },
    "shell": { name: "Shell", intervals: [0, 10, 14, 15], description: "No 5th" },
    "rootless_a": { name: "Rootless A", intervals: [3, 10, 14, 19], description: "Bill Evans A" },
    "rootless_b": { name: "Rootless B", intervals: [10, 14, 15, 21], description: "Bill Evans B" },
    "so_what": { name: "So What", intervals: [0, 5, 10, 14, 19], description: "Miles style" }
  },
  "min9": {
    "close": { name: "Close", intervals: [0, 3, 7, 10, 14], description: "Standard" },
    "shell": { name: "Shell", intervals: [0, 10, 14, 15], description: "No 5th" }
  },
  // Diminished 7th voicings
  "dim7": {
    "close": { name: "Close", intervals: [0, 3, 6, 9], description: "Standard symmetric voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 6, 9, 15], description: "Second voice dropped" },
    "spread": { name: "Spread", intervals: [0, 6, 15, 21], description: "Wide spread" }
  },
  // Half-diminished voicings
  "m7b5": {
    "close": { name: "Close", intervals: [0, 3, 6, 10], description: "Standard close voicing" },
    "drop2": { name: "Drop 2", intervals: [0, 6, 10, 15], description: "Second voice dropped" },
    "shell": { name: "Shell", intervals: [0, 10, 15], description: "Root, 7th, 3rd" },
    "rootless": { name: "Rootless", intervals: [3, 6, 10, 14], description: "No root" }
  },
  // Suspended voicings
  "7sus4": {
    "close": { name: "Close", intervals: [0, 5, 7, 10], description: "Standard" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 10, 17], description: "Open voicing" },
    "shell": { name: "Shell", intervals: [0, 10, 17], description: "Minimal" },
    "quartal": { name: "Quartal", intervals: [0, 5, 10, 17], description: "Stacked 4ths" }
  },
  "9sus4": {
    "close": { name: "Close", intervals: [0, 5, 7, 10, 14], description: "Standard" },
    "quartal": { name: "Quartal", intervals: [0, 5, 10, 14, 19], description: "Modern quartal" }
  },
  // 13th voicings
  "13": {
    "close": { name: "Close", intervals: [0, 4, 7, 10, 14, 21], description: "Omit 11th" },
    "shell": { name: "Shell", intervals: [0, 10, 16, 21], description: "3, 7, 13" },
    "rootless": { name: "Rootless", intervals: [4, 10, 14, 21], description: "No root" },
    "gospel": { name: "Gospel", intervals: [0, 4, 10, 14, 21], description: "Full gospel sound" }
  },
  // Altered dominant voicings
  "7alt": {
    "close": { name: "Close", intervals: [0, 4, 6, 10, 13], description: "b5, b9" },
    "open": { name: "Open", intervals: [0, 10, 13, 18], description: "Spread alt" },
    "rootless": { name: "Rootless", intervals: [4, 6, 10, 13], description: "No root" }
  },
  "7#9": {
    "close": { name: "Close", intervals: [0, 4, 7, 10, 15], description: "Hendrix chord" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 10, 15, 16], description: "Open" },
    "shell": { name: "Shell", intervals: [0, 10, 15, 16], description: "Minimal Hendrix" }
  },
  "7b9": {
    "close": { name: "Close", intervals: [0, 4, 7, 10, 13], description: "Standard" },
    "rootless": { name: "Rootless", intervals: [4, 7, 10, 13], description: "No root" }
  },
  // Add chord voicings
  "add9": {
    "close": { name: "Close", intervals: [0, 4, 7, 14], description: "Standard" },
    "spread": { name: "Spread", intervals: [0, 7, 14, 16], description: "Open" }
  },
  // 6th chord voicings
  "6": {
    "close": { name: "Close", intervals: [0, 4, 7, 9], description: "Standard" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 9, 16], description: "Open voicing" }
  },
  "m6": {
    "close": { name: "Close", intervals: [0, 3, 7, 9], description: "Standard" },
    "drop2": { name: "Drop 2", intervals: [0, 7, 9, 15], description: "Open voicing" }
  }
};
function getChordIntervals2(quality) {
  const intervals = CHORD_INTERVALS2[quality];
  if (!intervals) {
    throw new Error(`Unknown chord quality: "${quality}"`);
  }
  return [...intervals];
}
function buildChord(root, quality, octave = 3) {
  const intervals = getChordIntervals2(quality);
  const rootPitch = `${root}${octave}`;
  const rootMidi = pitchToMidi(rootPitch);
  return intervals.map((interval) => midiToPitch(rootMidi + interval));
}
function getVoicing(root, quality, voicingName, octave = 3) {
  const qualityVoicings = VOICINGS[quality];
  if (!qualityVoicings) {
    return buildChord(root, quality, octave);
  }
  const voicing = qualityVoicings[voicingName];
  if (!voicing) {
    return buildChord(root, quality, octave);
  }
  const rootPitch = `${root}${octave}`;
  const rootMidi = pitchToMidi(rootPitch);
  return voicing.intervals.map((interval) => midiToPitch(rootMidi + interval));
}
function invertChord(notes, inversion) {
  const result = [...notes];
  const inversionCount = inversion % notes.length;
  for (let i = 0; i < inversionCount; i++) {
    const bottomNote = result.shift();
    const midi = pitchToMidi(bottomNote);
    result.push(midiToPitch(midi + 12));
  }
  return result;
}
function getDiatonicChord(key, degree, seventh = false) {
  const { root, mode } = parseKey(key);
  const majorQualities = seventh ? ["maj7", "m7", "m7", "maj7", "7", "m7", "m7b5"] : ["maj", "min", "min", "maj", "maj", "min", "dim"];
  const minorQualities = seventh ? ["m7", "m7b5", "maj7", "m7", "m7", "maj7", "7"] : ["min", "dim", "maj", "min", "min", "maj", "maj"];
  const qualities = mode === "minor" ? minorQualities : majorQualities;
  const intervals = getScaleIntervals(mode);
  const rootMidi = pitchToMidi(`${root}4`) % 12;
  const degreeIndex = (degree - 1) % 7;
  const chordRootMidi = (rootMidi + intervals[degreeIndex]) % 12;
  const chordRoot = midiToPitch(48 + chordRootMidi).replace(/\d+$/, "");
  return `${chordRoot}${qualities[degreeIndex]}`;
}
function analyzeChordFunction(chordRoot, key) {
  const { root: keyRoot, mode } = parseKey(key);
  const keyRootMidi = pitchToMidi(`${keyRoot}4`) % 12;
  const chordRootMidi = pitchToMidi(`${chordRoot}4`) % 12;
  const interval = (chordRootMidi - keyRootMidi + 12) % 12;
  if (mode === "major") {
    if (interval === 0 || interval === 4 || interval === 9) return "tonic";
    if (interval === 5 || interval === 2) return "subdominant";
    if (interval === 7 || interval === 11) return "dominant";
  } else {
    if (interval === 0 || interval === 3 || interval === 8) return "tonic";
    if (interval === 5 || interval === 2) return "subdominant";
    if (interval === 7 || interval === 11) return "dominant";
  }
  return "unknown";
}
function getProgression(name, key) {
  const degreeProgressions = {
    // Basic pop/rock
    "I-IV-V": [1, 4, 5],
    "I-V-vi-IV": [1, 5, 6, 4],
    // "4 chord song" (pop)
    "I-vi-IV-V": [1, 6, 4, 5],
    // 50s doo-wop
    "I-IV-vi-V": [1, 4, 6, 5],
    "vi-IV-I-V": [6, 4, 1, 5],
    // "Sensitive female chord progression"
    "I-IV-I-V": [1, 4, 1, 5],
    // Basic rock
    // Blues
    "12-bar-blues": [1, 1, 1, 1, 4, 4, 1, 1, 5, 4, 1, 5],
    "8-bar-blues": [1, 1, 4, 4, 5, 4, 1, 5],
    "minor-blues": [1, 1, 1, 1, 4, 4, 1, 1, 6, 5, 1, 5],
    // Jazz basics
    "ii-V-I": [2, 5, 1],
    "ii-V": [2, 5],
    "I-vi-ii-V": [1, 6, 2, 5],
    // Rhythm changes A section
    "iii-vi-ii-V": [3, 6, 2, 5],
    // Extended turnaround
    // Circle progressions
    "circle-of-fourths": [1, 4, 7, 3, 6, 2, 5, 1],
    "circle-of-fifths": [1, 5, 2, 6, 3, 7, 4, 1],
    // Gospel
    "gospel-vamp": [1, 4, 1, 5],
    "gospel-turnaround": [1, 6, 2, 5],
    // Modal
    "dorian-vamp": [1, 4],
    // i-IV (minor key)
    "mixolydian-vamp": [1, 7],
    // I-bVII
    // Coltrane changes (simplified degree representation)
    "giant-steps": [1, 4, 1, 4, 1, 4]
    // Approximation - actual uses key centers
  };
  const degrees = degreeProgressions[name];
  if (degrees) {
    return degrees.map((degree) => getDiatonicChord(key, degree));
  }
  const { root } = parseKey(key);
  const jazzProgressions = {
    // ii-V-I with 7ths
    "ii-V-I-7": (r) => {
      const ii = transposeRoot(r, 2);
      const V = transposeRoot(r, 7);
      return [`${ii}m7`, `${V}7`, `${r}maj7`];
    },
    // Minor ii-V-i
    "ii-V-i": (r) => {
      const ii = transposeRoot(r, 2);
      const V = transposeRoot(r, 7);
      return [`${ii}m7b5`, `${V}7b9`, `${r}m7`];
    },
    // Rhythm changes A section
    "rhythm-a": (r) => {
      const vi = transposeRoot(r, 9);
      const ii = transposeRoot(r, 2);
      const V = transposeRoot(r, 7);
      return [`${r}maj7`, `${vi}m7`, `${ii}m7`, `${V}7`];
    },
    // Autumn Leaves turnaround
    "autumn-leaves": (r) => {
      const iv = transposeRoot(r, 5);
      const VII = transposeRoot(r, 10);
      const III = transposeRoot(r, 3);
      const VI = transposeRoot(r, 8);
      const ii = transposeRoot(r, 2);
      const V = transposeRoot(r, 7);
      return [`${iv}m7`, `${VII}7`, `${III}maj7`, `${VI}maj7`, `${ii}m7b5`, `${V}7`, `${r}m7`];
    },
    // All The Things You Are (simplified)
    "all-the-things": (r) => {
      const vi = transposeRoot(r, 8);
      const ii = transposeRoot(r, 1);
      const V = transposeRoot(r, 6);
      const I = transposeRoot(r, 11);
      return [`${vi}m7`, `${ii}m7`, `${V}7`, `${I}maj7`];
    },
    // Coltrane/Giant Steps pattern
    "coltrane-cycle": (r) => {
      const third = transposeRoot(r, 4);
      const sixth = transposeRoot(r, 8);
      return [`${r}maj7`, `${third}7`, `${sixth}maj7`, `${r}7`];
    },
    // Neo-soul progression
    "neo-soul": (r) => {
      const ii = transposeRoot(r, 2);
      const V = transposeRoot(r, 7);
      const vi = transposeRoot(r, 9);
      return [`${r}maj9`, `${ii}m9`, `${V}13`, `${vi}m9`];
    },
    // Gospel turnaround with extensions
    "gospel-extended": (r) => {
      const IV = transposeRoot(r, 5);
      const vi = transposeRoot(r, 9);
      const V = transposeRoot(r, 7);
      return [`${r}maj9`, `${IV}maj9`, `${vi}m9`, `${V}13sus4`];
    },
    // Backdoor ii-V (bVII7)
    "backdoor": (r) => {
      const iv = transposeRoot(r, 5);
      const bVII = transposeRoot(r, 10);
      return [`${iv}m7`, `${bVII}7`, `${r}maj7`];
    },
    // Tritone substitution
    "tritone-sub": (r) => {
      const ii = transposeRoot(r, 2);
      const bII = transposeRoot(r, 1);
      return [`${ii}m7`, `${bII}7`, `${r}maj7`];
    },
    // Lady Bird turnaround
    "lady-bird": (r) => {
      const bVII = transposeRoot(r, 10);
      const bVI = transposeRoot(r, 8);
      const V = transposeRoot(r, 7);
      return [`${r}maj7`, `${bVII}7`, `${bVI}maj7`, `${V}7`];
    },
    // Tadd Dameron turnaround
    "tadd-dameron": (r) => {
      const bVII = transposeRoot(r, 10);
      const VI = transposeRoot(r, 9);
      const bVI = transposeRoot(r, 8);
      return [`${r}maj7`, `${bVII}7`, `${VI}7`, `${bVI}7`];
    }
  };
  const jazzFn = jazzProgressions[name];
  if (jazzFn) {
    return jazzFn(root);
  }
  throw new Error(`Unknown progression: "${name}". Available: ${[...Object.keys(degreeProgressions), ...Object.keys(jazzProgressions)].join(", ")}`);
}
function transposeRoot(root, semitones) {
  const noteNames = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"];
  const flatToSharp = {
    "Db": "C#",
    "Eb": "D#",
    "Fb": "E",
    "Gb": "F#",
    "Ab": "G#",
    "Bb": "A#",
    "Cb": "B"
  };
  const normalizedRoot = flatToSharp[root] || root;
  const rootIndex = noteNames.indexOf(normalizedRoot);
  if (rootIndex === -1) {
    throw new Error(`Invalid root note: ${root}`);
  }
  const newIndex = (rootIndex + semitones) % 12;
  return noteNames[newIndex];
}
function getAvailableQualities() {
  return Object.keys(CHORD_INTERVALS2);
}

// src/theory/rhythm.ts
function applySwing(beat, swingAmount, division = 0.5) {
  if (swingAmount === 0) return beat;
  const beatInDivision = beat % division;
  const divisionNumber = Math.floor(beat % 1 / division);
  if (divisionNumber % 2 === 1) {
    const maxOffset = division / 3;
    const offset = maxOffset * swingAmount;
    return beat + offset;
  }
  return beat;
}
function humanizeTiming(beat, amount, maxDeviation = HUMANIZE.MAX_TIMING_VARIANCE_BEATS) {
  if (amount === 0) return beat;
  const deviation = (Math.random() * 2 - 1) * maxDeviation * amount;
  return Math.max(0, beat + deviation);
}
function humanizeVelocity(velocity, amount, maxDeviation = HUMANIZE.MAX_VELOCITY_VARIANCE) {
  if (amount === 0) return velocity;
  const deviation = (Math.random() * 2 - 1) * maxDeviation * amount;
  return Math.min(1, Math.max(0, velocity + deviation));
}
function humanizeDuration(duration, amount, maxDeviation = HUMANIZE.MAX_DURATION_VARIANCE) {
  if (amount === 0) return duration;
  const deviation = (Math.random() * 2 - 1) * maxDeviation * amount;
  return Math.max(0.01, duration * (1 + deviation));
}
var GROOVE_TEMPLATES2 = GROOVE_TEMPLATES;
function applyGroove(beat, velocity, groove) {
  const template = typeof groove === "string" ? GROOVE_TEMPLATES2[groove] : groove;
  if (!template) {
    return { beat, velocity };
  }
  const sixteenthInBeat = Math.floor(beat % 1 * 4) % 4;
  const timingOffset = template.timingOffsets[sixteenthInBeat] || 0;
  const velocityMultiplier = template.velocityMultipliers[sixteenthInBeat] || 1;
  return {
    beat: beat + timingOffset,
    velocity: velocity * velocityMultiplier
  };
}
function parseRhythmPattern(pattern, division = 0.5) {
  const result = [];
  let currentBeat = 0;
  for (const char of pattern) {
    if (char === "x" || char === "X") {
      result.push({ beat: currentBeat, accent: false });
      currentBeat += division;
    } else if (char === ">" || char === "!") {
      result.push({ beat: currentBeat, accent: true });
      currentBeat += division;
    } else if (char === "." || char === "-") {
      currentBeat += division;
    } else if (char === " ") {
    }
  }
  return result;
}
function calculateDensity(notes, totalBeats) {
  if (totalBeats === 0) return 0;
  return notes.length / totalBeats;
}
function parseTimeSignature(signature) {
  const match = signature.match(/^(\d+)\/(\d+)$/);
  if (!match) {
    throw new Error(`Invalid time signature: "${signature}"`);
  }
  const numerator = parseInt(match[1], 10);
  const denominator = parseInt(match[2], 10);
  const beatValue = 4 / denominator;
  const beatsPerBar = numerator * beatValue;
  return {
    numerator,
    denominator,
    beatsPerBar,
    beatValue
  };
}
function getBarPosition(beat, timeSignature) {
  const ts = typeof timeSignature === "string" ? parseTimeSignature(timeSignature) : timeSignature;
  const bar = Math.floor(beat / ts.beatsPerBar) + 1;
  const beatInBar = beat % ts.beatsPerBar + 1;
  return { bar, beatInBar };
}
function getAvailableGrooves() {
  return Object.keys(GROOVE_TEMPLATES2);
}

// src/engine/timeline.ts
var TimelineBuilder = class {
  events = [];
  instruments = /* @__PURE__ */ new Set();
  settings;
  currentTempo;
  constructor(settings) {
    this.settings = settings;
    this.currentTempo = settings.tempo;
  }
  /**
   * Add a note event
   * v0.4: Supports optional expression fields via options parameter
   */
  addNote(pitch, startBeat, durationBeats, velocity, instrument, options) {
    this.instruments.add(instrument);
    const timeSeconds = this.beatsToTime(startBeat);
    const durationSeconds = beatsToSeconds(durationBeats, this.currentTempo);
    const event = {
      type: "note",
      time: startBeat,
      timeSeconds,
      pitch,
      duration: durationBeats,
      durationSeconds,
      velocity,
      instrument
    };
    if (options) {
      if (options.timingOffset !== void 0) event.timingOffset = options.timingOffset;
      if (options.probability !== void 0) event.probability = options.probability;
      if (options.portamento !== void 0) event.portamento = options.portamento;
      if (options.humanize !== void 0) event.humanize = options.humanize;
    }
    this.events.push(event);
    return this;
  }
  /**
   * Add multiple notes as a chord
   */
  addChord(pitches, startBeat, durationBeats, velocity, instrument) {
    this.instruments.add(instrument);
    const timeSeconds = this.beatsToTime(startBeat);
    const durationSeconds = beatsToSeconds(durationBeats, this.currentTempo);
    const notes = pitches.map((pitch) => ({
      type: "note",
      time: startBeat,
      timeSeconds,
      pitch,
      duration: durationBeats,
      durationSeconds,
      velocity,
      instrument
    }));
    const event = {
      type: "chord",
      time: startBeat,
      timeSeconds,
      notes
    };
    this.events.push(event);
    return this;
  }
  /**
   * Add a tempo change
   */
  addTempoChange(beat, tempo) {
    const event = {
      type: "tempo",
      time: beat,
      timeSeconds: this.beatsToTime(beat),
      tempo
    };
    this.events.push(event);
    this.currentTempo = tempo;
    return this;
  }
  /**
   * Add a key change
   */
  addKeyChange(beat, key) {
    const event = {
      type: "key",
      time: beat,
      timeSeconds: this.beatsToTime(beat),
      key
    };
    this.events.push(event);
    return this;
  }
  /**
   * Convert beats to seconds considering tempo
   */
  beatsToTime(beats) {
    return beatsToSeconds(beats, this.currentTempo);
  }
  /**
   * Build the final timeline
   */
  build() {
    const sortedEvents = [...this.events].sort((a, b) => a.time - b.time);
    let totalBeats = 0;
    for (const event of sortedEvents) {
      if (event.type === "note") {
        totalBeats = Math.max(totalBeats, event.time + event.duration);
      } else if (event.type === "chord") {
        for (const note of event.notes) {
          totalBeats = Math.max(totalBeats, note.time + note.duration);
        }
      }
    }
    const processedEvents = this.recalculateTimes(sortedEvents);
    const totalSeconds = this.calculateTotalSeconds(processedEvents, totalBeats);
    return {
      events: processedEvents,
      totalBeats,
      totalSeconds,
      instruments: Array.from(this.instruments),
      settings: this.settings
    };
  }
  /**
   * Recalculate times considering tempo changes
   */
  recalculateTimes(events) {
    let currentTempo = this.settings.tempo;
    let currentBeat = 0;
    let currentTime = 0;
    const tempoEvents = events.filter((e) => e.type === "tempo").sort((a, b) => a.time - b.time);
    return events.map((event) => {
      for (const tempoEvent of tempoEvents) {
        if (tempoEvent.time <= event.time && tempoEvent.time > currentBeat) {
          currentTime += beatsToSeconds(tempoEvent.time - currentBeat, currentTempo);
          currentBeat = tempoEvent.time;
          currentTempo = tempoEvent.tempo;
        }
      }
      const beatsSinceLastChange = event.time - currentBeat;
      const timeSeconds = currentTime + beatsToSeconds(beatsSinceLastChange, currentTempo);
      const updated = { ...event, timeSeconds };
      if (event.type === "note") {
        updated.durationSeconds = beatsToSeconds(event.duration, currentTempo);
      } else if (event.type === "chord") {
        updated.notes = event.notes.map((note) => ({
          ...note,
          timeSeconds,
          durationSeconds: beatsToSeconds(note.duration, currentTempo)
        }));
      }
      return updated;
    });
  }
  /**
   * Calculate total seconds for the timeline
   */
  calculateTotalSeconds(events, totalBeats) {
    let maxTime = 0;
    for (const event of events) {
      if (event.type === "note") {
        maxTime = Math.max(maxTime, event.timeSeconds + event.durationSeconds);
      } else if (event.type === "chord") {
        for (const note of event.notes) {
          maxTime = Math.max(maxTime, note.timeSeconds + note.durationSeconds);
        }
      }
    }
    return maxTime;
  }
};
function getAllNotes(timeline) {
  const notes = [];
  for (const event of timeline.events) {
    if (event.type === "note") {
      notes.push(event);
    } else if (event.type === "chord") {
      notes.push(...event.notes);
    }
  }
  return notes.sort((a, b) => a.time - b.time);
}
function filterByInstrument(timeline, instrument) {
  return timeline.events.filter((event) => {
    if (event.type === "note") {
      return event.instrument === instrument;
    }
    if (event.type === "chord") {
      return event.notes.some((note) => note.instrument === instrument);
    }
    return true;
  });
}
function mergeTimelines(timelines) {
  if (timelines.length === 0) {
    throw new Error("Cannot merge empty array of timelines");
  }
  const allEvents = [];
  const allInstruments = /* @__PURE__ */ new Set();
  let maxBeats = 0;
  let maxSeconds = 0;
  for (const timeline of timelines) {
    allEvents.push(...timeline.events);
    timeline.instruments.forEach((i) => allInstruments.add(i));
    maxBeats = Math.max(maxBeats, timeline.totalBeats);
    maxSeconds = Math.max(maxSeconds, timeline.totalSeconds);
  }
  return {
    events: allEvents.sort((a, b) => a.time - b.time),
    totalBeats: maxBeats,
    totalSeconds: maxSeconds,
    instruments: Array.from(allInstruments),
    settings: timelines[0].settings
  };
}
function offsetTimeline(timeline, beatOffset) {
  const offsetEvents = timeline.events.map((event) => {
    const newEvent = { ...event, time: event.time + beatOffset };
    if (event.type === "chord") {
      newEvent.notes = event.notes.map((note) => ({
        ...note,
        time: note.time + beatOffset
      }));
    }
    return newEvent;
  });
  return {
    ...timeline,
    events: offsetEvents,
    totalBeats: timeline.totalBeats + beatOffset
  };
}

// src/generative/density.ts
function getDensityAtPosition(config, position) {
  const { start: start2, end, curve = "linear" } = config;
  const t = Math.max(0, Math.min(1, position));
  let interpolatedT;
  switch (curve) {
    case "linear":
      interpolatedT = t;
      break;
    case "exponential":
      interpolatedT = t === 1 ? 1 : 1 - Math.pow(2, -10 * t);
      break;
    case "logarithmic":
      interpolatedT = t === 0 ? 0 : Math.pow(2, 10 * t - 10);
      break;
    case "sine":
      interpolatedT = -(Math.cos(Math.PI * t) - 1) / 2;
      break;
    default:
      interpolatedT = t;
  }
  const density = start2 + (end - start2) * interpolatedT;
  return Math.max(0, Math.min(1, density));
}
function getDensityAtBeat(config, beat, totalBeats) {
  if (totalBeats <= 0) return config.start;
  const position = beat / totalBeats;
  return getDensityAtPosition(config, position);
}
function applyDensity(baseProbability, density) {
  const prob = baseProbability ?? 1;
  return prob * density;
}
function shouldPlayNote(baseProbability, density, random) {
  const effectiveProbability = applyDensity(baseProbability, density);
  const roll = random ?? Math.random();
  return roll < effectiveProbability;
}

// src/debug/logger.ts
var DebugLogger = class {
  level = 0;
  enabled = false;
  /**
   * Set debug level
   */
  setLevel(level) {
    this.level = level;
    this.enabled = level > 0;
  }
  /**
   * Get current debug level
   */
  getLevel() {
    return this.level;
  }
  /**
   * Check if debug is enabled
   */
  isEnabled() {
    return this.enabled;
  }
  /**
   * Log a message at a specific level
   */
  log(minLevel, message) {
    if (this.level >= minLevel) {
      console.log(`[DEBUG:${minLevel}] ${message}`);
    }
  }
  /**
   * Log compilation start
   */
  compileStart(title, tempo, key) {
    this.log(1, `Compiling '${title}'`);
    this.log(1, `Settings: tempo=${tempo}${key ? `, key=${key}` : ""}`);
  }
  /**
   * Log section start
   */
  sectionStart(name, bars, beatsPerBar) {
    this.log(1, ``);
    this.log(1, `Section '${name}' (${bars} bars = ${bars * beatsPerBar} beats)`);
  }
  /**
   * Log track processing
   */
  trackStart(name, noteCount) {
    this.log(1, `  Track '${name}': ${noteCount} note(s)`);
  }
  /**
   * Log pattern scheduling
   */
  patternSchedule(pattern, beat, totalBeats) {
    this.log(1, `    Pattern '${pattern}' at beat ${beat} (${totalBeats} beats)`);
  }
  /**
   * Log note placement
   */
  notePlace(pitch, beat, duration, velocity, instrument) {
    this.log(2, `      ${pitch} at beat ${beat.toFixed(2)}, duration ${duration.toFixed(2)}, velocity ${velocity.toFixed(2)} [${instrument}]`);
  }
  /**
   * Log pattern completion
   */
  patternComplete(pattern, noteCount, totalBeats) {
    this.log(2, `    Pattern '${pattern}' complete: ${noteCount} notes, ${totalBeats} beats`);
  }
  /**
   * Log section completion
   */
  sectionComplete(name, noteCount) {
    this.log(1, `  Section '${name}' complete: ${noteCount} notes`);
  }
  /**
   * Log compilation summary
   */
  compileSummary(totalNotes, totalSections, durationSeconds) {
    this.log(1, ``);
    this.log(1, `Compilation complete:`);
    this.log(1, `  Total notes: ${totalNotes}`);
    this.log(1, `  Sections: ${totalSections}`);
    this.log(1, `  Duration: ${formatTime(durationSeconds)}`);
  }
  /**
   * Log a warning
   */
  warn(message) {
    this.log(1, `\u26A0 ${message}`);
  }
  /**
   * Log timing mismatch
   */
  timingMismatch(section, track, trackBeats, sectionBeats) {
    this.log(1, `  \u26A0 Track '${track}' in section '${section}': ${trackBeats} beats vs section ${sectionBeats} beats`);
  }
  /**
   * Log fill operation
   */
  fillToLength(originalBeats, targetBeats) {
    this.log(2, `    Filling pattern: ${originalBeats} beats \u2192 ${targetBeats} beats`);
  }
  /**
   * Log humanization
   */
  humanize(amount) {
    this.log(3, `      Applying humanization: ${amount}`);
  }
  /**
   * Log swing
   */
  swing(amount) {
    this.log(3, `      Applying swing: ${amount}`);
  }
};
function formatTime(seconds) {
  const mins = Math.floor(seconds / 60);
  const secs = Math.floor(seconds % 60);
  return `${mins}:${secs.toString().padStart(2, "0")}`;
}
var debug = new DebugLogger();

// src/engine/pattern-resolver.ts
function resolveExpressionSettings(track, swing) {
  let humanize = track.humanize || 0;
  let groove = track.groove;
  let velocityVariance = 0;
  if (track.expression && EXPRESSION_PRESETS[track.expression]) {
    const preset = EXPRESSION_PRESETS[track.expression];
    humanize = preset.humanize;
    groove = preset.groove;
    velocityVariance = preset.velocityVariance;
    if (track.humanize !== void 0) {
      humanize = track.humanize;
    }
    if (track.groove !== void 0) {
      groove = track.groove;
    }
  }
  return {
    humanize,
    swing,
    groove,
    velocityVariance
  };
}
function resolveTrack(track, ctx) {
  if (track.mute) {
    return [];
  }
  if (track.probability !== void 0) {
    if (Math.random() > track.probability) {
      if (track.fallback) {
        return resolveTrack({ ...track, pattern: track.fallback, probability: void 0, fallback: void 0 }, ctx);
      }
      return [];
    }
  }
  if (track.parallel && track.parallel.length > 0) {
    return resolveParallelPatterns(track, ctx);
  }
  const patternNames = track.patterns || (track.pattern ? [track.pattern] : []);
  if (patternNames.length === 0) {
    return [];
  }
  const results = [];
  const repeatCount = track.repeat || 1;
  const processOptions = resolveExpressionSettings(track, ctx.settings.swing || 0);
  let cumulativeBeat = 0;
  for (let r = 0; r < repeatCount; r++) {
    for (let i = 0; i < patternNames.length; i++) {
      const patternName = patternNames[i];
      const pattern = ctx.patterns[patternName];
      if (!pattern) {
        console.warn(`Pattern not found: ${patternName}`);
        continue;
      }
      const patternCtx = {
        key: ctx.sectionKey || ctx.settings.key,
        tempo: ctx.sectionTempo || ctx.settings.tempo,
        velocity: track.velocity,
        octaveOffset: track.octave,
        transpose: track.transpose
      };
      const expanded = expandPattern(pattern, patternCtx);
      const currentBeat = cumulativeBeat;
      const actualPatternLength = expanded.totalBeats;
      debug.patternSchedule(patternName, currentBeat, actualPatternLength);
      const processedNotes = processExpandedNotes(
        { notes: expanded.notes, totalBeats: actualPatternLength },
        currentBeat,
        processOptions
      );
      results.push(...processedNotes);
      debug.patternComplete(patternName, processedNotes.length, actualPatternLength);
      cumulativeBeat += actualPatternLength;
    }
  }
  return results;
}
function resolveParallelPatterns(track, ctx) {
  const results = [];
  const repeatCount = track.repeat || 1;
  const processOptions = resolveExpressionSettings(track, ctx.settings.swing || 0);
  const expandedPatterns = [];
  let maxPatternLength = 0;
  for (const patternName of track.parallel) {
    const pattern = ctx.patterns[patternName];
    if (!pattern) {
      console.warn(`Pattern not found: ${patternName}`);
      continue;
    }
    const patternCtx = {
      key: ctx.sectionKey || ctx.settings.key,
      tempo: ctx.sectionTempo || ctx.settings.tempo,
      velocity: track.velocity,
      octaveOffset: track.octave,
      transpose: track.transpose
    };
    const expanded = expandPattern(pattern, patternCtx);
    expandedPatterns.push({ expanded, patternCtx });
    if (expanded.totalBeats > maxPatternLength) {
      maxPatternLength = expanded.totalBeats;
    }
  }
  maxPatternLength = Math.ceil(maxPatternLength / 4) * 4;
  for (let r = 0; r < repeatCount; r++) {
    const repeatOffset = r * maxPatternLength;
    for (const { expanded } of expandedPatterns) {
      const processedNotes = processExpandedNotes(
        expanded,
        repeatOffset,
        processOptions
      );
      results.push(...processedNotes);
    }
  }
  return results;
}
function processExpandedNotes(expanded, beatOffset, options) {
  const { humanize, swing, groove, velocityVariance = 0 } = options;
  return expanded.notes.map((note) => {
    let startBeat = note.startBeat + beatOffset;
    let velocity = note.velocity;
    let durationBeats = note.durationBeats;
    if (groove) {
      const grooved = applyGroove(startBeat, velocity, groove);
      startBeat = grooved.beat;
      velocity = grooved.velocity;
    }
    if (swing > 0) {
      debug.swing(swing);
      startBeat = applySwing(startBeat, swing);
    }
    if (humanize > 0) {
      debug.humanize(humanize);
      startBeat = humanizeTiming(startBeat, humanize);
      velocity = humanizeVelocity(velocity, humanize);
      durationBeats = humanizeDuration(durationBeats, humanize);
    }
    if (velocityVariance > 0) {
      const variance = (Math.random() * 2 - 1) * velocityVariance;
      velocity = velocity * (1 + variance);
    }
    const result = {
      pitch: note.pitch,
      startBeat: Math.max(0, startBeat),
      durationBeats,
      velocity: Math.min(1, Math.max(0, velocity))
    };
    if (note.timingOffset !== void 0) result.timingOffset = note.timingOffset;
    if (note.probability !== void 0) result.probability = note.probability;
    if (note.portamento !== void 0) result.portamento = note.portamento;
    if (humanize > 0) result.humanize = humanize;
    return result;
  });
}
function applyVelocityAutomation(notes, config, sectionBeats) {
  const { start: start2, end, curve = "linear" } = config;
  return notes.map((note) => {
    const progress = sectionBeats > 0 ? note.startBeat / sectionBeats : 0;
    let interpolatedValue;
    switch (curve) {
      case "exponential":
        interpolatedValue = start2 + (end - start2) * (progress * progress);
        break;
      case "sine":
        interpolatedValue = start2 + (end - start2) * (0.5 - Math.cos(progress * Math.PI) / 2);
        break;
      case "step":
        interpolatedValue = progress < 0.5 ? start2 : end;
        break;
      case "linear":
      default:
        interpolatedValue = start2 + (end - start2) * progress;
        break;
    }
    return {
      ...note,
      velocity: Math.min(1, Math.max(0, note.velocity * interpolatedValue))
    };
  });
}
function resolveSection(tracks, bars, ctx) {
  const result = /* @__PURE__ */ new Map();
  const beatsPerBar = getBeatsPerBar(ctx.settings.timeSignature || "4/4");
  const sectionBeats = bars * beatsPerBar;
  const enrichedCtx = {
    ...ctx,
    sectionBeats
  };
  for (const [instrumentName, track] of Object.entries(tracks)) {
    let notes = resolveTrack(track, enrichedCtx);
    notes = fillToLength(notes, sectionBeats);
    if (ctx.density) {
      notes = applyDensityCurve(notes, ctx.density, sectionBeats);
    }
    if (track.velocityAutomation) {
      notes = applyVelocityAutomation(notes, track.velocityAutomation, sectionBeats);
    }
    result.set(instrumentName, notes);
  }
  return result;
}
function applyDensityCurve(notes, density, sectionBeats) {
  return notes.filter((note) => {
    const densityAtBeat = getDensityAtBeat(density, note.startBeat, sectionBeats);
    return shouldPlayNote(note.probability, densityAtBeat);
  });
}
function getBeatsPerBar(timeSignature) {
  const match = timeSignature.match(/^(\d+)\/(\d+)$/);
  if (!match) return 4;
  const numerator = parseInt(match[1], 10);
  const denominator = parseInt(match[2], 10);
  return numerator * (4 / denominator);
}
function fillToLength(notes, targetBeats) {
  if (notes.length === 0) return [];
  const rawPatternLength = Math.max(...notes.map((n) => n.startBeat + n.durationBeats), 1);
  const patternLength = Math.ceil(rawPatternLength / 4) * 4;
  if (patternLength >= targetBeats) {
    return notes.filter((n) => n.startBeat < targetBeats);
  }
  debug.fillToLength(patternLength, targetBeats);
  const result = [];
  let offset = 0;
  while (offset < targetBeats) {
    for (const note of notes) {
      const newStart = note.startBeat + offset;
      if (newStart >= targetBeats) break;
      result.push({
        ...note,
        startBeat: newStart
      });
    }
    offset += patternLength;
  }
  return result;
}
function quantizeNotes(notes, gridSize, strength = 1) {
  return notes.map((note) => {
    const quantizedStart = Math.round(note.startBeat / gridSize) * gridSize;
    const diff = quantizedStart - note.startBeat;
    return {
      ...note,
      startBeat: note.startBeat + diff * strength
    };
  });
}
function transposeNotes(notes, semitones) {
  if (semitones === 0) return notes;
  return notes.map((note) => {
    const match = note.pitch.match(/^([A-G][#b]?)(-?\d+)$/);
    if (!match) return note;
    const [, noteName, octaveStr] = match;
    const octave = parseInt(octaveStr, 10);
    const noteOrder = ["C", "C#", "D", "D#", "E", "F", "F#", "G", "G#", "A", "A#", "B"];
    const flatToSharp = {
      "Db": "C#",
      "Eb": "D#",
      "Fb": "E",
      "Gb": "F#",
      "Ab": "G#",
      "Bb": "A#",
      "Cb": "B"
    };
    const normalizedNote = flatToSharp[noteName] || noteName;
    const currentIndex = noteOrder.indexOf(normalizedNote);
    if (currentIndex === -1) return note;
    const newIndex = (currentIndex + semitones % 12 + 12) % 12;
    const octaveChange = Math.floor((currentIndex + semitones) / 12);
    const newNote = noteOrder[newIndex];
    const newOctave = octave + octaveChange;
    return {
      ...note,
      pitch: `${newNote}${newOctave}`
    };
  });
}

// src/engine/compiler.ts
function compile(score, options = {}) {
  const warnings = [];
  debug.compileStart(
    score.meta?.title || "Untitled",
    score.settings.tempo,
    score.settings.key
  );
  for (const sectionName of score.arrangement) {
    if (!score.sections[sectionName]) {
      warnings.push(`Section "${sectionName}" in arrangement not found`);
      debug.warn(`Section '${sectionName}' in arrangement not found`);
    }
  }
  const sectionsToCompile = getSectionsToCompile(score, options);
  const settings = {
    ...score.settings,
    tempo: options.tempo || score.settings.tempo,
    key: options.key || score.settings.key
  };
  const builder = new TimelineBuilder(settings);
  const timeSignature = settings.timeSignature || "4/4";
  const ts = parseTimeSignature(timeSignature);
  const beatsPerBar = ts.beatsPerBar;
  let currentBeat = 0;
  let totalBars = 0;
  let totalNotes = 0;
  for (const sectionName of sectionsToCompile) {
    const section = score.sections[sectionName];
    if (!section) {
      continue;
    }
    debug.sectionStart(sectionName, section.bars, beatsPerBar);
    const sectionTempo = section.tempo || settings.tempo;
    const sectionKey = section.key || settings.key;
    if (section.tempo && section.tempo !== settings.tempo) {
      builder.addTempoChange(currentBeat, section.tempo);
    }
    if (section.key && section.key !== settings.key) {
      builder.addKeyChange(currentBeat, section.key);
    }
    const ctx = {
      patterns: score.patterns,
      settings,
      sectionKey,
      sectionTempo,
      // v0.6: Pass density curve if specified
      density: section.density
    };
    const resolvedTracks = resolveSection(section.tracks, section.bars, ctx);
    let sectionNotes = 0;
    for (const [instrumentName, notes] of resolvedTracks) {
      debug.trackStart(instrumentName, notes.length);
      for (const note of notes) {
        debug.notePlace(note.pitch, currentBeat + note.startBeat, note.durationBeats, note.velocity, instrumentName);
        const options2 = note.timingOffset !== void 0 || note.probability !== void 0 || note.portamento !== void 0 || note.humanize !== void 0 ? {
          timingOffset: note.timingOffset,
          probability: note.probability,
          portamento: note.portamento,
          humanize: note.humanize
        } : void 0;
        builder.addNote(
          note.pitch,
          currentBeat + note.startBeat,
          note.durationBeats,
          note.velocity,
          instrumentName,
          options2
        );
        totalNotes++;
        sectionNotes++;
      }
    }
    debug.sectionComplete(sectionName, sectionNotes);
    currentBeat += section.bars * beatsPerBar;
    totalBars += section.bars;
  }
  const timeline = builder.build();
  const stats = {
    totalSections: sectionsToCompile.length,
    totalBars,
    totalNotes,
    instruments: timeline.instruments,
    durationSeconds: timeline.totalSeconds
  };
  debug.compileSummary(totalNotes, sectionsToCompile.length, timeline.totalSeconds);
  return { timeline, warnings, stats };
}
function getSectionsToCompile(score, options) {
  const arrangement = score.arrangement;
  let startIndex = 0;
  let endIndex = arrangement.length;
  if (options.startSection) {
    const idx = arrangement.indexOf(options.startSection);
    if (idx !== -1) {
      startIndex = idx;
    }
  }
  if (options.endSection) {
    const idx = arrangement.indexOf(options.endSection);
    if (idx !== -1) {
      endIndex = idx + 1;
    }
  }
  return arrangement.slice(startIndex, endIndex);
}
function analyze(score) {
  const timeSignature = score.settings.timeSignature || "4/4";
  const ts = parseTimeSignature(timeSignature);
  let totalBars = 0;
  const sections = [];
  const allInstruments = /* @__PURE__ */ new Set();
  for (const sectionName of score.arrangement) {
    const section = score.sections[sectionName];
    if (section) {
      totalBars += section.bars;
      const instruments = Object.keys(section.tracks);
      instruments.forEach((i) => allInstruments.add(i));
      sections.push({
        name: sectionName,
        bars: section.bars,
        instruments
      });
    }
  }
  const totalBeats = totalBars * ts.beatsPerBar;
  const durationSeconds = totalBeats / score.settings.tempo * 60;
  return {
    totalSections: score.arrangement.length,
    totalBars,
    totalNotes: 0,
    // Would need full compilation
    instruments: Array.from(allInstruments),
    durationSeconds,
    sections,
    patterns: Object.keys(score.patterns)
  };
}
function validateScore(score) {
  const errors2 = [];
  const isComment = (key) => key.startsWith("//");
  for (const sectionName of score.arrangement) {
    if (!score.sections[sectionName]) {
      errors2.push(`Arrangement references unknown section: "${sectionName}"`);
    }
  }
  for (const [sectionName, section] of Object.entries(score.sections)) {
    if (isComment(sectionName)) continue;
    if (!section || typeof section !== "object" || !section.tracks) continue;
    for (const [trackName, track] of Object.entries(section.tracks)) {
      if (isComment(trackName)) continue;
      if (!track || typeof track !== "object") continue;
      if (track.pattern && !score.patterns[track.pattern]) {
        errors2.push(`Section "${sectionName}" track "${trackName}" references unknown pattern: "${track.pattern}"`);
      }
      if (track.patterns) {
        for (const patternName of track.patterns) {
          if (!score.patterns[patternName]) {
            errors2.push(`Section "${sectionName}" track "${trackName}" references unknown pattern: "${patternName}"`);
          }
        }
      }
    }
  }
  if (score.instruments) {
    const instrumentNames = new Set(Object.keys(score.instruments).filter((k) => !isComment(k)));
    for (const [sectionName, section] of Object.entries(score.sections)) {
      if (isComment(sectionName)) continue;
      if (!section || typeof section !== "object" || !section.tracks) continue;
      for (const trackName of Object.keys(section.tracks)) {
        if (isComment(trackName)) continue;
        if (!instrumentNames.has(trackName)) {
          errors2.push(`Section "${sectionName}" has track "${trackName}" with no matching instrument`);
        }
      }
    }
  }
  return errors2;
}
function createSimpleScore(patterns, bars = 4, tempo = 120, key = "C major") {
  const etherPatterns = {};
  const tracks = {};
  let i = 0;
  for (const [name, notes] of Object.entries(patterns)) {
    const patternName = name || `pattern_${i}`;
    etherPatterns[patternName] = { notes };
    tracks[patternName] = { pattern: patternName };
    i++;
  }
  return {
    settings: { tempo, key },
    patterns: etherPatterns,
    sections: {
      main: { bars, tracks }
    },
    arrangement: ["main"]
  };
}

// src/engine/automation.ts
function parseAutomationPath(path) {
  if (path === "tempo") {
    return {
      instrument: "_global",
      target: "tempo"
    };
  }
  const parts = path.split(".");
  if (parts.length < 2) {
    console.warn(`Invalid automation path: ${path}`);
    return null;
  }
  const instrument = parts[0];
  if (parts[1] === "params" && parts.length >= 3) {
    return {
      instrument,
      target: "params",
      paramName: parts[2]
    };
  }
  if (parts.length === 2 && (parts[1] === "volume" || parts[1] === "pan")) {
    return {
      instrument,
      target: "channel",
      channelParam: parts[1]
    };
  }
  if (parts.length >= 3) {
    return {
      instrument,
      target: "effects",
      effectType: parts[1],
      effectParam: parts[2]
    };
  }
  console.warn(`Could not parse automation path: ${path}`);
  return null;
}
var SEMANTIC_TO_TONEJS = {
  // Timbre (FM-specific)
  "brightness": {
    param: "modulationIndex",
    transform: (v) => v * 20
    // 0-1  0-20
  },
  "warmth": {
    param: "harmonicity",
    transform: (v) => 0.5 + (1 - v) * 7.5
    // 0-1  8.0-0.5 (inverted)
  },
  // Envelope
  "attack": {
    param: "envelope.attack",
    transform: (v) => 1e-3 + v * 1.999
    // 0-1  0.001-2.0
  },
  "decay": {
    param: "envelope.decay",
    transform: (v) => 0.05 + v * 3.95
    // 0-1  0.05-4.0
  },
  "sustain": {
    param: "envelope.sustain",
    transform: (v) => v
    // Direct 0-1 mapping
  },
  "release": {
    param: "envelope.release",
    transform: (v) => 0.1 + v * 3.9
    // 0-1  0.1-4.0
  }
};
function resolveSemanticParam(paramName, startValue, endValue) {
  const mapping = SEMANTIC_TO_TONEJS[paramName];
  if (!mapping) {
    console.warn(`Unknown semantic parameter: ${paramName}`);
    return null;
  }
  return {
    toneParam: mapping.param,
    startValue: mapping.transform(startValue),
    endValue: mapping.transform(endValue)
  };
}
function getAutomationValue(config, normalizedTime) {
  if (config.points && config.points.length > 0) {
    return interpolatePoints(config.points, normalizedTime);
  }
  const curve = config.curve || "linear";
  return interpolateCurve(config.start, config.end, normalizedTime, curve);
}
function interpolatePoints(points, t) {
  const sorted = [...points].sort((a, b) => a.time - b.time);
  let lower = sorted[0];
  let upper = sorted[sorted.length - 1];
  for (let i = 0; i < sorted.length - 1; i++) {
    if (t >= sorted[i].time && t <= sorted[i + 1].time) {
      lower = sorted[i];
      upper = sorted[i + 1];
      break;
    }
  }
  if (t <= lower.time) return lower.value;
  if (t >= upper.time) return upper.value;
  const localT = (t - lower.time) / (upper.time - lower.time);
  return lower.value + localT * (upper.value - lower.value);
}
function interpolateCurve(start2, end, t, curve) {
  switch (curve) {
    case "linear":
      return start2 + t * (end - start2);
    case "exponential":
      const safeStart = Math.max(1e-3, start2);
      const safeEnd = Math.max(1e-3, end);
      return safeStart * Math.pow(safeEnd / safeStart, t);
    case "sine":
      const sinT = 0.5 - 0.5 * Math.cos(t * Math.PI);
      return start2 + sinT * (end - start2);
    case "step":
      return t < 0.5 ? start2 : end;
    default:
      return start2 + t * (end - start2);
  }
}
function resolveAutomations(section, sectionDuration) {
  if (!section.automation) {
    return [];
  }
  const results = [];
  for (const [pathStr, config] of Object.entries(section.automation)) {
    const path = parseAutomationPath(pathStr);
    if (!path) {
      continue;
    }
    results.push({
      instrument: path.instrument,
      path,
      config,
      startTime: 0,
      // Automation starts at section start
      duration: sectionDuration
    });
  }
  return results;
}
function generateAutomationEvents(automation, resolution = 20) {
  const events = [];
  const { config, duration } = automation;
  for (let i = 0; i <= resolution; i++) {
    const normalizedTime = i / resolution;
    const value = getAutomationValue(config, normalizedTime);
    const time = normalizedTime * duration;
    events.push({ time, value });
  }
  return events;
}

// src/synthesis/drum-kits.ts
var KIT_808 = {
  name: "808",
  description: "Classic Roland TR-808 sounds - deep kicks, snappy snares",
  drums: {
    kick: {
      type: "membrane",
      pitch: "C2",
      pitchDecay: 5e-3,
      octaves: 14,
      decay: 0.8,
      attack: 5e-4,
      sustain: 0,
      release: 0.3,
      volume: 6
    },
    snare: {
      type: "noise",
      noiseType: "white",
      decay: 0.25,
      attack: 1e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    clap: {
      type: "noise",
      noiseType: "white",
      decay: 0.18,
      attack: 1e-3,
      sustain: 0,
      release: 0.08,
      volume: 0
    },
    hihat: {
      type: "metal",
      frequency: 6e3,
      harmonicity: 5.1,
      modulationIndex: 32,
      resonance: 4e3,
      decay: 0.03,
      attack: 1e-3,
      sustain: 0,
      release: 0.01,
      volume: -6
    },
    hihat_open: {
      type: "metal",
      frequency: 6e3,
      harmonicity: 5.1,
      modulationIndex: 32,
      resonance: 4e3,
      decay: 0.25,
      attack: 1e-3,
      sustain: 0.08,
      release: 0.1,
      volume: -6
    },
    tom_hi: {
      type: "membrane",
      pitch: "G2",
      pitchDecay: 8e-3,
      octaves: 8,
      decay: 0.3,
      attack: 1e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    tom_mid: {
      type: "membrane",
      pitch: "D2",
      pitchDecay: 8e-3,
      octaves: 8,
      decay: 0.35,
      attack: 1e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    tom_lo: {
      type: "membrane",
      pitch: "A1",
      pitchDecay: 8e-3,
      octaves: 8,
      decay: 0.4,
      attack: 1e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    cowbell: {
      type: "metal",
      frequency: 800,
      harmonicity: 1.5,
      modulationIndex: 2,
      resonance: 2e3,
      decay: 0.15,
      attack: 1e-3,
      sustain: 0,
      release: 0.1,
      volume: -6
    },
    rim: {
      type: "noise",
      noiseType: "white",
      decay: 0.04,
      attack: 1e-3,
      sustain: 0,
      release: 0.02,
      volume: 0
    },
    crash: {
      type: "metal",
      frequency: 4e3,
      harmonicity: 8,
      modulationIndex: 40,
      resonance: 5e3,
      decay: 1.5,
      attack: 1e-3,
      sustain: 0.1,
      release: 0.5,
      volume: -6
    },
    ride: {
      type: "metal",
      frequency: 5e3,
      harmonicity: 7,
      modulationIndex: 20,
      resonance: 3e3,
      decay: 0.8,
      attack: 1e-3,
      sustain: 0.05,
      release: 0.3,
      volume: -9
    },
    shaker: {
      type: "noise",
      noiseType: "white",
      decay: 0.06,
      attack: 1e-3,
      sustain: 0,
      release: 0.03,
      volume: -6
    }
  }
};
var KIT_909 = {
  name: "909",
  description: "Classic Roland TR-909 sounds - punchy kicks, snappy snares",
  drums: {
    kick: {
      type: "membrane",
      pitch: "D1",
      // Lower base pitch for thump
      pitchDecay: 0.015,
      // Faster pitch decay = tighter
      octaves: 3,
      // Reduced from 12 - no more laser!
      decay: 0.2,
      // Slightly shorter body
      attack: 1e-3,
      sustain: 0,
      release: 0.08,
      volume: 6
    },
    snare: {
      type: "noise",
      noiseType: "pink",
      decay: 0.12,
      attack: 1e-3,
      sustain: 0,
      release: 0.04,
      volume: 0
    },
    clap: {
      type: "noise",
      noiseType: "white",
      decay: 0.1,
      attack: 1e-3,
      sustain: 0,
      release: 0.04,
      volume: 0
    },
    hihat: {
      type: "metal",
      frequency: 8e3,
      harmonicity: 5.1,
      modulationIndex: 40,
      resonance: 6e3,
      decay: 0.04,
      attack: 1e-3,
      sustain: 0,
      release: 0.01,
      volume: -6
    },
    hihat_open: {
      type: "metal",
      frequency: 8e3,
      harmonicity: 5.1,
      modulationIndex: 40,
      resonance: 6e3,
      decay: 0.18,
      attack: 1e-3,
      sustain: 0.04,
      release: 0.08,
      volume: -6
    },
    tom_hi: {
      type: "membrane",
      pitch: "A2",
      pitchDecay: 6e-3,
      octaves: 10,
      decay: 0.18,
      attack: 1e-3,
      sustain: 0,
      release: 0.08,
      volume: 0
    },
    tom_mid: {
      type: "membrane",
      pitch: "E2",
      pitchDecay: 6e-3,
      octaves: 10,
      decay: 0.22,
      attack: 1e-3,
      sustain: 0,
      release: 0.08,
      volume: 0
    },
    tom_lo: {
      type: "membrane",
      pitch: "B1",
      pitchDecay: 6e-3,
      octaves: 10,
      decay: 0.26,
      attack: 1e-3,
      sustain: 0,
      release: 0.08,
      volume: 0
    },
    cowbell: {
      type: "metal",
      frequency: 560,
      harmonicity: 1,
      modulationIndex: 2,
      resonance: 1500,
      decay: 0.1,
      attack: 1e-3,
      sustain: 0,
      release: 0.06,
      volume: -6
    },
    rim: {
      type: "noise",
      noiseType: "white",
      decay: 0.04,
      attack: 1e-3,
      sustain: 0,
      release: 0.02,
      volume: 0
    },
    crash: {
      type: "metal",
      frequency: 5e3,
      harmonicity: 9,
      modulationIndex: 45,
      resonance: 6e3,
      decay: 1.8,
      attack: 1e-3,
      sustain: 0.1,
      release: 0.6,
      volume: -6
    },
    ride: {
      type: "metal",
      frequency: 6e3,
      harmonicity: 6,
      modulationIndex: 25,
      resonance: 4e3,
      decay: 0.5,
      attack: 1e-3,
      sustain: 0.04,
      release: 0.2,
      volume: -9
    },
    shaker: {
      type: "noise",
      noiseType: "pink",
      decay: 0.05,
      attack: 1e-3,
      sustain: 0,
      release: 0.025,
      volume: -6
    }
  }
};
var KIT_ACOUSTIC = {
  name: "acoustic",
  description: "Natural acoustic drum sounds",
  drums: {
    kick: {
      type: "membrane",
      pitch: "E2",
      pitchDecay: 0.012,
      octaves: 10,
      decay: 0.35,
      attack: 2e-3,
      sustain: 0,
      release: 0.12,
      volume: 3
    },
    snare: {
      type: "noise",
      noiseType: "pink",
      decay: 0.14,
      attack: 2e-3,
      sustain: 0,
      release: 0.06,
      volume: 0
    },
    clap: {
      type: "noise",
      noiseType: "white",
      decay: 0.1,
      attack: 3e-3,
      sustain: 0,
      release: 0.04,
      volume: 0
    },
    hihat: {
      type: "metal",
      frequency: 9e3,
      harmonicity: 4,
      modulationIndex: 25,
      resonance: 7e3,
      decay: 0.035,
      attack: 2e-3,
      sustain: 0,
      release: 0.015,
      volume: -9
    },
    hihat_open: {
      type: "metal",
      frequency: 9e3,
      harmonicity: 4,
      modulationIndex: 25,
      resonance: 7e3,
      decay: 0.3,
      attack: 2e-3,
      sustain: 0.08,
      release: 0.12,
      volume: -9
    },
    tom_hi: {
      type: "membrane",
      pitch: "G2",
      pitchDecay: 0.01,
      octaves: 8,
      decay: 0.25,
      attack: 3e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    tom_mid: {
      type: "membrane",
      pitch: "D2",
      pitchDecay: 0.01,
      octaves: 8,
      decay: 0.3,
      attack: 3e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    tom_lo: {
      type: "membrane",
      pitch: "G1",
      pitchDecay: 0.01,
      octaves: 8,
      decay: 0.35,
      attack: 3e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    cowbell: {
      type: "metal",
      frequency: 700,
      harmonicity: 1.2,
      modulationIndex: 3,
      resonance: 1800,
      decay: 0.18,
      attack: 2e-3,
      sustain: 0,
      release: 0.08,
      volume: -6
    },
    rim: {
      type: "noise",
      noiseType: "white",
      decay: 0.012,
      attack: 1e-3,
      sustain: 0,
      release: 8e-3,
      volume: -3
    },
    crash: {
      type: "metal",
      frequency: 4500,
      harmonicity: 6,
      modulationIndex: 30,
      resonance: 5500,
      decay: 2,
      attack: 3e-3,
      sustain: 0.15,
      release: 0.8,
      volume: -6
    },
    ride: {
      type: "metal",
      frequency: 5500,
      harmonicity: 5,
      modulationIndex: 18,
      resonance: 4500,
      decay: 0.9,
      attack: 2e-3,
      sustain: 0.06,
      release: 0.35,
      volume: -9
    },
    shaker: {
      type: "noise",
      noiseType: "brown",
      decay: 0.06,
      attack: 3e-3,
      sustain: 0,
      release: 0.03,
      volume: -6
    }
  }
};
var KIT_LOFI = {
  name: "lofi",
  description: "Lo-fi, vintage, dusty drum sounds",
  drums: {
    kick: {
      type: "membrane",
      pitch: "D2",
      pitchDecay: 0.015,
      octaves: 9,
      decay: 0.45,
      attack: 5e-3,
      sustain: 0,
      release: 0.12,
      volume: 3
    },
    snare: {
      type: "noise",
      noiseType: "brown",
      decay: 0.18,
      attack: 4e-3,
      sustain: 0,
      release: 0.08,
      volume: 0
    },
    clap: {
      type: "noise",
      noiseType: "pink",
      decay: 0.14,
      attack: 6e-3,
      sustain: 0,
      release: 0.05,
      volume: 0
    },
    hihat: {
      type: "metal",
      frequency: 5e3,
      harmonicity: 4.5,
      modulationIndex: 20,
      resonance: 3500,
      decay: 0.05,
      attack: 4e-3,
      sustain: 0,
      release: 0.025,
      volume: -9
    },
    hihat_open: {
      type: "metal",
      frequency: 5e3,
      harmonicity: 4.5,
      modulationIndex: 20,
      resonance: 3500,
      decay: 0.28,
      attack: 4e-3,
      sustain: 0.06,
      release: 0.1,
      volume: -9
    },
    tom_hi: {
      type: "membrane",
      pitch: "F2",
      pitchDecay: 0.012,
      octaves: 7,
      decay: 0.3,
      attack: 6e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    tom_mid: {
      type: "membrane",
      pitch: "C2",
      pitchDecay: 0.012,
      octaves: 7,
      decay: 0.35,
      attack: 6e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    tom_lo: {
      type: "membrane",
      pitch: "F1",
      pitchDecay: 0.012,
      octaves: 7,
      decay: 0.4,
      attack: 6e-3,
      sustain: 0,
      release: 0.1,
      volume: 0
    },
    cowbell: {
      type: "metal",
      frequency: 650,
      harmonicity: 1.3,
      modulationIndex: 2.5,
      resonance: 1400,
      decay: 0.22,
      attack: 4e-3,
      sustain: 0,
      release: 0.1,
      volume: -6
    },
    rim: {
      type: "noise",
      noiseType: "pink",
      decay: 0.05,
      attack: 1e-3,
      sustain: 0,
      release: 0.02,
      volume: 0
    },
    crash: {
      type: "metal",
      frequency: 3500,
      harmonicity: 5,
      modulationIndex: 25,
      resonance: 4e3,
      decay: 1.6,
      attack: 6e-3,
      sustain: 0.08,
      release: 0.5,
      volume: -6
    },
    ride: {
      type: "metal",
      frequency: 4500,
      harmonicity: 4.5,
      modulationIndex: 15,
      resonance: 3e3,
      decay: 0.8,
      attack: 4e-3,
      sustain: 0.05,
      release: 0.3,
      volume: -9
    },
    shaker: {
      type: "noise",
      noiseType: "brown",
      decay: 0.07,
      attack: 5e-3,
      sustain: 0,
      release: 0.04,
      volume: -6
    }
  }
};
var DRUM_KITS = {
  "808": KIT_808,
  "909": KIT_909,
  "acoustic": KIT_ACOUSTIC,
  "lofi": KIT_LOFI
};
var DRUM_ALIASES = {
  // Open hi-hat variations
  "openhat": "hihat_open",
  "open_hat": "hihat_open",
  "open_hihat": "hihat_open",
  "oh": "hihat_open",
  // Closed hi-hat variations
  "closedhat": "hihat",
  "closed_hat": "hihat",
  "closed_hihat": "hihat",
  "ch": "hihat",
  "hh": "hihat",
  // Other common aliases
  "bd": "kick",
  "bassdrum": "kick",
  "bass_drum": "kick",
  "sd": "snare",
  "rimshot": "rim",
  "tomhi": "tom_hi",
  "tommid": "tom_mid",
  "tomlo": "tom_lo",
  "tom_high": "tom_hi",
  "tom_low": "tom_lo",
  "cy": "crash",
  "cymbal": "crash",
  "rd": "ride",
  "cb": "cowbell",
  "sh": "shaker",
  "cp": "clap",
  "handclap": "clap"
};
function normalizeDrumName(name) {
  const lower = name.toLowerCase();
  const validDrums = ["kick", "snare", "clap", "hihat", "hihat_open", "tom_hi", "tom_mid", "tom_lo", "crash", "ride", "rim", "cowbell", "shaker"];
  if (validDrums.includes(lower)) {
    return lower;
  }
  return DRUM_ALIASES[lower] || lower;
}
function getDrumParams(kit, drum) {
  const normalizedDrum = normalizeDrumName(drum);
  return DRUM_KITS[kit]?.drums[normalizedDrum];
}
function getAvailableDrums() {
  return [
    "kick",
    "snare",
    "clap",
    "hihat",
    "hihat_open",
    "tom_hi",
    "tom_mid",
    "tom_lo",
    "crash",
    "ride",
    "rim",
    "cowbell",
    "shaker"
  ];
}
function getAvailableKits() {
  return ["808", "909", "acoustic", "lofi"];
}
function parseStepPattern(pattern, defaultVelocity = 0.8) {
  const hits = [];
  for (let i = 0; i < pattern.length; i++) {
    const char = pattern[i];
    if (char === "x" || char === "X") {
      hits.push({ step: i, velocity: defaultVelocity });
    } else if (char === ">") {
      hits.push({ step: i, velocity: 1 });
    }
  }
  return hits;
}

// node_modules/tone/build/esm/version.js
var version = "15.1.22";

// node_modules/standardized-audio-context/build/es2019/module.js
var import_automation_events2 = __toESM(require_bundle());

// node_modules/standardized-audio-context/build/es2019/factories/abort-error.js
var createAbortError = () => new DOMException("", "AbortError");

// node_modules/standardized-audio-context/build/es2019/factories/add-active-input-connection-to-audio-node.js
var createAddActiveInputConnectionToAudioNode = (insertElementInSet2) => {
  return (activeInputs, source, [output, input, eventListener], ignoreDuplicates) => {
    insertElementInSet2(activeInputs[input], [source, output, eventListener], (activeInputConnection) => activeInputConnection[0] === source && activeInputConnection[1] === output, ignoreDuplicates);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/add-audio-node-connections.js
var createAddAudioNodeConnections = (audioNodeConnectionsStore) => {
  return (audioNode, audioNodeRenderer, nativeAudioNode) => {
    const activeInputs = [];
    for (let i = 0; i < nativeAudioNode.numberOfInputs; i += 1) {
      activeInputs.push(/* @__PURE__ */ new Set());
    }
    audioNodeConnectionsStore.set(audioNode, {
      activeInputs,
      outputs: /* @__PURE__ */ new Set(),
      passiveInputs: /* @__PURE__ */ new WeakMap(),
      renderer: audioNodeRenderer
    });
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/add-audio-param-connections.js
var createAddAudioParamConnections = (audioParamConnectionsStore) => {
  return (audioParam, audioParamRenderer) => {
    audioParamConnectionsStore.set(audioParam, { activeInputs: /* @__PURE__ */ new Set(), passiveInputs: /* @__PURE__ */ new WeakMap(), renderer: audioParamRenderer });
  };
};

// node_modules/standardized-audio-context/build/es2019/globals.js
var ACTIVE_AUDIO_NODE_STORE = /* @__PURE__ */ new WeakSet();
var AUDIO_NODE_CONNECTIONS_STORE = /* @__PURE__ */ new WeakMap();
var AUDIO_NODE_STORE = /* @__PURE__ */ new WeakMap();
var AUDIO_PARAM_CONNECTIONS_STORE = /* @__PURE__ */ new WeakMap();
var AUDIO_PARAM_STORE = /* @__PURE__ */ new WeakMap();
var CONTEXT_STORE = /* @__PURE__ */ new WeakMap();
var EVENT_LISTENERS = /* @__PURE__ */ new WeakMap();
var CYCLE_COUNTERS = /* @__PURE__ */ new WeakMap();
var NODE_NAME_TO_PROCESSOR_CONSTRUCTOR_MAPS = /* @__PURE__ */ new WeakMap();
var NODE_TO_PROCESSOR_MAPS = /* @__PURE__ */ new WeakMap();

// node_modules/standardized-audio-context/build/es2019/helpers/is-constructible.js
var handler = {
  construct() {
    return handler;
  }
};
var isConstructible = (constructible) => {
  try {
    const proxy = new Proxy(constructible, handler);
    new proxy();
  } catch {
    return false;
  }
  return true;
};

// node_modules/standardized-audio-context/build/es2019/helpers/split-import-statements.js
var IMPORT_STATEMENT_REGEX = /^import(?:(?:[\s]+[\w]+|(?:[\s]+[\w]+[\s]*,)?[\s]*\{[\s]*[\w]+(?:[\s]+as[\s]+[\w]+)?(?:[\s]*,[\s]*[\w]+(?:[\s]+as[\s]+[\w]+)?)*[\s]*}|(?:[\s]+[\w]+[\s]*,)?[\s]*\*[\s]+as[\s]+[\w]+)[\s]+from)?(?:[\s]*)("([^"\\]|\\.)+"|'([^'\\]|\\.)+')(?:[\s]*);?/;
var splitImportStatements = (source, url) => {
  const importStatements = [];
  let sourceWithoutImportStatements = source.replace(/^[\s]+/, "");
  let result = sourceWithoutImportStatements.match(IMPORT_STATEMENT_REGEX);
  while (result !== null) {
    const unresolvedUrl = result[1].slice(1, -1);
    const importStatementWithResolvedUrl = result[0].replace(/([\s]+)?;?$/, "").replace(unresolvedUrl, new URL(unresolvedUrl, url).toString());
    importStatements.push(importStatementWithResolvedUrl);
    sourceWithoutImportStatements = sourceWithoutImportStatements.slice(result[0].length).replace(/^[\s]+/, "");
    result = sourceWithoutImportStatements.match(IMPORT_STATEMENT_REGEX);
  }
  return [importStatements.join(";"), sourceWithoutImportStatements];
};

// node_modules/standardized-audio-context/build/es2019/factories/add-audio-worklet-module.js
var verifyParameterDescriptors = (parameterDescriptors) => {
  if (parameterDescriptors !== void 0 && !Array.isArray(parameterDescriptors)) {
    throw new TypeError("The parameterDescriptors property of given value for processorCtor is not an array.");
  }
};
var verifyProcessorCtor = (processorCtor) => {
  if (!isConstructible(processorCtor)) {
    throw new TypeError("The given value for processorCtor should be a constructor.");
  }
  if (processorCtor.prototype === null || typeof processorCtor.prototype !== "object") {
    throw new TypeError("The given value for processorCtor should have a prototype.");
  }
};
var createAddAudioWorkletModule = (cacheTestResult2, createNotSupportedError2, evaluateSource, exposeCurrentFrameAndCurrentTime2, fetchSource, getNativeContext2, getOrCreateBackupOfflineAudioContext2, isNativeOfflineAudioContext2, nativeAudioWorkletNodeConstructor2, ongoingRequests, resolvedRequests, testAudioWorkletProcessorPostMessageSupport, window3) => {
  let index = 0;
  return (context2, moduleURL, options = { credentials: "omit" }) => {
    const resolvedRequestsOfContext = resolvedRequests.get(context2);
    if (resolvedRequestsOfContext !== void 0 && resolvedRequestsOfContext.has(moduleURL)) {
      return Promise.resolve();
    }
    const ongoingRequestsOfContext = ongoingRequests.get(context2);
    if (ongoingRequestsOfContext !== void 0) {
      const promiseOfOngoingRequest = ongoingRequestsOfContext.get(moduleURL);
      if (promiseOfOngoingRequest !== void 0) {
        return promiseOfOngoingRequest;
      }
    }
    const nativeContext = getNativeContext2(context2);
    const promise = nativeContext.audioWorklet === void 0 ? fetchSource(moduleURL).then(([source, absoluteUrl]) => {
      const [importStatements, sourceWithoutImportStatements] = splitImportStatements(source, absoluteUrl);
      const wrappedSource = `${importStatements};((a,b)=>{(a[b]=a[b]||[]).push((AudioWorkletProcessor,global,registerProcessor,sampleRate,self,window)=>{${sourceWithoutImportStatements}
})})(window,'_AWGS')`;
      return evaluateSource(wrappedSource);
    }).then(() => {
      const evaluateAudioWorkletGlobalScope = window3._AWGS.pop();
      if (evaluateAudioWorkletGlobalScope === void 0) {
        throw new SyntaxError();
      }
      exposeCurrentFrameAndCurrentTime2(nativeContext.currentTime, nativeContext.sampleRate, () => evaluateAudioWorkletGlobalScope(class AudioWorkletProcessor {
      }, void 0, (name, processorCtor) => {
        if (name.trim() === "") {
          throw createNotSupportedError2();
        }
        const nodeNameToProcessorConstructorMap = NODE_NAME_TO_PROCESSOR_CONSTRUCTOR_MAPS.get(nativeContext);
        if (nodeNameToProcessorConstructorMap !== void 0) {
          if (nodeNameToProcessorConstructorMap.has(name)) {
            throw createNotSupportedError2();
          }
          verifyProcessorCtor(processorCtor);
          verifyParameterDescriptors(processorCtor.parameterDescriptors);
          nodeNameToProcessorConstructorMap.set(name, processorCtor);
        } else {
          verifyProcessorCtor(processorCtor);
          verifyParameterDescriptors(processorCtor.parameterDescriptors);
          NODE_NAME_TO_PROCESSOR_CONSTRUCTOR_MAPS.set(nativeContext, /* @__PURE__ */ new Map([[name, processorCtor]]));
        }
      }, nativeContext.sampleRate, void 0, void 0));
    }) : Promise.all([
      fetchSource(moduleURL),
      Promise.resolve(cacheTestResult2(testAudioWorkletProcessorPostMessageSupport, testAudioWorkletProcessorPostMessageSupport))
    ]).then(([[source, absoluteUrl], isSupportingPostMessage]) => {
      const currentIndex = index + 1;
      index = currentIndex;
      const [importStatements, sourceWithoutImportStatements] = splitImportStatements(source, absoluteUrl);
      const patchedAudioWorkletProcessor = isSupportingPostMessage ? "AudioWorkletProcessor" : "class extends AudioWorkletProcessor {__b=new WeakSet();constructor(){super();(p=>p.postMessage=(q=>(m,t)=>q.call(p,m,t?t.filter(u=>!this.__b.has(u)):t))(p.postMessage))(this.port)}}";
      const memberDefinition = isSupportingPostMessage ? "" : "__c = (a) => a.forEach(e=>this.__b.add(e.buffer));";
      const bufferRegistration = isSupportingPostMessage ? "" : "i.forEach(this.__c);o.forEach(this.__c);this.__c(Object.values(p));";
      const wrappedSource = `${importStatements};((AudioWorkletProcessor,registerProcessor)=>{${sourceWithoutImportStatements}
})(${patchedAudioWorkletProcessor},(n,p)=>registerProcessor(n,class extends p{${memberDefinition}process(i,o,p){${bufferRegistration}return super.process(i.map(j=>j.some(k=>k.length===0)?[]:j),o,p)}}));registerProcessor('__sac${currentIndex}',class extends AudioWorkletProcessor{process(){return !1}})`;
      const blob = new Blob([wrappedSource], { type: "application/javascript; charset=utf-8" });
      const url = URL.createObjectURL(blob);
      return nativeContext.audioWorklet.addModule(url, options).then(() => {
        if (isNativeOfflineAudioContext2(nativeContext)) {
          return nativeContext;
        }
        const backupOfflineAudioContext = getOrCreateBackupOfflineAudioContext2(nativeContext);
        return backupOfflineAudioContext.audioWorklet.addModule(url, options).then(() => backupOfflineAudioContext);
      }).then((nativeContextOrBackupOfflineAudioContext) => {
        if (nativeAudioWorkletNodeConstructor2 === null) {
          throw new SyntaxError();
        }
        try {
          new nativeAudioWorkletNodeConstructor2(nativeContextOrBackupOfflineAudioContext, `__sac${currentIndex}`);
        } catch {
          throw new SyntaxError();
        }
      }).finally(() => URL.revokeObjectURL(url));
    });
    if (ongoingRequestsOfContext === void 0) {
      ongoingRequests.set(context2, /* @__PURE__ */ new Map([[moduleURL, promise]]));
    } else {
      ongoingRequestsOfContext.set(moduleURL, promise);
    }
    promise.then(() => {
      const updatedResolvedRequestsOfContext = resolvedRequests.get(context2);
      if (updatedResolvedRequestsOfContext === void 0) {
        resolvedRequests.set(context2, /* @__PURE__ */ new Set([moduleURL]));
      } else {
        updatedResolvedRequestsOfContext.add(moduleURL);
      }
    }).finally(() => {
      const updatedOngoingRequestsOfContext = ongoingRequests.get(context2);
      if (updatedOngoingRequestsOfContext !== void 0) {
        updatedOngoingRequestsOfContext.delete(moduleURL);
      }
    });
    return promise;
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/get-value-for-key.js
var getValueForKey = (map, key) => {
  const value = map.get(key);
  if (value === void 0) {
    throw new Error("A value with the given key could not be found.");
  }
  return value;
};

// node_modules/standardized-audio-context/build/es2019/helpers/pick-element-from-set.js
var pickElementFromSet = (set, predicate) => {
  const matchingElements = Array.from(set).filter(predicate);
  if (matchingElements.length > 1) {
    throw Error("More than one element was found.");
  }
  if (matchingElements.length === 0) {
    throw Error("No element was found.");
  }
  const [matchingElement] = matchingElements;
  set.delete(matchingElement);
  return matchingElement;
};

// node_modules/standardized-audio-context/build/es2019/helpers/delete-passive-input-connection-to-audio-node.js
var deletePassiveInputConnectionToAudioNode = (passiveInputs, source, output, input) => {
  const passiveInputConnections = getValueForKey(passiveInputs, source);
  const matchingConnection = pickElementFromSet(passiveInputConnections, (passiveInputConnection) => passiveInputConnection[0] === output && passiveInputConnection[1] === input);
  if (passiveInputConnections.size === 0) {
    passiveInputs.delete(source);
  }
  return matchingConnection;
};

// node_modules/standardized-audio-context/build/es2019/helpers/get-event-listeners-of-audio-node.js
var getEventListenersOfAudioNode = (audioNode) => {
  return getValueForKey(EVENT_LISTENERS, audioNode);
};

// node_modules/standardized-audio-context/build/es2019/helpers/set-internal-state-to-active.js
var setInternalStateToActive = (audioNode) => {
  if (ACTIVE_AUDIO_NODE_STORE.has(audioNode)) {
    throw new Error("The AudioNode is already stored.");
  }
  ACTIVE_AUDIO_NODE_STORE.add(audioNode);
  getEventListenersOfAudioNode(audioNode).forEach((eventListener) => eventListener(true));
};

// node_modules/standardized-audio-context/build/es2019/guards/audio-worklet-node.js
var isAudioWorkletNode = (audioNode) => {
  return "port" in audioNode;
};

// node_modules/standardized-audio-context/build/es2019/helpers/set-internal-state-to-passive.js
var setInternalStateToPassive = (audioNode) => {
  if (!ACTIVE_AUDIO_NODE_STORE.has(audioNode)) {
    throw new Error("The AudioNode is not stored.");
  }
  ACTIVE_AUDIO_NODE_STORE.delete(audioNode);
  getEventListenersOfAudioNode(audioNode).forEach((eventListener) => eventListener(false));
};

// node_modules/standardized-audio-context/build/es2019/helpers/set-internal-state-to-passive-when-necessary.js
var setInternalStateToPassiveWhenNecessary = (audioNode, activeInputs) => {
  if (!isAudioWorkletNode(audioNode) && activeInputs.every((connections) => connections.size === 0)) {
    setInternalStateToPassive(audioNode);
  }
};

// node_modules/standardized-audio-context/build/es2019/factories/add-connection-to-audio-node.js
var createAddConnectionToAudioNode = (addActiveInputConnectionToAudioNode2, addPassiveInputConnectionToAudioNode2, connectNativeAudioNodeToNativeAudioNode2, deleteActiveInputConnectionToAudioNode2, disconnectNativeAudioNodeFromNativeAudioNode2, getAudioNodeConnections2, getAudioNodeTailTime2, getEventListenersOfAudioNode2, getNativeAudioNode2, insertElementInSet2, isActiveAudioNode2, isPartOfACycle2, isPassiveAudioNode2) => {
  const tailTimeTimeoutIds = /* @__PURE__ */ new WeakMap();
  return (source, destination, output, input, isOffline) => {
    const { activeInputs, passiveInputs } = getAudioNodeConnections2(destination);
    const { outputs } = getAudioNodeConnections2(source);
    const eventListeners = getEventListenersOfAudioNode2(source);
    const eventListener = (isActive) => {
      const nativeDestinationAudioNode = getNativeAudioNode2(destination);
      const nativeSourceAudioNode = getNativeAudioNode2(source);
      if (isActive) {
        const partialConnection = deletePassiveInputConnectionToAudioNode(passiveInputs, source, output, input);
        addActiveInputConnectionToAudioNode2(activeInputs, source, partialConnection, false);
        if (!isOffline && !isPartOfACycle2(source)) {
          connectNativeAudioNodeToNativeAudioNode2(nativeSourceAudioNode, nativeDestinationAudioNode, output, input);
        }
        if (isPassiveAudioNode2(destination)) {
          setInternalStateToActive(destination);
        }
      } else {
        const partialConnection = deleteActiveInputConnectionToAudioNode2(activeInputs, source, output, input);
        addPassiveInputConnectionToAudioNode2(passiveInputs, input, partialConnection, false);
        if (!isOffline && !isPartOfACycle2(source)) {
          disconnectNativeAudioNodeFromNativeAudioNode2(nativeSourceAudioNode, nativeDestinationAudioNode, output, input);
        }
        const tailTime = getAudioNodeTailTime2(destination);
        if (tailTime === 0) {
          if (isActiveAudioNode2(destination)) {
            setInternalStateToPassiveWhenNecessary(destination, activeInputs);
          }
        } else {
          const tailTimeTimeoutId = tailTimeTimeoutIds.get(destination);
          if (tailTimeTimeoutId !== void 0) {
            clearTimeout(tailTimeTimeoutId);
          }
          tailTimeTimeoutIds.set(destination, setTimeout(() => {
            if (isActiveAudioNode2(destination)) {
              setInternalStateToPassiveWhenNecessary(destination, activeInputs);
            }
          }, tailTime * 1e3));
        }
      }
    };
    if (insertElementInSet2(outputs, [destination, output, input], (outputConnection) => outputConnection[0] === destination && outputConnection[1] === output && outputConnection[2] === input, true)) {
      eventListeners.add(eventListener);
      if (isActiveAudioNode2(source)) {
        addActiveInputConnectionToAudioNode2(activeInputs, source, [output, input, eventListener], true);
      } else {
        addPassiveInputConnectionToAudioNode2(passiveInputs, input, [source, output, eventListener], true);
      }
      return true;
    }
    return false;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/add-passive-input-connection-to-audio-node.js
var createAddPassiveInputConnectionToAudioNode = (insertElementInSet2) => {
  return (passiveInputs, input, [source, output, eventListener], ignoreDuplicates) => {
    const passiveInputConnections = passiveInputs.get(source);
    if (passiveInputConnections === void 0) {
      passiveInputs.set(source, /* @__PURE__ */ new Set([[output, input, eventListener]]));
    } else {
      insertElementInSet2(passiveInputConnections, [output, input, eventListener], (passiveInputConnection) => passiveInputConnection[0] === output && passiveInputConnection[1] === input, ignoreDuplicates);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/add-silent-connection.js
var createAddSilentConnection = (createNativeGainNode2) => {
  return (nativeContext, nativeAudioScheduledSourceNode) => {
    const nativeGainNode = createNativeGainNode2(nativeContext, {
      channelCount: 1,
      channelCountMode: "explicit",
      channelInterpretation: "discrete",
      gain: 0
    });
    nativeAudioScheduledSourceNode.connect(nativeGainNode).connect(nativeContext.destination);
    const disconnect2 = () => {
      nativeAudioScheduledSourceNode.removeEventListener("ended", disconnect2);
      nativeAudioScheduledSourceNode.disconnect(nativeGainNode);
      nativeGainNode.disconnect();
    };
    nativeAudioScheduledSourceNode.addEventListener("ended", disconnect2);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/add-unrendered-audio-worklet-node.js
var createAddUnrenderedAudioWorkletNode = (getUnrenderedAudioWorkletNodes2) => {
  return (nativeContext, audioWorkletNode) => {
    getUnrenderedAudioWorkletNodes2(nativeContext).add(audioWorkletNode);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/analyser-node-constructor.js
var DEFAULT_OPTIONS = {
  channelCount: 2,
  channelCountMode: "max",
  channelInterpretation: "speakers",
  fftSize: 2048,
  maxDecibels: -30,
  minDecibels: -100,
  smoothingTimeConstant: 0.8
};
var createAnalyserNodeConstructor = (audionNodeConstructor, createAnalyserNodeRenderer2, createIndexSizeError2, createNativeAnalyserNode2, getNativeContext2, isNativeOfflineAudioContext2) => {
  return class AnalyserNode extends audionNodeConstructor {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS, ...options };
      const nativeAnalyserNode = createNativeAnalyserNode2(nativeContext, mergedOptions);
      const analyserNodeRenderer = isNativeOfflineAudioContext2(nativeContext) ? createAnalyserNodeRenderer2() : null;
      super(context2, false, nativeAnalyserNode, analyserNodeRenderer);
      this._nativeAnalyserNode = nativeAnalyserNode;
    }
    get fftSize() {
      return this._nativeAnalyserNode.fftSize;
    }
    set fftSize(value) {
      this._nativeAnalyserNode.fftSize = value;
    }
    get frequencyBinCount() {
      return this._nativeAnalyserNode.frequencyBinCount;
    }
    get maxDecibels() {
      return this._nativeAnalyserNode.maxDecibels;
    }
    set maxDecibels(value) {
      const maxDecibels = this._nativeAnalyserNode.maxDecibels;
      this._nativeAnalyserNode.maxDecibels = value;
      if (!(value > this._nativeAnalyserNode.minDecibels)) {
        this._nativeAnalyserNode.maxDecibels = maxDecibels;
        throw createIndexSizeError2();
      }
    }
    get minDecibels() {
      return this._nativeAnalyserNode.minDecibels;
    }
    set minDecibels(value) {
      const minDecibels = this._nativeAnalyserNode.minDecibels;
      this._nativeAnalyserNode.minDecibels = value;
      if (!(this._nativeAnalyserNode.maxDecibels > value)) {
        this._nativeAnalyserNode.minDecibels = minDecibels;
        throw createIndexSizeError2();
      }
    }
    get smoothingTimeConstant() {
      return this._nativeAnalyserNode.smoothingTimeConstant;
    }
    set smoothingTimeConstant(value) {
      this._nativeAnalyserNode.smoothingTimeConstant = value;
    }
    getByteFrequencyData(array) {
      this._nativeAnalyserNode.getByteFrequencyData(array);
    }
    getByteTimeDomainData(array) {
      this._nativeAnalyserNode.getByteTimeDomainData(array);
    }
    getFloatFrequencyData(array) {
      this._nativeAnalyserNode.getFloatFrequencyData(array);
    }
    getFloatTimeDomainData(array) {
      this._nativeAnalyserNode.getFloatTimeDomainData(array);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/is-owned-by-context.js
var isOwnedByContext = (nativeAudioNode, nativeContext) => {
  return nativeAudioNode.context === nativeContext;
};

// node_modules/standardized-audio-context/build/es2019/factories/analyser-node-renderer-factory.js
var createAnalyserNodeRendererFactory = (createNativeAnalyserNode2, getNativeAudioNode2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeAnalyserNodes = /* @__PURE__ */ new WeakMap();
    const createAnalyserNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeAnalyserNode = getNativeAudioNode2(proxy);
      const nativeAnalyserNodeIsOwnedByContext = isOwnedByContext(nativeAnalyserNode, nativeOfflineAudioContext);
      if (!nativeAnalyserNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeAnalyserNode.channelCount,
          channelCountMode: nativeAnalyserNode.channelCountMode,
          channelInterpretation: nativeAnalyserNode.channelInterpretation,
          fftSize: nativeAnalyserNode.fftSize,
          maxDecibels: nativeAnalyserNode.maxDecibels,
          minDecibels: nativeAnalyserNode.minDecibels,
          smoothingTimeConstant: nativeAnalyserNode.smoothingTimeConstant
        };
        nativeAnalyserNode = createNativeAnalyserNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeAnalyserNodes.set(nativeOfflineAudioContext, nativeAnalyserNode);
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeAnalyserNode);
      return nativeAnalyserNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeAnalyserNode = renderedNativeAnalyserNodes.get(nativeOfflineAudioContext);
        if (renderedNativeAnalyserNode !== void 0) {
          return Promise.resolve(renderedNativeAnalyserNode);
        }
        return createAnalyserNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-buffer-copy-channel-methods-out-of-bounds-support.js
var testAudioBufferCopyChannelMethodsOutOfBoundsSupport = (nativeAudioBuffer) => {
  try {
    nativeAudioBuffer.copyToChannel(new Float32Array(1), 0, -1);
  } catch {
    return false;
  }
  return true;
};

// node_modules/standardized-audio-context/build/es2019/factories/index-size-error.js
var createIndexSizeError = () => new DOMException("", "IndexSizeError");

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-buffer-get-channel-data-method.js
var wrapAudioBufferGetChannelDataMethod = (audioBuffer) => {
  audioBuffer.getChannelData = /* @__PURE__ */ ((getChannelData) => {
    return (channel) => {
      try {
        return getChannelData.call(audioBuffer, channel);
      } catch (err) {
        if (err.code === 12) {
          throw createIndexSizeError();
        }
        throw err;
      }
    };
  })(audioBuffer.getChannelData);
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-buffer-constructor.js
var DEFAULT_OPTIONS2 = {
  numberOfChannels: 1
};
var createAudioBufferConstructor = (audioBufferStore2, cacheTestResult2, createNotSupportedError2, nativeAudioBufferConstructor2, nativeOfflineAudioContextConstructor2, testNativeAudioBufferConstructorSupport, wrapAudioBufferCopyChannelMethods2, wrapAudioBufferCopyChannelMethodsOutOfBounds2) => {
  let nativeOfflineAudioContext = null;
  return class AudioBuffer {
    constructor(options) {
      if (nativeOfflineAudioContextConstructor2 === null) {
        throw new Error("Missing the native OfflineAudioContext constructor.");
      }
      const { length, numberOfChannels, sampleRate } = { ...DEFAULT_OPTIONS2, ...options };
      if (nativeOfflineAudioContext === null) {
        nativeOfflineAudioContext = new nativeOfflineAudioContextConstructor2(1, 1, 44100);
      }
      const audioBuffer = nativeAudioBufferConstructor2 !== null && cacheTestResult2(testNativeAudioBufferConstructorSupport, testNativeAudioBufferConstructorSupport) ? new nativeAudioBufferConstructor2({ length, numberOfChannels, sampleRate }) : nativeOfflineAudioContext.createBuffer(numberOfChannels, length, sampleRate);
      if (audioBuffer.numberOfChannels === 0) {
        throw createNotSupportedError2();
      }
      if (typeof audioBuffer.copyFromChannel !== "function") {
        wrapAudioBufferCopyChannelMethods2(audioBuffer);
        wrapAudioBufferGetChannelDataMethod(audioBuffer);
      } else if (!cacheTestResult2(testAudioBufferCopyChannelMethodsOutOfBoundsSupport, () => testAudioBufferCopyChannelMethodsOutOfBoundsSupport(audioBuffer))) {
        wrapAudioBufferCopyChannelMethodsOutOfBounds2(audioBuffer);
      }
      audioBufferStore2.add(audioBuffer);
      return audioBuffer;
    }
    static [Symbol.hasInstance](instance) {
      return instance !== null && typeof instance === "object" && Object.getPrototypeOf(instance) === AudioBuffer.prototype || audioBufferStore2.has(instance);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/constants.js
var MOST_NEGATIVE_SINGLE_FLOAT = -34028234663852886e22;
var MOST_POSITIVE_SINGLE_FLOAT = -MOST_NEGATIVE_SINGLE_FLOAT;

// node_modules/standardized-audio-context/build/es2019/helpers/is-active-audio-node.js
var isActiveAudioNode = (audioNode) => ACTIVE_AUDIO_NODE_STORE.has(audioNode);

// node_modules/standardized-audio-context/build/es2019/factories/audio-buffer-source-node-constructor.js
var DEFAULT_OPTIONS3 = {
  buffer: null,
  channelCount: 2,
  channelCountMode: "max",
  channelInterpretation: "speakers",
  // Bug #149: Safari does not yet support the detune AudioParam.
  loop: false,
  loopEnd: 0,
  loopStart: 0,
  playbackRate: 1
};
var createAudioBufferSourceNodeConstructor = (audioNodeConstructor2, createAudioBufferSourceNodeRenderer2, createAudioParam2, createInvalidStateError2, createNativeAudioBufferSourceNode2, getNativeContext2, isNativeOfflineAudioContext2, wrapEventListener2) => {
  return class AudioBufferSourceNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS3, ...options };
      const nativeAudioBufferSourceNode = createNativeAudioBufferSourceNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const audioBufferSourceNodeRenderer = isOffline ? createAudioBufferSourceNodeRenderer2() : null;
      super(context2, false, nativeAudioBufferSourceNode, audioBufferSourceNodeRenderer);
      this._audioBufferSourceNodeRenderer = audioBufferSourceNodeRenderer;
      this._isBufferNullified = false;
      this._isBufferSet = mergedOptions.buffer !== null;
      this._nativeAudioBufferSourceNode = nativeAudioBufferSourceNode;
      this._onended = null;
      this._playbackRate = createAudioParam2(this, isOffline, nativeAudioBufferSourceNode.playbackRate, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
    }
    get buffer() {
      if (this._isBufferNullified) {
        return null;
      }
      return this._nativeAudioBufferSourceNode.buffer;
    }
    set buffer(value) {
      this._nativeAudioBufferSourceNode.buffer = value;
      if (value !== null) {
        if (this._isBufferSet) {
          throw createInvalidStateError2();
        }
        this._isBufferSet = true;
      }
    }
    get loop() {
      return this._nativeAudioBufferSourceNode.loop;
    }
    set loop(value) {
      this._nativeAudioBufferSourceNode.loop = value;
    }
    get loopEnd() {
      return this._nativeAudioBufferSourceNode.loopEnd;
    }
    set loopEnd(value) {
      this._nativeAudioBufferSourceNode.loopEnd = value;
    }
    get loopStart() {
      return this._nativeAudioBufferSourceNode.loopStart;
    }
    set loopStart(value) {
      this._nativeAudioBufferSourceNode.loopStart = value;
    }
    get onended() {
      return this._onended;
    }
    set onended(value) {
      const wrappedListener = typeof value === "function" ? wrapEventListener2(this, value) : null;
      this._nativeAudioBufferSourceNode.onended = wrappedListener;
      const nativeOnEnded = this._nativeAudioBufferSourceNode.onended;
      this._onended = nativeOnEnded !== null && nativeOnEnded === wrappedListener ? value : nativeOnEnded;
    }
    get playbackRate() {
      return this._playbackRate;
    }
    start(when = 0, offset = 0, duration) {
      this._nativeAudioBufferSourceNode.start(when, offset, duration);
      if (this._audioBufferSourceNodeRenderer !== null) {
        this._audioBufferSourceNodeRenderer.start = duration === void 0 ? [when, offset] : [when, offset, duration];
      }
      if (this.context.state !== "closed") {
        setInternalStateToActive(this);
        const resetInternalStateToPassive = () => {
          this._nativeAudioBufferSourceNode.removeEventListener("ended", resetInternalStateToPassive);
          if (isActiveAudioNode(this)) {
            setInternalStateToPassive(this);
          }
        };
        this._nativeAudioBufferSourceNode.addEventListener("ended", resetInternalStateToPassive);
      }
    }
    stop(when = 0) {
      this._nativeAudioBufferSourceNode.stop(when);
      if (this._audioBufferSourceNodeRenderer !== null) {
        this._audioBufferSourceNodeRenderer.stop = when;
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-buffer-source-node-renderer-factory.js
var createAudioBufferSourceNodeRendererFactory = (connectAudioParam2, createNativeAudioBufferSourceNode2, getNativeAudioNode2, renderAutomation2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeAudioBufferSourceNodes = /* @__PURE__ */ new WeakMap();
    let start2 = null;
    let stop = null;
    const createAudioBufferSourceNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeAudioBufferSourceNode = getNativeAudioNode2(proxy);
      const nativeAudioBufferSourceNodeIsOwnedByContext = isOwnedByContext(nativeAudioBufferSourceNode, nativeOfflineAudioContext);
      if (!nativeAudioBufferSourceNodeIsOwnedByContext) {
        const options = {
          buffer: nativeAudioBufferSourceNode.buffer,
          channelCount: nativeAudioBufferSourceNode.channelCount,
          channelCountMode: nativeAudioBufferSourceNode.channelCountMode,
          channelInterpretation: nativeAudioBufferSourceNode.channelInterpretation,
          // Bug #149: Safari does not yet support the detune AudioParam.
          loop: nativeAudioBufferSourceNode.loop,
          loopEnd: nativeAudioBufferSourceNode.loopEnd,
          loopStart: nativeAudioBufferSourceNode.loopStart,
          playbackRate: nativeAudioBufferSourceNode.playbackRate.value
        };
        nativeAudioBufferSourceNode = createNativeAudioBufferSourceNode2(nativeOfflineAudioContext, options);
        if (start2 !== null) {
          nativeAudioBufferSourceNode.start(...start2);
        }
        if (stop !== null) {
          nativeAudioBufferSourceNode.stop(stop);
        }
      }
      renderedNativeAudioBufferSourceNodes.set(nativeOfflineAudioContext, nativeAudioBufferSourceNode);
      if (!nativeAudioBufferSourceNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.playbackRate, nativeAudioBufferSourceNode.playbackRate);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.playbackRate, nativeAudioBufferSourceNode.playbackRate);
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeAudioBufferSourceNode);
      return nativeAudioBufferSourceNode;
    };
    return {
      set start(value) {
        start2 = value;
      },
      set stop(value) {
        stop = value;
      },
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeAudioBufferSourceNode = renderedNativeAudioBufferSourceNodes.get(nativeOfflineAudioContext);
        if (renderedNativeAudioBufferSourceNode !== void 0) {
          return Promise.resolve(renderedNativeAudioBufferSourceNode);
        }
        return createAudioBufferSourceNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/guards/audio-buffer-source-node.js
var isAudioBufferSourceNode = (audioNode) => {
  return "playbackRate" in audioNode;
};

// node_modules/standardized-audio-context/build/es2019/guards/biquad-filter-node.js
var isBiquadFilterNode = (audioNode) => {
  return "frequency" in audioNode && "gain" in audioNode;
};

// node_modules/standardized-audio-context/build/es2019/guards/constant-source-node.js
var isConstantSourceNode = (audioNode) => {
  return "offset" in audioNode;
};

// node_modules/standardized-audio-context/build/es2019/guards/gain-node.js
var isGainNode = (audioNode) => {
  return !("frequency" in audioNode) && "gain" in audioNode;
};

// node_modules/standardized-audio-context/build/es2019/guards/oscillator-node.js
var isOscillatorNode = (audioNode) => {
  return "detune" in audioNode && "frequency" in audioNode && !("gain" in audioNode);
};

// node_modules/standardized-audio-context/build/es2019/guards/stereo-panner-node.js
var isStereoPannerNode = (audioNode) => {
  return "pan" in audioNode;
};

// node_modules/standardized-audio-context/build/es2019/helpers/get-audio-node-connections.js
var getAudioNodeConnections = (audioNode) => {
  return getValueForKey(AUDIO_NODE_CONNECTIONS_STORE, audioNode);
};

// node_modules/standardized-audio-context/build/es2019/helpers/get-audio-param-connections.js
var getAudioParamConnections = (audioParam) => {
  return getValueForKey(AUDIO_PARAM_CONNECTIONS_STORE, audioParam);
};

// node_modules/standardized-audio-context/build/es2019/helpers/deactivate-active-audio-node-input-connections.js
var deactivateActiveAudioNodeInputConnections = (audioNode, trace) => {
  const { activeInputs } = getAudioNodeConnections(audioNode);
  activeInputs.forEach((connections) => connections.forEach(([source]) => {
    if (!trace.includes(audioNode)) {
      deactivateActiveAudioNodeInputConnections(source, [...trace, audioNode]);
    }
  }));
  const audioParams = isAudioBufferSourceNode(audioNode) ? [
    // Bug #149: Safari does not yet support the detune AudioParam.
    audioNode.playbackRate
  ] : isAudioWorkletNode(audioNode) ? Array.from(audioNode.parameters.values()) : isBiquadFilterNode(audioNode) ? [audioNode.Q, audioNode.detune, audioNode.frequency, audioNode.gain] : isConstantSourceNode(audioNode) ? [audioNode.offset] : isGainNode(audioNode) ? [audioNode.gain] : isOscillatorNode(audioNode) ? [audioNode.detune, audioNode.frequency] : isStereoPannerNode(audioNode) ? [audioNode.pan] : [];
  for (const audioParam of audioParams) {
    const audioParamConnections = getAudioParamConnections(audioParam);
    if (audioParamConnections !== void 0) {
      audioParamConnections.activeInputs.forEach(([source]) => deactivateActiveAudioNodeInputConnections(source, trace));
    }
  }
  if (isActiveAudioNode(audioNode)) {
    setInternalStateToPassive(audioNode);
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/deactivate-audio-graph.js
var deactivateAudioGraph = (context2) => {
  deactivateActiveAudioNodeInputConnections(context2.destination, []);
};

// node_modules/standardized-audio-context/build/es2019/helpers/is-valid-latency-hint.js
var isValidLatencyHint = (latencyHint) => {
  return latencyHint === void 0 || typeof latencyHint === "number" || typeof latencyHint === "string" && (latencyHint === "balanced" || latencyHint === "interactive" || latencyHint === "playback");
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-context-constructor.js
var createAudioContextConstructor = (baseAudioContextConstructor2, createInvalidStateError2, createNotSupportedError2, createUnknownError2, mediaElementAudioSourceNodeConstructor2, mediaStreamAudioDestinationNodeConstructor2, mediaStreamAudioSourceNodeConstructor2, mediaStreamTrackAudioSourceNodeConstructor2, nativeAudioContextConstructor2) => {
  return class AudioContext extends baseAudioContextConstructor2 {
    constructor(options = {}) {
      if (nativeAudioContextConstructor2 === null) {
        throw new Error("Missing the native AudioContext constructor.");
      }
      let nativeAudioContext;
      try {
        nativeAudioContext = new nativeAudioContextConstructor2(options);
      } catch (err) {
        if (err.code === 12 && err.message === "sampleRate is not in range") {
          throw createNotSupportedError2();
        }
        throw err;
      }
      if (nativeAudioContext === null) {
        throw createUnknownError2();
      }
      if (!isValidLatencyHint(options.latencyHint)) {
        throw new TypeError(`The provided value '${options.latencyHint}' is not a valid enum value of type AudioContextLatencyCategory.`);
      }
      if (options.sampleRate !== void 0 && nativeAudioContext.sampleRate !== options.sampleRate) {
        throw createNotSupportedError2();
      }
      super(nativeAudioContext, 2);
      const { latencyHint } = options;
      const { sampleRate } = nativeAudioContext;
      this._baseLatency = typeof nativeAudioContext.baseLatency === "number" ? nativeAudioContext.baseLatency : latencyHint === "balanced" ? 512 / sampleRate : latencyHint === "interactive" || latencyHint === void 0 ? 256 / sampleRate : latencyHint === "playback" ? 1024 / sampleRate : (
        /*
         * @todo The min (256) and max (16384) values are taken from the allowed bufferSize values of a
         * ScriptProcessorNode.
         */
        Math.max(2, Math.min(128, Math.round(latencyHint * sampleRate / 128))) * 128 / sampleRate
      );
      this._nativeAudioContext = nativeAudioContext;
      if (nativeAudioContextConstructor2.name === "webkitAudioContext") {
        this._nativeGainNode = nativeAudioContext.createGain();
        this._nativeOscillatorNode = nativeAudioContext.createOscillator();
        this._nativeGainNode.gain.value = 1e-37;
        this._nativeOscillatorNode.connect(this._nativeGainNode).connect(nativeAudioContext.destination);
        this._nativeOscillatorNode.start();
      } else {
        this._nativeGainNode = null;
        this._nativeOscillatorNode = null;
      }
      this._state = null;
      if (nativeAudioContext.state === "running") {
        this._state = "suspended";
        const revokeState = () => {
          if (this._state === "suspended") {
            this._state = null;
          }
          nativeAudioContext.removeEventListener("statechange", revokeState);
        };
        nativeAudioContext.addEventListener("statechange", revokeState);
      }
    }
    get baseLatency() {
      return this._baseLatency;
    }
    get state() {
      return this._state !== null ? this._state : this._nativeAudioContext.state;
    }
    close() {
      if (this.state === "closed") {
        return this._nativeAudioContext.close().then(() => {
          throw createInvalidStateError2();
        });
      }
      if (this._state === "suspended") {
        this._state = null;
      }
      return this._nativeAudioContext.close().then(() => {
        if (this._nativeGainNode !== null && this._nativeOscillatorNode !== null) {
          this._nativeOscillatorNode.stop();
          this._nativeGainNode.disconnect();
          this._nativeOscillatorNode.disconnect();
        }
        deactivateAudioGraph(this);
      });
    }
    createMediaElementSource(mediaElement) {
      return new mediaElementAudioSourceNodeConstructor2(this, { mediaElement });
    }
    createMediaStreamDestination() {
      return new mediaStreamAudioDestinationNodeConstructor2(this);
    }
    createMediaStreamSource(mediaStream) {
      return new mediaStreamAudioSourceNodeConstructor2(this, { mediaStream });
    }
    createMediaStreamTrackSource(mediaStreamTrack) {
      return new mediaStreamTrackAudioSourceNodeConstructor2(this, { mediaStreamTrack });
    }
    resume() {
      if (this._state === "suspended") {
        return new Promise((resolve, reject) => {
          const resolvePromise = () => {
            this._nativeAudioContext.removeEventListener("statechange", resolvePromise);
            if (this._nativeAudioContext.state === "running") {
              resolve();
            } else {
              this.resume().then(resolve, reject);
            }
          };
          this._nativeAudioContext.addEventListener("statechange", resolvePromise);
        });
      }
      return this._nativeAudioContext.resume().catch((err) => {
        if (err === void 0 || err.code === 15) {
          throw createInvalidStateError2();
        }
        throw err;
      });
    }
    suspend() {
      return this._nativeAudioContext.suspend().catch((err) => {
        if (err === void 0) {
          throw createInvalidStateError2();
        }
        throw err;
      });
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-destination-node-constructor.js
var createAudioDestinationNodeConstructor = (audioNodeConstructor2, createAudioDestinationNodeRenderer2, createIndexSizeError2, createInvalidStateError2, createNativeAudioDestinationNode, getNativeContext2, isNativeOfflineAudioContext2, renderInputsOfAudioNode2) => {
  return class AudioDestinationNode extends audioNodeConstructor2 {
    constructor(context2, channelCount) {
      const nativeContext = getNativeContext2(context2);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const nativeAudioDestinationNode = createNativeAudioDestinationNode(nativeContext, channelCount, isOffline);
      const audioDestinationNodeRenderer = isOffline ? createAudioDestinationNodeRenderer2(renderInputsOfAudioNode2) : null;
      super(context2, false, nativeAudioDestinationNode, audioDestinationNodeRenderer);
      this._isNodeOfNativeOfflineAudioContext = isOffline;
      this._nativeAudioDestinationNode = nativeAudioDestinationNode;
    }
    get channelCount() {
      return this._nativeAudioDestinationNode.channelCount;
    }
    set channelCount(value) {
      if (this._isNodeOfNativeOfflineAudioContext) {
        throw createInvalidStateError2();
      }
      if (value > this._nativeAudioDestinationNode.maxChannelCount) {
        throw createIndexSizeError2();
      }
      this._nativeAudioDestinationNode.channelCount = value;
    }
    get channelCountMode() {
      return this._nativeAudioDestinationNode.channelCountMode;
    }
    set channelCountMode(value) {
      if (this._isNodeOfNativeOfflineAudioContext) {
        throw createInvalidStateError2();
      }
      this._nativeAudioDestinationNode.channelCountMode = value;
    }
    get maxChannelCount() {
      return this._nativeAudioDestinationNode.maxChannelCount;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-destination-node-renderer-factory.js
var createAudioDestinationNodeRenderer = (renderInputsOfAudioNode2) => {
  const renderedNativeAudioDestinationNodes = /* @__PURE__ */ new WeakMap();
  const createAudioDestinationNode = async (proxy, nativeOfflineAudioContext) => {
    const nativeAudioDestinationNode = nativeOfflineAudioContext.destination;
    renderedNativeAudioDestinationNodes.set(nativeOfflineAudioContext, nativeAudioDestinationNode);
    await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeAudioDestinationNode);
    return nativeAudioDestinationNode;
  };
  return {
    render(proxy, nativeOfflineAudioContext) {
      const renderedNativeAudioDestinationNode = renderedNativeAudioDestinationNodes.get(nativeOfflineAudioContext);
      if (renderedNativeAudioDestinationNode !== void 0) {
        return Promise.resolve(renderedNativeAudioDestinationNode);
      }
      return createAudioDestinationNode(proxy, nativeOfflineAudioContext);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-listener-factory.js
var createAudioListenerFactory = (createAudioParam2, createNativeChannelMergerNode2, createNativeConstantSourceNode2, createNativeScriptProcessorNode2, createNotSupportedError2, getFirstSample2, isNativeOfflineAudioContext2, overwriteAccessors2) => {
  return (context2, nativeContext) => {
    const nativeListener = nativeContext.listener;
    const createFakeAudioParams = () => {
      const buffer = new Float32Array(1);
      const channelMergerNode = createNativeChannelMergerNode2(nativeContext, {
        channelCount: 1,
        channelCountMode: "explicit",
        channelInterpretation: "speakers",
        numberOfInputs: 9
      });
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      let isScriptProcessorNodeCreated = false;
      let lastOrientation = [0, 0, -1, 0, 1, 0];
      let lastPosition = [0, 0, 0];
      const createScriptProcessorNode = () => {
        if (isScriptProcessorNodeCreated) {
          return;
        }
        isScriptProcessorNodeCreated = true;
        const scriptProcessorNode = createNativeScriptProcessorNode2(nativeContext, 256, 9, 0);
        scriptProcessorNode.onaudioprocess = ({ inputBuffer }) => {
          const orientation = [
            getFirstSample2(inputBuffer, buffer, 0),
            getFirstSample2(inputBuffer, buffer, 1),
            getFirstSample2(inputBuffer, buffer, 2),
            getFirstSample2(inputBuffer, buffer, 3),
            getFirstSample2(inputBuffer, buffer, 4),
            getFirstSample2(inputBuffer, buffer, 5)
          ];
          if (orientation.some((value, index) => value !== lastOrientation[index])) {
            nativeListener.setOrientation(...orientation);
            lastOrientation = orientation;
          }
          const positon = [
            getFirstSample2(inputBuffer, buffer, 6),
            getFirstSample2(inputBuffer, buffer, 7),
            getFirstSample2(inputBuffer, buffer, 8)
          ];
          if (positon.some((value, index) => value !== lastPosition[index])) {
            nativeListener.setPosition(...positon);
            lastPosition = positon;
          }
        };
        channelMergerNode.connect(scriptProcessorNode);
      };
      const createSetOrientation = (index) => (value) => {
        if (value !== lastOrientation[index]) {
          lastOrientation[index] = value;
          nativeListener.setOrientation(...lastOrientation);
        }
      };
      const createSetPosition = (index) => (value) => {
        if (value !== lastPosition[index]) {
          lastPosition[index] = value;
          nativeListener.setPosition(...lastPosition);
        }
      };
      const createFakeAudioParam = (input, initialValue, setValue) => {
        const constantSourceNode = createNativeConstantSourceNode2(nativeContext, {
          channelCount: 1,
          channelCountMode: "explicit",
          channelInterpretation: "discrete",
          offset: initialValue
        });
        constantSourceNode.connect(channelMergerNode, 0, input);
        constantSourceNode.start();
        Object.defineProperty(constantSourceNode.offset, "defaultValue", {
          get() {
            return initialValue;
          }
        });
        const audioParam = createAudioParam2({ context: context2 }, isOffline, constantSourceNode.offset, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
        overwriteAccessors2(audioParam, "value", (get) => () => get.call(audioParam), (set) => (value) => {
          try {
            set.call(audioParam, value);
          } catch (err) {
            if (err.code !== 9) {
              throw err;
            }
          }
          createScriptProcessorNode();
          if (isOffline) {
            setValue(value);
          }
        });
        audioParam.cancelAndHoldAtTime = ((cancelAndHoldAtTime) => {
          if (isOffline) {
            return () => {
              throw createNotSupportedError2();
            };
          }
          return (...args) => {
            const value = cancelAndHoldAtTime.apply(audioParam, args);
            createScriptProcessorNode();
            return value;
          };
        })(audioParam.cancelAndHoldAtTime);
        audioParam.cancelScheduledValues = ((cancelScheduledValues) => {
          if (isOffline) {
            return () => {
              throw createNotSupportedError2();
            };
          }
          return (...args) => {
            const value = cancelScheduledValues.apply(audioParam, args);
            createScriptProcessorNode();
            return value;
          };
        })(audioParam.cancelScheduledValues);
        audioParam.exponentialRampToValueAtTime = ((exponentialRampToValueAtTime) => {
          if (isOffline) {
            return () => {
              throw createNotSupportedError2();
            };
          }
          return (...args) => {
            const value = exponentialRampToValueAtTime.apply(audioParam, args);
            createScriptProcessorNode();
            return value;
          };
        })(audioParam.exponentialRampToValueAtTime);
        audioParam.linearRampToValueAtTime = ((linearRampToValueAtTime) => {
          if (isOffline) {
            return () => {
              throw createNotSupportedError2();
            };
          }
          return (...args) => {
            const value = linearRampToValueAtTime.apply(audioParam, args);
            createScriptProcessorNode();
            return value;
          };
        })(audioParam.linearRampToValueAtTime);
        audioParam.setTargetAtTime = ((setTargetAtTime) => {
          if (isOffline) {
            return () => {
              throw createNotSupportedError2();
            };
          }
          return (...args) => {
            const value = setTargetAtTime.apply(audioParam, args);
            createScriptProcessorNode();
            return value;
          };
        })(audioParam.setTargetAtTime);
        audioParam.setValueAtTime = ((setValueAtTime) => {
          if (isOffline) {
            return () => {
              throw createNotSupportedError2();
            };
          }
          return (...args) => {
            const value = setValueAtTime.apply(audioParam, args);
            createScriptProcessorNode();
            return value;
          };
        })(audioParam.setValueAtTime);
        audioParam.setValueCurveAtTime = ((setValueCurveAtTime) => {
          if (isOffline) {
            return () => {
              throw createNotSupportedError2();
            };
          }
          return (...args) => {
            const value = setValueCurveAtTime.apply(audioParam, args);
            createScriptProcessorNode();
            return value;
          };
        })(audioParam.setValueCurveAtTime);
        return audioParam;
      };
      return {
        forwardX: createFakeAudioParam(0, 0, createSetOrientation(0)),
        forwardY: createFakeAudioParam(1, 0, createSetOrientation(1)),
        forwardZ: createFakeAudioParam(2, -1, createSetOrientation(2)),
        positionX: createFakeAudioParam(6, 0, createSetPosition(0)),
        positionY: createFakeAudioParam(7, 0, createSetPosition(1)),
        positionZ: createFakeAudioParam(8, 0, createSetPosition(2)),
        upX: createFakeAudioParam(3, 0, createSetOrientation(3)),
        upY: createFakeAudioParam(4, 1, createSetOrientation(4)),
        upZ: createFakeAudioParam(5, 0, createSetOrientation(5))
      };
    };
    const { forwardX, forwardY, forwardZ, positionX, positionY, positionZ, upX, upY, upZ } = nativeListener.forwardX === void 0 ? createFakeAudioParams() : nativeListener;
    return {
      get forwardX() {
        return forwardX;
      },
      get forwardY() {
        return forwardY;
      },
      get forwardZ() {
        return forwardZ;
      },
      get positionX() {
        return positionX;
      },
      get positionY() {
        return positionY;
      },
      get positionZ() {
        return positionZ;
      },
      get upX() {
        return upX;
      },
      get upY() {
        return upY;
      },
      get upZ() {
        return upZ;
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/guards/audio-node.js
var isAudioNode = (audioNodeOrAudioParam) => {
  return "context" in audioNodeOrAudioParam;
};

// node_modules/standardized-audio-context/build/es2019/guards/audio-node-output-connection.js
var isAudioNodeOutputConnection = (outputConnection) => {
  return isAudioNode(outputConnection[0]);
};

// node_modules/standardized-audio-context/build/es2019/helpers/insert-element-in-set.js
var insertElementInSet = (set, element, predicate, ignoreDuplicates) => {
  for (const lmnt of set) {
    if (predicate(lmnt)) {
      if (ignoreDuplicates) {
        return false;
      }
      throw Error("The set contains at least one similar element.");
    }
  }
  set.add(element);
  return true;
};

// node_modules/standardized-audio-context/build/es2019/helpers/add-active-input-connection-to-audio-param.js
var addActiveInputConnectionToAudioParam = (activeInputs, source, [output, eventListener], ignoreDuplicates) => {
  insertElementInSet(activeInputs, [source, output, eventListener], (activeInputConnection) => activeInputConnection[0] === source && activeInputConnection[1] === output, ignoreDuplicates);
};

// node_modules/standardized-audio-context/build/es2019/helpers/add-passive-input-connection-to-audio-param.js
var addPassiveInputConnectionToAudioParam = (passiveInputs, [source, output, eventListener], ignoreDuplicates) => {
  const passiveInputConnections = passiveInputs.get(source);
  if (passiveInputConnections === void 0) {
    passiveInputs.set(source, /* @__PURE__ */ new Set([[output, eventListener]]));
  } else {
    insertElementInSet(passiveInputConnections, [output, eventListener], (passiveInputConnection) => passiveInputConnection[0] === output, ignoreDuplicates);
  }
};

// node_modules/standardized-audio-context/build/es2019/guards/native-audio-node-faker.js
var isNativeAudioNodeFaker = (nativeAudioNodeOrNativeAudioNodeFaker) => {
  return "inputs" in nativeAudioNodeOrNativeAudioNodeFaker;
};

// node_modules/standardized-audio-context/build/es2019/helpers/connect-native-audio-node-to-native-audio-node.js
var connectNativeAudioNodeToNativeAudioNode = (nativeSourceAudioNode, nativeDestinationAudioNode, output, input) => {
  if (isNativeAudioNodeFaker(nativeDestinationAudioNode)) {
    const fakeNativeDestinationAudioNode = nativeDestinationAudioNode.inputs[input];
    nativeSourceAudioNode.connect(fakeNativeDestinationAudioNode, output, 0);
    return [fakeNativeDestinationAudioNode, output, 0];
  }
  nativeSourceAudioNode.connect(nativeDestinationAudioNode, output, input);
  return [nativeDestinationAudioNode, output, input];
};

// node_modules/standardized-audio-context/build/es2019/helpers/delete-active-input-connection.js
var deleteActiveInputConnection = (activeInputConnections, source, output) => {
  for (const activeInputConnection of activeInputConnections) {
    if (activeInputConnection[0] === source && activeInputConnection[1] === output) {
      activeInputConnections.delete(activeInputConnection);
      return activeInputConnection;
    }
  }
  return null;
};

// node_modules/standardized-audio-context/build/es2019/helpers/delete-active-input-connection-to-audio-param.js
var deleteActiveInputConnectionToAudioParam = (activeInputs, source, output) => {
  return pickElementFromSet(activeInputs, (activeInputConnection) => activeInputConnection[0] === source && activeInputConnection[1] === output);
};

// node_modules/standardized-audio-context/build/es2019/helpers/delete-event-listeners-of-audio-node.js
var deleteEventListenerOfAudioNode = (audioNode, eventListener) => {
  const eventListeners = getEventListenersOfAudioNode(audioNode);
  if (!eventListeners.delete(eventListener)) {
    throw new Error("Missing the expected event listener.");
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/delete-passive-input-connection-to-audio-param.js
var deletePassiveInputConnectionToAudioParam = (passiveInputs, source, output) => {
  const passiveInputConnections = getValueForKey(passiveInputs, source);
  const matchingConnection = pickElementFromSet(passiveInputConnections, (passiveInputConnection) => passiveInputConnection[0] === output);
  if (passiveInputConnections.size === 0) {
    passiveInputs.delete(source);
  }
  return matchingConnection;
};

// node_modules/standardized-audio-context/build/es2019/helpers/disconnect-native-audio-node-from-native-audio-node.js
var disconnectNativeAudioNodeFromNativeAudioNode = (nativeSourceAudioNode, nativeDestinationAudioNode, output, input) => {
  if (isNativeAudioNodeFaker(nativeDestinationAudioNode)) {
    nativeSourceAudioNode.disconnect(nativeDestinationAudioNode.inputs[input], output, 0);
  } else {
    nativeSourceAudioNode.disconnect(nativeDestinationAudioNode, output, input);
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/get-native-audio-node.js
var getNativeAudioNode = (audioNode) => {
  return getValueForKey(AUDIO_NODE_STORE, audioNode);
};

// node_modules/standardized-audio-context/build/es2019/helpers/get-native-audio-param.js
var getNativeAudioParam = (audioParam) => {
  return getValueForKey(AUDIO_PARAM_STORE, audioParam);
};

// node_modules/standardized-audio-context/build/es2019/helpers/is-part-of-a-cycle.js
var isPartOfACycle = (audioNode) => {
  return CYCLE_COUNTERS.has(audioNode);
};

// node_modules/standardized-audio-context/build/es2019/helpers/is-passive-audio-node.js
var isPassiveAudioNode = (audioNode) => {
  return !ACTIVE_AUDIO_NODE_STORE.has(audioNode);
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-node-disconnect-method-support.js
var testAudioNodeDisconnectMethodSupport = (nativeAudioContext, nativeAudioWorkletNodeConstructor2) => {
  return new Promise((resolve) => {
    if (nativeAudioWorkletNodeConstructor2 !== null) {
      resolve(true);
    } else {
      const analyzer = nativeAudioContext.createScriptProcessor(256, 1, 1);
      const dummy = nativeAudioContext.createGain();
      const ones = nativeAudioContext.createBuffer(1, 2, 44100);
      const channelData = ones.getChannelData(0);
      channelData[0] = 1;
      channelData[1] = 1;
      const source = nativeAudioContext.createBufferSource();
      source.buffer = ones;
      source.loop = true;
      source.connect(analyzer).connect(nativeAudioContext.destination);
      source.connect(dummy);
      source.disconnect(dummy);
      analyzer.onaudioprocess = (event) => {
        const chnnlDt = event.inputBuffer.getChannelData(0);
        if (Array.prototype.some.call(chnnlDt, (sample) => sample === 1)) {
          resolve(true);
        } else {
          resolve(false);
        }
        source.stop();
        analyzer.onaudioprocess = null;
        source.disconnect(analyzer);
        analyzer.disconnect(nativeAudioContext.destination);
      };
      source.start();
    }
  });
};

// node_modules/standardized-audio-context/build/es2019/helpers/visit-each-audio-node-once.js
var visitEachAudioNodeOnce = (cycles, visitor) => {
  const counts = /* @__PURE__ */ new Map();
  for (const cycle of cycles) {
    for (const audioNode of cycle) {
      const count = counts.get(audioNode);
      counts.set(audioNode, count === void 0 ? 1 : count + 1);
    }
  }
  counts.forEach((count, audioNode) => visitor(audioNode, count));
};

// node_modules/standardized-audio-context/build/es2019/guards/native-audio-node.js
var isNativeAudioNode = (nativeAudioNodeOrAudioParam) => {
  return "context" in nativeAudioNodeOrAudioParam;
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-node-disconnect-method.js
var wrapAudioNodeDisconnectMethod = (nativeAudioNode) => {
  const connections = /* @__PURE__ */ new Map();
  nativeAudioNode.connect = /* @__PURE__ */ ((connect2) => {
    return (destination, output = 0, input = 0) => {
      const returnValue = isNativeAudioNode(destination) ? connect2(destination, output, input) : connect2(destination, output);
      const connectionsToDestination = connections.get(destination);
      if (connectionsToDestination === void 0) {
        connections.set(destination, [{ input, output }]);
      } else {
        if (connectionsToDestination.every((connection) => connection.input !== input || connection.output !== output)) {
          connectionsToDestination.push({ input, output });
        }
      }
      return returnValue;
    };
  })(nativeAudioNode.connect.bind(nativeAudioNode));
  nativeAudioNode.disconnect = /* @__PURE__ */ ((disconnect2) => {
    return (destinationOrOutput, output, input) => {
      disconnect2.apply(nativeAudioNode);
      if (destinationOrOutput === void 0) {
        connections.clear();
      } else if (typeof destinationOrOutput === "number") {
        for (const [destination, connectionsToDestination] of connections) {
          const filteredConnections = connectionsToDestination.filter((connection) => connection.output !== destinationOrOutput);
          if (filteredConnections.length === 0) {
            connections.delete(destination);
          } else {
            connections.set(destination, filteredConnections);
          }
        }
      } else if (connections.has(destinationOrOutput)) {
        if (output === void 0) {
          connections.delete(destinationOrOutput);
        } else {
          const connectionsToDestination = connections.get(destinationOrOutput);
          if (connectionsToDestination !== void 0) {
            const filteredConnections = connectionsToDestination.filter((connection) => connection.output !== output && (connection.input !== input || input === void 0));
            if (filteredConnections.length === 0) {
              connections.delete(destinationOrOutput);
            } else {
              connections.set(destinationOrOutput, filteredConnections);
            }
          }
        }
      }
      for (const [destination, connectionsToDestination] of connections) {
        connectionsToDestination.forEach((connection) => {
          if (isNativeAudioNode(destination)) {
            nativeAudioNode.connect(destination, connection.output, connection.input);
          } else {
            nativeAudioNode.connect(destination, connection.output);
          }
        });
      }
    };
  })(nativeAudioNode.disconnect);
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-node-constructor.js
var addConnectionToAudioParamOfAudioContext = (source, destination, output, isOffline) => {
  const { activeInputs, passiveInputs } = getAudioParamConnections(destination);
  const { outputs } = getAudioNodeConnections(source);
  const eventListeners = getEventListenersOfAudioNode(source);
  const eventListener = (isActive) => {
    const nativeAudioNode = getNativeAudioNode(source);
    const nativeAudioParam = getNativeAudioParam(destination);
    if (isActive) {
      const partialConnection = deletePassiveInputConnectionToAudioParam(passiveInputs, source, output);
      addActiveInputConnectionToAudioParam(activeInputs, source, partialConnection, false);
      if (!isOffline && !isPartOfACycle(source)) {
        nativeAudioNode.connect(nativeAudioParam, output);
      }
    } else {
      const partialConnection = deleteActiveInputConnectionToAudioParam(activeInputs, source, output);
      addPassiveInputConnectionToAudioParam(passiveInputs, partialConnection, false);
      if (!isOffline && !isPartOfACycle(source)) {
        nativeAudioNode.disconnect(nativeAudioParam, output);
      }
    }
  };
  if (insertElementInSet(outputs, [destination, output], (outputConnection) => outputConnection[0] === destination && outputConnection[1] === output, true)) {
    eventListeners.add(eventListener);
    if (isActiveAudioNode(source)) {
      addActiveInputConnectionToAudioParam(activeInputs, source, [output, eventListener], true);
    } else {
      addPassiveInputConnectionToAudioParam(passiveInputs, [source, output, eventListener], true);
    }
    return true;
  }
  return false;
};
var deleteInputConnectionOfAudioNode = (source, destination, output, input) => {
  const { activeInputs, passiveInputs } = getAudioNodeConnections(destination);
  const activeInputConnection = deleteActiveInputConnection(activeInputs[input], source, output);
  if (activeInputConnection === null) {
    const passiveInputConnection = deletePassiveInputConnectionToAudioNode(passiveInputs, source, output, input);
    return [passiveInputConnection[2], false];
  }
  return [activeInputConnection[2], true];
};
var deleteInputConnectionOfAudioParam = (source, destination, output) => {
  const { activeInputs, passiveInputs } = getAudioParamConnections(destination);
  const activeInputConnection = deleteActiveInputConnection(activeInputs, source, output);
  if (activeInputConnection === null) {
    const passiveInputConnection = deletePassiveInputConnectionToAudioParam(passiveInputs, source, output);
    return [passiveInputConnection[1], false];
  }
  return [activeInputConnection[2], true];
};
var deleteInputsOfAudioNode = (source, isOffline, destination, output, input) => {
  const [listener, isActive] = deleteInputConnectionOfAudioNode(source, destination, output, input);
  if (listener !== null) {
    deleteEventListenerOfAudioNode(source, listener);
    if (isActive && !isOffline && !isPartOfACycle(source)) {
      disconnectNativeAudioNodeFromNativeAudioNode(getNativeAudioNode(source), getNativeAudioNode(destination), output, input);
    }
  }
  if (isActiveAudioNode(destination)) {
    const { activeInputs } = getAudioNodeConnections(destination);
    setInternalStateToPassiveWhenNecessary(destination, activeInputs);
  }
};
var deleteInputsOfAudioParam = (source, isOffline, destination, output) => {
  const [listener, isActive] = deleteInputConnectionOfAudioParam(source, destination, output);
  if (listener !== null) {
    deleteEventListenerOfAudioNode(source, listener);
    if (isActive && !isOffline && !isPartOfACycle(source)) {
      getNativeAudioNode(source).disconnect(getNativeAudioParam(destination), output);
    }
  }
};
var deleteAnyConnection = (source, isOffline) => {
  const audioNodeConnectionsOfSource = getAudioNodeConnections(source);
  const destinations = [];
  for (const outputConnection of audioNodeConnectionsOfSource.outputs) {
    if (isAudioNodeOutputConnection(outputConnection)) {
      deleteInputsOfAudioNode(source, isOffline, ...outputConnection);
    } else {
      deleteInputsOfAudioParam(source, isOffline, ...outputConnection);
    }
    destinations.push(outputConnection[0]);
  }
  audioNodeConnectionsOfSource.outputs.clear();
  return destinations;
};
var deleteConnectionAtOutput = (source, isOffline, output) => {
  const audioNodeConnectionsOfSource = getAudioNodeConnections(source);
  const destinations = [];
  for (const outputConnection of audioNodeConnectionsOfSource.outputs) {
    if (outputConnection[1] === output) {
      if (isAudioNodeOutputConnection(outputConnection)) {
        deleteInputsOfAudioNode(source, isOffline, ...outputConnection);
      } else {
        deleteInputsOfAudioParam(source, isOffline, ...outputConnection);
      }
      destinations.push(outputConnection[0]);
      audioNodeConnectionsOfSource.outputs.delete(outputConnection);
    }
  }
  return destinations;
};
var deleteConnectionToDestination = (source, isOffline, destination, output, input) => {
  const audioNodeConnectionsOfSource = getAudioNodeConnections(source);
  return Array.from(audioNodeConnectionsOfSource.outputs).filter((outputConnection) => outputConnection[0] === destination && (output === void 0 || outputConnection[1] === output) && (input === void 0 || outputConnection[2] === input)).map((outputConnection) => {
    if (isAudioNodeOutputConnection(outputConnection)) {
      deleteInputsOfAudioNode(source, isOffline, ...outputConnection);
    } else {
      deleteInputsOfAudioParam(source, isOffline, ...outputConnection);
    }
    audioNodeConnectionsOfSource.outputs.delete(outputConnection);
    return outputConnection[0];
  });
};
var createAudioNodeConstructor = (addAudioNodeConnections, addConnectionToAudioNode, cacheTestResult2, createIncrementCycleCounter, createIndexSizeError2, createInvalidAccessError2, createNotSupportedError2, decrementCycleCounter, detectCycles, eventTargetConstructor2, getNativeContext2, isNativeAudioContext2, isNativeAudioNode3, isNativeAudioParam2, isNativeOfflineAudioContext2, nativeAudioWorkletNodeConstructor2) => {
  return class AudioNode extends eventTargetConstructor2 {
    constructor(context2, isActive, nativeAudioNode, audioNodeRenderer) {
      super(nativeAudioNode);
      this._context = context2;
      this._nativeAudioNode = nativeAudioNode;
      const nativeContext = getNativeContext2(context2);
      if (isNativeAudioContext2(nativeContext) && true !== cacheTestResult2(testAudioNodeDisconnectMethodSupport, () => {
        return testAudioNodeDisconnectMethodSupport(nativeContext, nativeAudioWorkletNodeConstructor2);
      })) {
        wrapAudioNodeDisconnectMethod(nativeAudioNode);
      }
      AUDIO_NODE_STORE.set(this, nativeAudioNode);
      EVENT_LISTENERS.set(this, /* @__PURE__ */ new Set());
      if (context2.state !== "closed" && isActive) {
        setInternalStateToActive(this);
      }
      addAudioNodeConnections(this, audioNodeRenderer, nativeAudioNode);
    }
    get channelCount() {
      return this._nativeAudioNode.channelCount;
    }
    set channelCount(value) {
      this._nativeAudioNode.channelCount = value;
    }
    get channelCountMode() {
      return this._nativeAudioNode.channelCountMode;
    }
    set channelCountMode(value) {
      this._nativeAudioNode.channelCountMode = value;
    }
    get channelInterpretation() {
      return this._nativeAudioNode.channelInterpretation;
    }
    set channelInterpretation(value) {
      this._nativeAudioNode.channelInterpretation = value;
    }
    get context() {
      return this._context;
    }
    get numberOfInputs() {
      return this._nativeAudioNode.numberOfInputs;
    }
    get numberOfOutputs() {
      return this._nativeAudioNode.numberOfOutputs;
    }
    // tslint:disable-next-line:invalid-void
    connect(destination, output = 0, input = 0) {
      if (output < 0 || output >= this._nativeAudioNode.numberOfOutputs) {
        throw createIndexSizeError2();
      }
      const nativeContext = getNativeContext2(this._context);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      if (isNativeAudioNode3(destination) || isNativeAudioParam2(destination)) {
        throw createInvalidAccessError2();
      }
      if (isAudioNode(destination)) {
        const nativeDestinationAudioNode = getNativeAudioNode(destination);
        try {
          const connection = connectNativeAudioNodeToNativeAudioNode(this._nativeAudioNode, nativeDestinationAudioNode, output, input);
          const isPassive = isPassiveAudioNode(this);
          if (isOffline || isPassive) {
            this._nativeAudioNode.disconnect(...connection);
          }
          if (this.context.state !== "closed" && !isPassive && isPassiveAudioNode(destination)) {
            setInternalStateToActive(destination);
          }
        } catch (err) {
          if (err.code === 12) {
            throw createInvalidAccessError2();
          }
          throw err;
        }
        const isNewConnectionToAudioNode = addConnectionToAudioNode(this, destination, output, input, isOffline);
        if (isNewConnectionToAudioNode) {
          const cycles = detectCycles([this], destination);
          visitEachAudioNodeOnce(cycles, createIncrementCycleCounter(isOffline));
        }
        return destination;
      }
      const nativeAudioParam = getNativeAudioParam(destination);
      if (nativeAudioParam.name === "playbackRate" && nativeAudioParam.maxValue === 1024) {
        throw createNotSupportedError2();
      }
      try {
        this._nativeAudioNode.connect(nativeAudioParam, output);
        if (isOffline || isPassiveAudioNode(this)) {
          this._nativeAudioNode.disconnect(nativeAudioParam, output);
        }
      } catch (err) {
        if (err.code === 12) {
          throw createInvalidAccessError2();
        }
        throw err;
      }
      const isNewConnectionToAudioParam = addConnectionToAudioParamOfAudioContext(this, destination, output, isOffline);
      if (isNewConnectionToAudioParam) {
        const cycles = detectCycles([this], destination);
        visitEachAudioNodeOnce(cycles, createIncrementCycleCounter(isOffline));
      }
    }
    disconnect(destinationOrOutput, output, input) {
      let destinations;
      const nativeContext = getNativeContext2(this._context);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      if (destinationOrOutput === void 0) {
        destinations = deleteAnyConnection(this, isOffline);
      } else if (typeof destinationOrOutput === "number") {
        if (destinationOrOutput < 0 || destinationOrOutput >= this.numberOfOutputs) {
          throw createIndexSizeError2();
        }
        destinations = deleteConnectionAtOutput(this, isOffline, destinationOrOutput);
      } else {
        if (output !== void 0 && (output < 0 || output >= this.numberOfOutputs)) {
          throw createIndexSizeError2();
        }
        if (isAudioNode(destinationOrOutput) && input !== void 0 && (input < 0 || input >= destinationOrOutput.numberOfInputs)) {
          throw createIndexSizeError2();
        }
        destinations = deleteConnectionToDestination(this, isOffline, destinationOrOutput, output, input);
        if (destinations.length === 0) {
          throw createInvalidAccessError2();
        }
      }
      for (const destination of destinations) {
        const cycles = detectCycles([this], destination);
        visitEachAudioNodeOnce(cycles, decrementCycleCounter);
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-param-factory.js
var import_automation_events = __toESM(require_bundle());
var createAudioParamFactory = (addAudioParamConnections, audioParamAudioNodeStore2, audioParamStore, createAudioParamRenderer2, createCancelAndHoldAutomationEvent2, createCancelScheduledValuesAutomationEvent2, createExponentialRampToValueAutomationEvent2, createLinearRampToValueAutomationEvent2, createSetTargetAutomationEvent2, createSetValueAutomationEvent2, createSetValueCurveAutomationEvent2, nativeAudioContextConstructor2, setValueAtTimeUntilPossible2) => {
  return (audioNode, isAudioParamOfOfflineAudioContext, nativeAudioParam, maxValue = null, minValue = null) => {
    const defaultValue = nativeAudioParam.value;
    const automationEventList = new import_automation_events.AutomationEventList(defaultValue);
    const audioParamRenderer = isAudioParamOfOfflineAudioContext ? createAudioParamRenderer2(automationEventList) : null;
    const audioParam = {
      get defaultValue() {
        return defaultValue;
      },
      get maxValue() {
        return maxValue === null ? nativeAudioParam.maxValue : maxValue;
      },
      get minValue() {
        return minValue === null ? nativeAudioParam.minValue : minValue;
      },
      get value() {
        return nativeAudioParam.value;
      },
      set value(value) {
        nativeAudioParam.value = value;
        audioParam.setValueAtTime(value, audioNode.context.currentTime);
      },
      cancelAndHoldAtTime(cancelTime) {
        if (typeof nativeAudioParam.cancelAndHoldAtTime === "function") {
          if (audioParamRenderer === null) {
            automationEventList.flush(audioNode.context.currentTime);
          }
          automationEventList.add(createCancelAndHoldAutomationEvent2(cancelTime));
          nativeAudioParam.cancelAndHoldAtTime(cancelTime);
        } else {
          const previousLastEvent = Array.from(automationEventList).pop();
          if (audioParamRenderer === null) {
            automationEventList.flush(audioNode.context.currentTime);
          }
          automationEventList.add(createCancelAndHoldAutomationEvent2(cancelTime));
          const currentLastEvent = Array.from(automationEventList).pop();
          nativeAudioParam.cancelScheduledValues(cancelTime);
          if (previousLastEvent !== currentLastEvent && currentLastEvent !== void 0) {
            if (currentLastEvent.type === "exponentialRampToValue") {
              nativeAudioParam.exponentialRampToValueAtTime(currentLastEvent.value, currentLastEvent.endTime);
            } else if (currentLastEvent.type === "linearRampToValue") {
              nativeAudioParam.linearRampToValueAtTime(currentLastEvent.value, currentLastEvent.endTime);
            } else if (currentLastEvent.type === "setValue") {
              nativeAudioParam.setValueAtTime(currentLastEvent.value, currentLastEvent.startTime);
            } else if (currentLastEvent.type === "setValueCurve") {
              nativeAudioParam.setValueCurveAtTime(currentLastEvent.values, currentLastEvent.startTime, currentLastEvent.duration);
            }
          }
        }
        return audioParam;
      },
      cancelScheduledValues(cancelTime) {
        if (audioParamRenderer === null) {
          automationEventList.flush(audioNode.context.currentTime);
        }
        automationEventList.add(createCancelScheduledValuesAutomationEvent2(cancelTime));
        nativeAudioParam.cancelScheduledValues(cancelTime);
        return audioParam;
      },
      exponentialRampToValueAtTime(value, endTime) {
        if (value === 0) {
          throw new RangeError();
        }
        if (!Number.isFinite(endTime) || endTime < 0) {
          throw new RangeError();
        }
        const currentTime = audioNode.context.currentTime;
        if (audioParamRenderer === null) {
          automationEventList.flush(currentTime);
        }
        if (Array.from(automationEventList).length === 0) {
          automationEventList.add(createSetValueAutomationEvent2(defaultValue, currentTime));
          nativeAudioParam.setValueAtTime(defaultValue, currentTime);
        }
        automationEventList.add(createExponentialRampToValueAutomationEvent2(value, endTime));
        nativeAudioParam.exponentialRampToValueAtTime(value, endTime);
        return audioParam;
      },
      linearRampToValueAtTime(value, endTime) {
        const currentTime = audioNode.context.currentTime;
        if (audioParamRenderer === null) {
          automationEventList.flush(currentTime);
        }
        if (Array.from(automationEventList).length === 0) {
          automationEventList.add(createSetValueAutomationEvent2(defaultValue, currentTime));
          nativeAudioParam.setValueAtTime(defaultValue, currentTime);
        }
        automationEventList.add(createLinearRampToValueAutomationEvent2(value, endTime));
        nativeAudioParam.linearRampToValueAtTime(value, endTime);
        return audioParam;
      },
      setTargetAtTime(target, startTime, timeConstant) {
        if (audioParamRenderer === null) {
          automationEventList.flush(audioNode.context.currentTime);
        }
        automationEventList.add(createSetTargetAutomationEvent2(target, startTime, timeConstant));
        nativeAudioParam.setTargetAtTime(target, startTime, timeConstant);
        return audioParam;
      },
      setValueAtTime(value, startTime) {
        if (audioParamRenderer === null) {
          automationEventList.flush(audioNode.context.currentTime);
        }
        automationEventList.add(createSetValueAutomationEvent2(value, startTime));
        nativeAudioParam.setValueAtTime(value, startTime);
        return audioParam;
      },
      setValueCurveAtTime(values, startTime, duration) {
        const convertedValues = values instanceof Float32Array ? values : new Float32Array(values);
        if (nativeAudioContextConstructor2 !== null && nativeAudioContextConstructor2.name === "webkitAudioContext") {
          const endTime = startTime + duration;
          const sampleRate = audioNode.context.sampleRate;
          const firstSample = Math.ceil(startTime * sampleRate);
          const lastSample = Math.floor(endTime * sampleRate);
          const numberOfInterpolatedValues = lastSample - firstSample;
          const interpolatedValues = new Float32Array(numberOfInterpolatedValues);
          for (let i = 0; i < numberOfInterpolatedValues; i += 1) {
            const theoreticIndex = (convertedValues.length - 1) / duration * ((firstSample + i) / sampleRate - startTime);
            const lowerIndex = Math.floor(theoreticIndex);
            const upperIndex = Math.ceil(theoreticIndex);
            interpolatedValues[i] = lowerIndex === upperIndex ? convertedValues[lowerIndex] : (1 - (theoreticIndex - lowerIndex)) * convertedValues[lowerIndex] + (1 - (upperIndex - theoreticIndex)) * convertedValues[upperIndex];
          }
          if (audioParamRenderer === null) {
            automationEventList.flush(audioNode.context.currentTime);
          }
          automationEventList.add(createSetValueCurveAutomationEvent2(interpolatedValues, startTime, duration));
          nativeAudioParam.setValueCurveAtTime(interpolatedValues, startTime, duration);
          const timeOfLastSample = lastSample / sampleRate;
          if (timeOfLastSample < endTime) {
            setValueAtTimeUntilPossible2(audioParam, interpolatedValues[interpolatedValues.length - 1], timeOfLastSample);
          }
          setValueAtTimeUntilPossible2(audioParam, convertedValues[convertedValues.length - 1], endTime);
        } else {
          if (audioParamRenderer === null) {
            automationEventList.flush(audioNode.context.currentTime);
          }
          automationEventList.add(createSetValueCurveAutomationEvent2(convertedValues, startTime, duration));
          nativeAudioParam.setValueCurveAtTime(convertedValues, startTime, duration);
        }
        return audioParam;
      }
    };
    audioParamStore.set(audioParam, nativeAudioParam);
    audioParamAudioNodeStore2.set(audioParam, audioNode);
    addAudioParamConnections(audioParam, audioParamRenderer);
    return audioParam;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-param-renderer.js
var createAudioParamRenderer = (automationEventList) => {
  return {
    replay(audioParam) {
      for (const automationEvent of automationEventList) {
        if (automationEvent.type === "exponentialRampToValue") {
          const { endTime, value } = automationEvent;
          audioParam.exponentialRampToValueAtTime(value, endTime);
        } else if (automationEvent.type === "linearRampToValue") {
          const { endTime, value } = automationEvent;
          audioParam.linearRampToValueAtTime(value, endTime);
        } else if (automationEvent.type === "setTarget") {
          const { startTime, target, timeConstant } = automationEvent;
          audioParam.setTargetAtTime(target, startTime, timeConstant);
        } else if (automationEvent.type === "setValue") {
          const { startTime, value } = automationEvent;
          audioParam.setValueAtTime(value, startTime);
        } else if (automationEvent.type === "setValueCurve") {
          const { duration, startTime, values } = automationEvent;
          audioParam.setValueCurveAtTime(values, startTime, duration);
        } else {
          throw new Error("Can't apply an unknown automation.");
        }
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/read-only-map.js
var ReadOnlyMap = class {
  constructor(parameters) {
    this._map = new Map(parameters);
  }
  get size() {
    return this._map.size;
  }
  entries() {
    return this._map.entries();
  }
  forEach(callback, thisArg = null) {
    return this._map.forEach((value, key) => callback.call(thisArg, value, key, this));
  }
  get(name) {
    return this._map.get(name);
  }
  has(name) {
    return this._map.has(name);
  }
  keys() {
    return this._map.keys();
  }
  values() {
    return this._map.values();
  }
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-worklet-node-constructor.js
var DEFAULT_OPTIONS4 = {
  channelCount: 2,
  // Bug #61: The channelCountMode should be 'max' according to the spec but is set to 'explicit' to achieve consistent behavior.
  channelCountMode: "explicit",
  channelInterpretation: "speakers",
  numberOfInputs: 1,
  numberOfOutputs: 1,
  parameterData: {},
  processorOptions: {}
};
var createAudioWorkletNodeConstructor = (addUnrenderedAudioWorkletNode2, audioNodeConstructor2, createAudioParam2, createAudioWorkletNodeRenderer2, createNativeAudioWorkletNode2, getAudioNodeConnections2, getBackupOfflineAudioContext2, getNativeContext2, isNativeOfflineAudioContext2, nativeAudioWorkletNodeConstructor2, sanitizeAudioWorkletNodeOptions2, setActiveAudioWorkletNodeInputs2, testAudioWorkletNodeOptionsClonability2, wrapEventListener2) => {
  return class AudioWorkletNode extends audioNodeConstructor2 {
    constructor(context2, name, options) {
      var _a;
      const nativeContext = getNativeContext2(context2);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const mergedOptions = sanitizeAudioWorkletNodeOptions2({ ...DEFAULT_OPTIONS4, ...options });
      testAudioWorkletNodeOptionsClonability2(mergedOptions);
      const nodeNameToProcessorConstructorMap = NODE_NAME_TO_PROCESSOR_CONSTRUCTOR_MAPS.get(nativeContext);
      const processorConstructor = nodeNameToProcessorConstructorMap === null || nodeNameToProcessorConstructorMap === void 0 ? void 0 : nodeNameToProcessorConstructorMap.get(name);
      const nativeContextOrBackupOfflineAudioContext = isOffline || nativeContext.state !== "closed" ? nativeContext : (_a = getBackupOfflineAudioContext2(nativeContext)) !== null && _a !== void 0 ? _a : nativeContext;
      const nativeAudioWorkletNode = createNativeAudioWorkletNode2(nativeContextOrBackupOfflineAudioContext, isOffline ? null : context2.baseLatency, nativeAudioWorkletNodeConstructor2, name, processorConstructor, mergedOptions);
      const audioWorkletNodeRenderer = isOffline ? createAudioWorkletNodeRenderer2(name, mergedOptions, processorConstructor) : null;
      super(context2, true, nativeAudioWorkletNode, audioWorkletNodeRenderer);
      const parameters = [];
      nativeAudioWorkletNode.parameters.forEach((nativeAudioParam, nm) => {
        const audioParam = createAudioParam2(this, isOffline, nativeAudioParam);
        parameters.push([nm, audioParam]);
      });
      this._nativeAudioWorkletNode = nativeAudioWorkletNode;
      this._onprocessorerror = null;
      this._parameters = new ReadOnlyMap(parameters);
      if (isOffline) {
        addUnrenderedAudioWorkletNode2(nativeContext, this);
      }
      const { activeInputs } = getAudioNodeConnections2(this);
      setActiveAudioWorkletNodeInputs2(nativeAudioWorkletNode, activeInputs);
    }
    get onprocessorerror() {
      return this._onprocessorerror;
    }
    set onprocessorerror(value) {
      const wrappedListener = typeof value === "function" ? wrapEventListener2(this, value) : null;
      this._nativeAudioWorkletNode.onprocessorerror = wrappedListener;
      const nativeOnProcessorError = this._nativeAudioWorkletNode.onprocessorerror;
      this._onprocessorerror = nativeOnProcessorError !== null && nativeOnProcessorError === wrappedListener ? value : nativeOnProcessorError;
    }
    get parameters() {
      if (this._parameters === null) {
        return this._nativeAudioWorkletNode.parameters;
      }
      return this._parameters;
    }
    get port() {
      return this._nativeAudioWorkletNode.port;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/copy-from-channel.js
function copyFromChannel(audioBuffer, parent, key, channelNumber, bufferOffset) {
  if (typeof audioBuffer.copyFromChannel === "function") {
    if (parent[key].byteLength === 0) {
      parent[key] = new Float32Array(128);
    }
    audioBuffer.copyFromChannel(parent[key], channelNumber, bufferOffset);
  } else {
    const channelData = audioBuffer.getChannelData(channelNumber);
    if (parent[key].byteLength === 0) {
      parent[key] = channelData.slice(bufferOffset, bufferOffset + 128);
    } else {
      const slicedInput = new Float32Array(channelData.buffer, bufferOffset * Float32Array.BYTES_PER_ELEMENT, 128);
      parent[key].set(slicedInput);
    }
  }
}

// node_modules/standardized-audio-context/build/es2019/helpers/copy-to-channel.js
var copyToChannel = (audioBuffer, parent, key, channelNumber, bufferOffset) => {
  if (typeof audioBuffer.copyToChannel === "function") {
    if (parent[key].byteLength !== 0) {
      audioBuffer.copyToChannel(parent[key], channelNumber, bufferOffset);
    }
  } else {
    if (parent[key].byteLength !== 0) {
      audioBuffer.getChannelData(channelNumber).set(parent[key], bufferOffset);
    }
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/create-nested-arrays.js
var createNestedArrays = (x, y) => {
  const arrays = [];
  for (let i = 0; i < x; i += 1) {
    const array = [];
    const length = typeof y === "number" ? y : y[i];
    for (let j = 0; j < length; j += 1) {
      array.push(new Float32Array(128));
    }
    arrays.push(array);
  }
  return arrays;
};

// node_modules/standardized-audio-context/build/es2019/helpers/get-audio-worklet-processor.js
var getAudioWorkletProcessor = (nativeOfflineAudioContext, proxy) => {
  const nodeToProcessorMap = getValueForKey(NODE_TO_PROCESSOR_MAPS, nativeOfflineAudioContext);
  const nativeAudioWorkletNode = getNativeAudioNode(proxy);
  return getValueForKey(nodeToProcessorMap, nativeAudioWorkletNode);
};

// node_modules/standardized-audio-context/build/es2019/factories/audio-worklet-node-renderer-factory.js
var processBuffer = async (proxy, renderedBuffer, nativeOfflineAudioContext, options, outputChannelCount, processorConstructor, exposeCurrentFrameAndCurrentTime2) => {
  const length = renderedBuffer === null ? Math.ceil(proxy.context.length / 128) * 128 : renderedBuffer.length;
  const numberOfInputChannels = options.channelCount * options.numberOfInputs;
  const numberOfOutputChannels = outputChannelCount.reduce((sum, value) => sum + value, 0);
  const processedBuffer = numberOfOutputChannels === 0 ? null : nativeOfflineAudioContext.createBuffer(numberOfOutputChannels, length, nativeOfflineAudioContext.sampleRate);
  if (processorConstructor === void 0) {
    throw new Error("Missing the processor constructor.");
  }
  const audioNodeConnections = getAudioNodeConnections(proxy);
  const audioWorkletProcessor = await getAudioWorkletProcessor(nativeOfflineAudioContext, proxy);
  const inputs = createNestedArrays(options.numberOfInputs, options.channelCount);
  const outputs = createNestedArrays(options.numberOfOutputs, outputChannelCount);
  const parameters = Array.from(proxy.parameters.keys()).reduce((prmtrs, name) => ({ ...prmtrs, [name]: new Float32Array(128) }), {});
  for (let i = 0; i < length; i += 128) {
    if (options.numberOfInputs > 0 && renderedBuffer !== null) {
      for (let j = 0; j < options.numberOfInputs; j += 1) {
        for (let k = 0; k < options.channelCount; k += 1) {
          copyFromChannel(renderedBuffer, inputs[j], k, k, i);
        }
      }
    }
    if (processorConstructor.parameterDescriptors !== void 0 && renderedBuffer !== null) {
      processorConstructor.parameterDescriptors.forEach(({ name }, index) => {
        copyFromChannel(renderedBuffer, parameters, name, numberOfInputChannels + index, i);
      });
    }
    for (let j = 0; j < options.numberOfInputs; j += 1) {
      for (let k = 0; k < outputChannelCount[j]; k += 1) {
        if (outputs[j][k].byteLength === 0) {
          outputs[j][k] = new Float32Array(128);
        }
      }
    }
    try {
      const potentiallyEmptyInputs = inputs.map((input, index) => {
        if (audioNodeConnections.activeInputs[index].size === 0) {
          return [];
        }
        return input;
      });
      const activeSourceFlag = exposeCurrentFrameAndCurrentTime2(i / nativeOfflineAudioContext.sampleRate, nativeOfflineAudioContext.sampleRate, () => audioWorkletProcessor.process(potentiallyEmptyInputs, outputs, parameters));
      if (processedBuffer !== null) {
        for (let j = 0, outputChannelSplitterNodeOutput = 0; j < options.numberOfOutputs; j += 1) {
          for (let k = 0; k < outputChannelCount[j]; k += 1) {
            copyToChannel(processedBuffer, outputs[j], k, outputChannelSplitterNodeOutput + k, i);
          }
          outputChannelSplitterNodeOutput += outputChannelCount[j];
        }
      }
      if (!activeSourceFlag) {
        break;
      }
    } catch (error) {
      proxy.dispatchEvent(new ErrorEvent("processorerror", {
        colno: error.colno,
        filename: error.filename,
        lineno: error.lineno,
        message: error.message
      }));
      break;
    }
  }
  return processedBuffer;
};
var createAudioWorkletNodeRendererFactory = (connectAudioParam2, connectMultipleOutputs2, createNativeAudioBufferSourceNode2, createNativeChannelMergerNode2, createNativeChannelSplitterNode2, createNativeConstantSourceNode2, createNativeGainNode2, deleteUnrenderedAudioWorkletNode2, disconnectMultipleOutputs2, exposeCurrentFrameAndCurrentTime2, getNativeAudioNode2, nativeAudioWorkletNodeConstructor2, nativeOfflineAudioContextConstructor2, renderAutomation2, renderInputsOfAudioNode2, renderNativeOfflineAudioContext2) => {
  return (name, options, processorConstructor) => {
    const renderedNativeAudioNodes = /* @__PURE__ */ new WeakMap();
    let processedBufferPromise = null;
    const createAudioNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeAudioWorkletNode = getNativeAudioNode2(proxy);
      let nativeOutputNodes = null;
      const nativeAudioWorkletNodeIsOwnedByContext = isOwnedByContext(nativeAudioWorkletNode, nativeOfflineAudioContext);
      const outputChannelCount = Array.isArray(options.outputChannelCount) ? options.outputChannelCount : Array.from(options.outputChannelCount);
      if (nativeAudioWorkletNodeConstructor2 === null) {
        const numberOfOutputChannels = outputChannelCount.reduce((sum, value) => sum + value, 0);
        const outputChannelSplitterNode = createNativeChannelSplitterNode2(nativeOfflineAudioContext, {
          channelCount: Math.max(1, numberOfOutputChannels),
          channelCountMode: "explicit",
          channelInterpretation: "discrete",
          numberOfOutputs: Math.max(1, numberOfOutputChannels)
        });
        const outputChannelMergerNodes = [];
        for (let i = 0; i < proxy.numberOfOutputs; i += 1) {
          outputChannelMergerNodes.push(createNativeChannelMergerNode2(nativeOfflineAudioContext, {
            channelCount: 1,
            channelCountMode: "explicit",
            channelInterpretation: "speakers",
            numberOfInputs: outputChannelCount[i]
          }));
        }
        const outputGainNode = createNativeGainNode2(nativeOfflineAudioContext, {
          channelCount: options.channelCount,
          channelCountMode: options.channelCountMode,
          channelInterpretation: options.channelInterpretation,
          gain: 1
        });
        outputGainNode.connect = connectMultipleOutputs2.bind(null, outputChannelMergerNodes);
        outputGainNode.disconnect = disconnectMultipleOutputs2.bind(null, outputChannelMergerNodes);
        nativeOutputNodes = [outputChannelSplitterNode, outputChannelMergerNodes, outputGainNode];
      } else if (!nativeAudioWorkletNodeIsOwnedByContext) {
        nativeAudioWorkletNode = new nativeAudioWorkletNodeConstructor2(nativeOfflineAudioContext, name);
      }
      renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeOutputNodes === null ? nativeAudioWorkletNode : nativeOutputNodes[2]);
      if (nativeOutputNodes !== null) {
        if (processedBufferPromise === null) {
          if (processorConstructor === void 0) {
            throw new Error("Missing the processor constructor.");
          }
          if (nativeOfflineAudioContextConstructor2 === null) {
            throw new Error("Missing the native OfflineAudioContext constructor.");
          }
          const numberOfInputChannels = proxy.channelCount * proxy.numberOfInputs;
          const numberOfParameters = processorConstructor.parameterDescriptors === void 0 ? 0 : processorConstructor.parameterDescriptors.length;
          const numberOfChannels = numberOfInputChannels + numberOfParameters;
          const renderBuffer = async () => {
            const partialOfflineAudioContext = new nativeOfflineAudioContextConstructor2(
              numberOfChannels,
              // Ceil the length to the next full render quantum.
              // Bug #17: Safari does not yet expose the length.
              Math.ceil(proxy.context.length / 128) * 128,
              nativeOfflineAudioContext.sampleRate
            );
            const gainNodes = [];
            const inputChannelSplitterNodes = [];
            for (let i = 0; i < options.numberOfInputs; i += 1) {
              gainNodes.push(createNativeGainNode2(partialOfflineAudioContext, {
                channelCount: options.channelCount,
                channelCountMode: options.channelCountMode,
                channelInterpretation: options.channelInterpretation,
                gain: 1
              }));
              inputChannelSplitterNodes.push(createNativeChannelSplitterNode2(partialOfflineAudioContext, {
                channelCount: options.channelCount,
                channelCountMode: "explicit",
                channelInterpretation: "discrete",
                numberOfOutputs: options.channelCount
              }));
            }
            const constantSourceNodes = await Promise.all(Array.from(proxy.parameters.values()).map(async (audioParam) => {
              const constantSourceNode = createNativeConstantSourceNode2(partialOfflineAudioContext, {
                channelCount: 1,
                channelCountMode: "explicit",
                channelInterpretation: "discrete",
                offset: audioParam.value
              });
              await renderAutomation2(partialOfflineAudioContext, audioParam, constantSourceNode.offset);
              return constantSourceNode;
            }));
            const inputChannelMergerNode = createNativeChannelMergerNode2(partialOfflineAudioContext, {
              channelCount: 1,
              channelCountMode: "explicit",
              channelInterpretation: "speakers",
              numberOfInputs: Math.max(1, numberOfInputChannels + numberOfParameters)
            });
            for (let i = 0; i < options.numberOfInputs; i += 1) {
              gainNodes[i].connect(inputChannelSplitterNodes[i]);
              for (let j = 0; j < options.channelCount; j += 1) {
                inputChannelSplitterNodes[i].connect(inputChannelMergerNode, j, i * options.channelCount + j);
              }
            }
            for (const [index, constantSourceNode] of constantSourceNodes.entries()) {
              constantSourceNode.connect(inputChannelMergerNode, 0, numberOfInputChannels + index);
              constantSourceNode.start(0);
            }
            inputChannelMergerNode.connect(partialOfflineAudioContext.destination);
            await Promise.all(gainNodes.map((gainNode) => renderInputsOfAudioNode2(proxy, partialOfflineAudioContext, gainNode)));
            return renderNativeOfflineAudioContext2(partialOfflineAudioContext);
          };
          processedBufferPromise = processBuffer(proxy, numberOfChannels === 0 ? null : await renderBuffer(), nativeOfflineAudioContext, options, outputChannelCount, processorConstructor, exposeCurrentFrameAndCurrentTime2);
        }
        const processedBuffer = await processedBufferPromise;
        const audioBufferSourceNode = createNativeAudioBufferSourceNode2(nativeOfflineAudioContext, {
          buffer: null,
          channelCount: 2,
          channelCountMode: "max",
          channelInterpretation: "speakers",
          loop: false,
          loopEnd: 0,
          loopStart: 0,
          playbackRate: 1
        });
        const [outputChannelSplitterNode, outputChannelMergerNodes, outputGainNode] = nativeOutputNodes;
        if (processedBuffer !== null) {
          audioBufferSourceNode.buffer = processedBuffer;
          audioBufferSourceNode.start(0);
        }
        audioBufferSourceNode.connect(outputChannelSplitterNode);
        for (let i = 0, outputChannelSplitterNodeOutput = 0; i < proxy.numberOfOutputs; i += 1) {
          const outputChannelMergerNode = outputChannelMergerNodes[i];
          for (let j = 0; j < outputChannelCount[i]; j += 1) {
            outputChannelSplitterNode.connect(outputChannelMergerNode, outputChannelSplitterNodeOutput + j, j);
          }
          outputChannelSplitterNodeOutput += outputChannelCount[i];
        }
        return outputGainNode;
      }
      if (!nativeAudioWorkletNodeIsOwnedByContext) {
        for (const [nm, audioParam] of proxy.parameters.entries()) {
          await renderAutomation2(
            nativeOfflineAudioContext,
            audioParam,
            // @todo The definition that TypeScript uses of the AudioParamMap is lacking many methods.
            nativeAudioWorkletNode.parameters.get(nm)
          );
        }
      } else {
        for (const [nm, audioParam] of proxy.parameters.entries()) {
          await connectAudioParam2(
            nativeOfflineAudioContext,
            audioParam,
            // @todo The definition that TypeScript uses of the AudioParamMap is lacking many methods.
            nativeAudioWorkletNode.parameters.get(nm)
          );
        }
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeAudioWorkletNode);
      return nativeAudioWorkletNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        deleteUnrenderedAudioWorkletNode2(nativeOfflineAudioContext, proxy);
        const renderedNativeAudioWorkletNodeOrGainNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);
        if (renderedNativeAudioWorkletNodeOrGainNode !== void 0) {
          return Promise.resolve(renderedNativeAudioWorkletNodeOrGainNode);
        }
        return createAudioNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/base-audio-context-constructor.js
var createBaseAudioContextConstructor = (addAudioWorkletModule2, analyserNodeConstructor2, audioBufferConstructor2, audioBufferSourceNodeConstructor2, biquadFilterNodeConstructor2, channelMergerNodeConstructor2, channelSplitterNodeConstructor2, constantSourceNodeConstructor2, convolverNodeConstructor2, decodeAudioData2, delayNodeConstructor2, dynamicsCompressorNodeConstructor2, gainNodeConstructor2, iIRFilterNodeConstructor2, minimalBaseAudioContextConstructor2, oscillatorNodeConstructor2, pannerNodeConstructor2, periodicWaveConstructor2, stereoPannerNodeConstructor2, waveShaperNodeConstructor2) => {
  return class BaseAudioContext extends minimalBaseAudioContextConstructor2 {
    constructor(_nativeContext, numberOfChannels) {
      super(_nativeContext, numberOfChannels);
      this._nativeContext = _nativeContext;
      this._audioWorklet = addAudioWorkletModule2 === void 0 ? void 0 : {
        addModule: (moduleURL, options) => {
          return addAudioWorkletModule2(this, moduleURL, options);
        }
      };
    }
    get audioWorklet() {
      return this._audioWorklet;
    }
    createAnalyser() {
      return new analyserNodeConstructor2(this);
    }
    createBiquadFilter() {
      return new biquadFilterNodeConstructor2(this);
    }
    createBuffer(numberOfChannels, length, sampleRate) {
      return new audioBufferConstructor2({ length, numberOfChannels, sampleRate });
    }
    createBufferSource() {
      return new audioBufferSourceNodeConstructor2(this);
    }
    createChannelMerger(numberOfInputs = 6) {
      return new channelMergerNodeConstructor2(this, { numberOfInputs });
    }
    createChannelSplitter(numberOfOutputs = 6) {
      return new channelSplitterNodeConstructor2(this, { numberOfOutputs });
    }
    createConstantSource() {
      return new constantSourceNodeConstructor2(this);
    }
    createConvolver() {
      return new convolverNodeConstructor2(this);
    }
    createDelay(maxDelayTime = 1) {
      return new delayNodeConstructor2(this, { maxDelayTime });
    }
    createDynamicsCompressor() {
      return new dynamicsCompressorNodeConstructor2(this);
    }
    createGain() {
      return new gainNodeConstructor2(this);
    }
    createIIRFilter(feedforward, feedback) {
      return new iIRFilterNodeConstructor2(this, { feedback, feedforward });
    }
    createOscillator() {
      return new oscillatorNodeConstructor2(this);
    }
    createPanner() {
      return new pannerNodeConstructor2(this);
    }
    createPeriodicWave(real, imag, constraints = { disableNormalization: false }) {
      return new periodicWaveConstructor2(this, { ...constraints, imag, real });
    }
    createStereoPanner() {
      return new stereoPannerNodeConstructor2(this);
    }
    createWaveShaper() {
      return new waveShaperNodeConstructor2(this);
    }
    decodeAudioData(audioData, successCallback, errorCallback) {
      return decodeAudioData2(this._nativeContext, audioData).then((audioBuffer) => {
        if (typeof successCallback === "function") {
          successCallback(audioBuffer);
        }
        return audioBuffer;
      }, (err) => {
        if (typeof errorCallback === "function") {
          errorCallback(err);
        }
        throw err;
      });
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/biquad-filter-node-constructor.js
var DEFAULT_OPTIONS5 = {
  Q: 1,
  channelCount: 2,
  channelCountMode: "max",
  channelInterpretation: "speakers",
  detune: 0,
  frequency: 350,
  gain: 0,
  type: "lowpass"
};
var createBiquadFilterNodeConstructor = (audioNodeConstructor2, createAudioParam2, createBiquadFilterNodeRenderer2, createInvalidAccessError2, createNativeBiquadFilterNode2, getNativeContext2, isNativeOfflineAudioContext2, setAudioNodeTailTime2) => {
  return class BiquadFilterNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS5, ...options };
      const nativeBiquadFilterNode = createNativeBiquadFilterNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const biquadFilterNodeRenderer = isOffline ? createBiquadFilterNodeRenderer2() : null;
      super(context2, false, nativeBiquadFilterNode, biquadFilterNodeRenderer);
      this._Q = createAudioParam2(this, isOffline, nativeBiquadFilterNode.Q, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
      this._detune = createAudioParam2(this, isOffline, nativeBiquadFilterNode.detune, 1200 * Math.log2(MOST_POSITIVE_SINGLE_FLOAT), -1200 * Math.log2(MOST_POSITIVE_SINGLE_FLOAT));
      this._frequency = createAudioParam2(this, isOffline, nativeBiquadFilterNode.frequency, context2.sampleRate / 2, 0);
      this._gain = createAudioParam2(this, isOffline, nativeBiquadFilterNode.gain, 40 * Math.log10(MOST_POSITIVE_SINGLE_FLOAT), MOST_NEGATIVE_SINGLE_FLOAT);
      this._nativeBiquadFilterNode = nativeBiquadFilterNode;
      setAudioNodeTailTime2(this, 1);
    }
    get detune() {
      return this._detune;
    }
    get frequency() {
      return this._frequency;
    }
    get gain() {
      return this._gain;
    }
    get Q() {
      return this._Q;
    }
    get type() {
      return this._nativeBiquadFilterNode.type;
    }
    set type(value) {
      this._nativeBiquadFilterNode.type = value;
    }
    getFrequencyResponse(frequencyHz, magResponse, phaseResponse) {
      try {
        this._nativeBiquadFilterNode.getFrequencyResponse(frequencyHz, magResponse, phaseResponse);
      } catch (err) {
        if (err.code === 11) {
          throw createInvalidAccessError2();
        }
        throw err;
      }
      if (frequencyHz.length !== magResponse.length || magResponse.length !== phaseResponse.length) {
        throw createInvalidAccessError2();
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/biquad-filter-node-renderer-factory.js
var createBiquadFilterNodeRendererFactory = (connectAudioParam2, createNativeBiquadFilterNode2, getNativeAudioNode2, renderAutomation2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeBiquadFilterNodes = /* @__PURE__ */ new WeakMap();
    const createBiquadFilterNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeBiquadFilterNode = getNativeAudioNode2(proxy);
      const nativeBiquadFilterNodeIsOwnedByContext = isOwnedByContext(nativeBiquadFilterNode, nativeOfflineAudioContext);
      if (!nativeBiquadFilterNodeIsOwnedByContext) {
        const options = {
          Q: nativeBiquadFilterNode.Q.value,
          channelCount: nativeBiquadFilterNode.channelCount,
          channelCountMode: nativeBiquadFilterNode.channelCountMode,
          channelInterpretation: nativeBiquadFilterNode.channelInterpretation,
          detune: nativeBiquadFilterNode.detune.value,
          frequency: nativeBiquadFilterNode.frequency.value,
          gain: nativeBiquadFilterNode.gain.value,
          type: nativeBiquadFilterNode.type
        };
        nativeBiquadFilterNode = createNativeBiquadFilterNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeBiquadFilterNodes.set(nativeOfflineAudioContext, nativeBiquadFilterNode);
      if (!nativeBiquadFilterNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.Q, nativeBiquadFilterNode.Q);
        await renderAutomation2(nativeOfflineAudioContext, proxy.detune, nativeBiquadFilterNode.detune);
        await renderAutomation2(nativeOfflineAudioContext, proxy.frequency, nativeBiquadFilterNode.frequency);
        await renderAutomation2(nativeOfflineAudioContext, proxy.gain, nativeBiquadFilterNode.gain);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.Q, nativeBiquadFilterNode.Q);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.detune, nativeBiquadFilterNode.detune);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.frequency, nativeBiquadFilterNode.frequency);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.gain, nativeBiquadFilterNode.gain);
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeBiquadFilterNode);
      return nativeBiquadFilterNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeBiquadFilterNode = renderedNativeBiquadFilterNodes.get(nativeOfflineAudioContext);
        if (renderedNativeBiquadFilterNode !== void 0) {
          return Promise.resolve(renderedNativeBiquadFilterNode);
        }
        return createBiquadFilterNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/cache-test-result.js
var createCacheTestResult = (ongoingTests, testResults) => {
  return (tester, test) => {
    const cachedTestResult = testResults.get(tester);
    if (cachedTestResult !== void 0) {
      return cachedTestResult;
    }
    const ongoingTest = ongoingTests.get(tester);
    if (ongoingTest !== void 0) {
      return ongoingTest;
    }
    try {
      const synchronousTestResult = test();
      if (synchronousTestResult instanceof Promise) {
        ongoingTests.set(tester, synchronousTestResult);
        return synchronousTestResult.catch(() => false).then((finalTestResult) => {
          ongoingTests.delete(tester);
          testResults.set(tester, finalTestResult);
          return finalTestResult;
        });
      }
      testResults.set(tester, synchronousTestResult);
      return synchronousTestResult;
    } catch {
      testResults.set(tester, false);
      return false;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/channel-merger-node-constructor.js
var DEFAULT_OPTIONS6 = {
  channelCount: 1,
  channelCountMode: "explicit",
  channelInterpretation: "speakers",
  numberOfInputs: 6
};
var createChannelMergerNodeConstructor = (audioNodeConstructor2, createChannelMergerNodeRenderer2, createNativeChannelMergerNode2, getNativeContext2, isNativeOfflineAudioContext2) => {
  return class ChannelMergerNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS6, ...options };
      const nativeChannelMergerNode = createNativeChannelMergerNode2(nativeContext, mergedOptions);
      const channelMergerNodeRenderer = isNativeOfflineAudioContext2(nativeContext) ? createChannelMergerNodeRenderer2() : null;
      super(context2, false, nativeChannelMergerNode, channelMergerNodeRenderer);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/channel-merger-node-renderer-factory.js
var createChannelMergerNodeRendererFactory = (createNativeChannelMergerNode2, getNativeAudioNode2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeAudioNodes = /* @__PURE__ */ new WeakMap();
    const createAudioNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeAudioNode = getNativeAudioNode2(proxy);
      const nativeAudioNodeIsOwnedByContext = isOwnedByContext(nativeAudioNode, nativeOfflineAudioContext);
      if (!nativeAudioNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeAudioNode.channelCount,
          channelCountMode: nativeAudioNode.channelCountMode,
          channelInterpretation: nativeAudioNode.channelInterpretation,
          numberOfInputs: nativeAudioNode.numberOfInputs
        };
        nativeAudioNode = createNativeChannelMergerNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeAudioNode);
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeAudioNode);
      return nativeAudioNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeAudioNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);
        if (renderedNativeAudioNode !== void 0) {
          return Promise.resolve(renderedNativeAudioNode);
        }
        return createAudioNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/channel-splitter-node-constructor.js
var DEFAULT_OPTIONS7 = {
  channelCount: 6,
  channelCountMode: "explicit",
  channelInterpretation: "discrete",
  numberOfOutputs: 6
};
var createChannelSplitterNodeConstructor = (audioNodeConstructor2, createChannelSplitterNodeRenderer2, createNativeChannelSplitterNode2, getNativeContext2, isNativeOfflineAudioContext2, sanitizeChannelSplitterOptions2) => {
  return class ChannelSplitterNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = sanitizeChannelSplitterOptions2({ ...DEFAULT_OPTIONS7, ...options });
      const nativeChannelSplitterNode = createNativeChannelSplitterNode2(nativeContext, mergedOptions);
      const channelSplitterNodeRenderer = isNativeOfflineAudioContext2(nativeContext) ? createChannelSplitterNodeRenderer2() : null;
      super(context2, false, nativeChannelSplitterNode, channelSplitterNodeRenderer);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/channel-splitter-node-renderer-factory.js
var createChannelSplitterNodeRendererFactory = (createNativeChannelSplitterNode2, getNativeAudioNode2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeAudioNodes = /* @__PURE__ */ new WeakMap();
    const createAudioNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeAudioNode = getNativeAudioNode2(proxy);
      const nativeAudioNodeIsOwnedByContext = isOwnedByContext(nativeAudioNode, nativeOfflineAudioContext);
      if (!nativeAudioNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeAudioNode.channelCount,
          channelCountMode: nativeAudioNode.channelCountMode,
          channelInterpretation: nativeAudioNode.channelInterpretation,
          numberOfOutputs: nativeAudioNode.numberOfOutputs
        };
        nativeAudioNode = createNativeChannelSplitterNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeAudioNode);
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeAudioNode);
      return nativeAudioNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeAudioNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);
        if (renderedNativeAudioNode !== void 0) {
          return Promise.resolve(renderedNativeAudioNode);
        }
        return createAudioNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/connect-audio-param.js
var createConnectAudioParam = (renderInputsOfAudioParam2) => {
  return (nativeOfflineAudioContext, audioParam, nativeAudioParam) => {
    return renderInputsOfAudioParam2(audioParam, nativeOfflineAudioContext, nativeAudioParam);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/connect-multiple-outputs.js
var createConnectMultipleOutputs = (createIndexSizeError2) => {
  return (outputAudioNodes, destination, output = 0, input = 0) => {
    const outputAudioNode = outputAudioNodes[output];
    if (outputAudioNode === void 0) {
      throw createIndexSizeError2();
    }
    if (isNativeAudioNode(destination)) {
      return outputAudioNode.connect(destination, 0, input);
    }
    return outputAudioNode.connect(destination, 0);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/connected-native-audio-buffer-source-node-factory.js
var createConnectedNativeAudioBufferSourceNodeFactory = (createNativeAudioBufferSourceNode2) => {
  return (nativeContext, nativeAudioNode) => {
    const nativeAudioBufferSourceNode = createNativeAudioBufferSourceNode2(nativeContext, {
      buffer: null,
      channelCount: 2,
      channelCountMode: "max",
      channelInterpretation: "speakers",
      loop: false,
      loopEnd: 0,
      loopStart: 0,
      playbackRate: 1
    });
    const nativeAudioBuffer = nativeContext.createBuffer(1, 2, 44100);
    nativeAudioBufferSourceNode.buffer = nativeAudioBuffer;
    nativeAudioBufferSourceNode.loop = true;
    nativeAudioBufferSourceNode.connect(nativeAudioNode);
    nativeAudioBufferSourceNode.start();
    return () => {
      nativeAudioBufferSourceNode.stop();
      nativeAudioBufferSourceNode.disconnect(nativeAudioNode);
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/constant-source-node-constructor.js
var DEFAULT_OPTIONS8 = {
  channelCount: 2,
  channelCountMode: "max",
  channelInterpretation: "speakers",
  offset: 1
};
var createConstantSourceNodeConstructor = (audioNodeConstructor2, createAudioParam2, createConstantSourceNodeRendererFactory2, createNativeConstantSourceNode2, getNativeContext2, isNativeOfflineAudioContext2, wrapEventListener2) => {
  return class ConstantSourceNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS8, ...options };
      const nativeConstantSourceNode = createNativeConstantSourceNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const constantSourceNodeRenderer = isOffline ? createConstantSourceNodeRendererFactory2() : null;
      super(context2, false, nativeConstantSourceNode, constantSourceNodeRenderer);
      this._constantSourceNodeRenderer = constantSourceNodeRenderer;
      this._nativeConstantSourceNode = nativeConstantSourceNode;
      this._offset = createAudioParam2(this, isOffline, nativeConstantSourceNode.offset, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
      this._onended = null;
    }
    get offset() {
      return this._offset;
    }
    get onended() {
      return this._onended;
    }
    set onended(value) {
      const wrappedListener = typeof value === "function" ? wrapEventListener2(this, value) : null;
      this._nativeConstantSourceNode.onended = wrappedListener;
      const nativeOnEnded = this._nativeConstantSourceNode.onended;
      this._onended = nativeOnEnded !== null && nativeOnEnded === wrappedListener ? value : nativeOnEnded;
    }
    start(when = 0) {
      this._nativeConstantSourceNode.start(when);
      if (this._constantSourceNodeRenderer !== null) {
        this._constantSourceNodeRenderer.start = when;
      }
      if (this.context.state !== "closed") {
        setInternalStateToActive(this);
        const resetInternalStateToPassive = () => {
          this._nativeConstantSourceNode.removeEventListener("ended", resetInternalStateToPassive);
          if (isActiveAudioNode(this)) {
            setInternalStateToPassive(this);
          }
        };
        this._nativeConstantSourceNode.addEventListener("ended", resetInternalStateToPassive);
      }
    }
    stop(when = 0) {
      this._nativeConstantSourceNode.stop(when);
      if (this._constantSourceNodeRenderer !== null) {
        this._constantSourceNodeRenderer.stop = when;
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/constant-source-node-renderer-factory.js
var createConstantSourceNodeRendererFactory = (connectAudioParam2, createNativeConstantSourceNode2, getNativeAudioNode2, renderAutomation2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeConstantSourceNodes = /* @__PURE__ */ new WeakMap();
    let start2 = null;
    let stop = null;
    const createConstantSourceNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeConstantSourceNode = getNativeAudioNode2(proxy);
      const nativeConstantSourceNodeIsOwnedByContext = isOwnedByContext(nativeConstantSourceNode, nativeOfflineAudioContext);
      if (!nativeConstantSourceNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeConstantSourceNode.channelCount,
          channelCountMode: nativeConstantSourceNode.channelCountMode,
          channelInterpretation: nativeConstantSourceNode.channelInterpretation,
          offset: nativeConstantSourceNode.offset.value
        };
        nativeConstantSourceNode = createNativeConstantSourceNode2(nativeOfflineAudioContext, options);
        if (start2 !== null) {
          nativeConstantSourceNode.start(start2);
        }
        if (stop !== null) {
          nativeConstantSourceNode.stop(stop);
        }
      }
      renderedNativeConstantSourceNodes.set(nativeOfflineAudioContext, nativeConstantSourceNode);
      if (!nativeConstantSourceNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.offset, nativeConstantSourceNode.offset);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.offset, nativeConstantSourceNode.offset);
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeConstantSourceNode);
      return nativeConstantSourceNode;
    };
    return {
      set start(value) {
        start2 = value;
      },
      set stop(value) {
        stop = value;
      },
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeConstantSourceNode = renderedNativeConstantSourceNodes.get(nativeOfflineAudioContext);
        if (renderedNativeConstantSourceNode !== void 0) {
          return Promise.resolve(renderedNativeConstantSourceNode);
        }
        return createConstantSourceNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/convert-number-to-unsigned-long.js
var createConvertNumberToUnsignedLong = (unit32Array) => {
  return (value) => {
    unit32Array[0] = value;
    return unit32Array[0];
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/convolver-node-constructor.js
var DEFAULT_OPTIONS9 = {
  buffer: null,
  channelCount: 2,
  channelCountMode: "clamped-max",
  channelInterpretation: "speakers",
  disableNormalization: false
};
var createConvolverNodeConstructor = (audioNodeConstructor2, createConvolverNodeRenderer2, createNativeConvolverNode2, getNativeContext2, isNativeOfflineAudioContext2, setAudioNodeTailTime2) => {
  return class ConvolverNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS9, ...options };
      const nativeConvolverNode = createNativeConvolverNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const convolverNodeRenderer = isOffline ? createConvolverNodeRenderer2() : null;
      super(context2, false, nativeConvolverNode, convolverNodeRenderer);
      this._isBufferNullified = false;
      this._nativeConvolverNode = nativeConvolverNode;
      if (mergedOptions.buffer !== null) {
        setAudioNodeTailTime2(this, mergedOptions.buffer.duration);
      }
    }
    get buffer() {
      if (this._isBufferNullified) {
        return null;
      }
      return this._nativeConvolverNode.buffer;
    }
    set buffer(value) {
      this._nativeConvolverNode.buffer = value;
      if (value === null && this._nativeConvolverNode.buffer !== null) {
        const nativeContext = this._nativeConvolverNode.context;
        this._nativeConvolverNode.buffer = nativeContext.createBuffer(1, 1, nativeContext.sampleRate);
        this._isBufferNullified = true;
        setAudioNodeTailTime2(this, 0);
      } else {
        this._isBufferNullified = false;
        setAudioNodeTailTime2(this, this._nativeConvolverNode.buffer === null ? 0 : this._nativeConvolverNode.buffer.duration);
      }
    }
    get normalize() {
      return this._nativeConvolverNode.normalize;
    }
    set normalize(value) {
      this._nativeConvolverNode.normalize = value;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/convolver-node-renderer-factory.js
var createConvolverNodeRendererFactory = (createNativeConvolverNode2, getNativeAudioNode2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeConvolverNodes = /* @__PURE__ */ new WeakMap();
    const createConvolverNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeConvolverNode = getNativeAudioNode2(proxy);
      const nativeConvolverNodeIsOwnedByContext = isOwnedByContext(nativeConvolverNode, nativeOfflineAudioContext);
      if (!nativeConvolverNodeIsOwnedByContext) {
        const options = {
          buffer: nativeConvolverNode.buffer,
          channelCount: nativeConvolverNode.channelCount,
          channelCountMode: nativeConvolverNode.channelCountMode,
          channelInterpretation: nativeConvolverNode.channelInterpretation,
          disableNormalization: !nativeConvolverNode.normalize
        };
        nativeConvolverNode = createNativeConvolverNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeConvolverNodes.set(nativeOfflineAudioContext, nativeConvolverNode);
      if (isNativeAudioNodeFaker(nativeConvolverNode)) {
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeConvolverNode.inputs[0]);
      } else {
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeConvolverNode);
      }
      return nativeConvolverNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeConvolverNode = renderedNativeConvolverNodes.get(nativeOfflineAudioContext);
        if (renderedNativeConvolverNode !== void 0) {
          return Promise.resolve(renderedNativeConvolverNode);
        }
        return createConvolverNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/create-native-offline-audio-context.js
var createCreateNativeOfflineAudioContext = (createNotSupportedError2, nativeOfflineAudioContextConstructor2) => {
  return (numberOfChannels, length, sampleRate) => {
    if (nativeOfflineAudioContextConstructor2 === null) {
      throw new Error("Missing the native OfflineAudioContext constructor.");
    }
    try {
      return new nativeOfflineAudioContextConstructor2(numberOfChannels, length, sampleRate);
    } catch (err) {
      if (err.name === "SyntaxError") {
        throw createNotSupportedError2();
      }
      throw err;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/data-clone-error.js
var createDataCloneError = () => new DOMException("", "DataCloneError");

// node_modules/standardized-audio-context/build/es2019/helpers/detach-array-buffer.js
var detachArrayBuffer = (arrayBuffer) => {
  const { port1, port2 } = new MessageChannel();
  return new Promise((resolve) => {
    const closeAndResolve = () => {
      port2.onmessage = null;
      port1.close();
      port2.close();
      resolve();
    };
    port2.onmessage = () => closeAndResolve();
    try {
      port1.postMessage(arrayBuffer, [arrayBuffer]);
    } catch {
    } finally {
      closeAndResolve();
    }
  });
};

// node_modules/standardized-audio-context/build/es2019/factories/decode-audio-data.js
var createDecodeAudioData = (audioBufferStore2, cacheTestResult2, createDataCloneError2, createEncodingError2, detachedArrayBuffers, getNativeContext2, isNativeContext2, testAudioBufferCopyChannelMethodsOutOfBoundsSupport2, testPromiseSupport2, wrapAudioBufferCopyChannelMethods2, wrapAudioBufferCopyChannelMethodsOutOfBounds2) => {
  return (anyContext, audioData) => {
    const nativeContext = isNativeContext2(anyContext) ? anyContext : getNativeContext2(anyContext);
    if (detachedArrayBuffers.has(audioData)) {
      const err = createDataCloneError2();
      return Promise.reject(err);
    }
    try {
      detachedArrayBuffers.add(audioData);
    } catch {
    }
    if (cacheTestResult2(testPromiseSupport2, () => testPromiseSupport2(nativeContext))) {
      return nativeContext.decodeAudioData(audioData).then((audioBuffer) => {
        detachArrayBuffer(audioData).catch(() => {
        });
        if (!cacheTestResult2(testAudioBufferCopyChannelMethodsOutOfBoundsSupport2, () => testAudioBufferCopyChannelMethodsOutOfBoundsSupport2(audioBuffer))) {
          wrapAudioBufferCopyChannelMethodsOutOfBounds2(audioBuffer);
        }
        audioBufferStore2.add(audioBuffer);
        return audioBuffer;
      });
    }
    return new Promise((resolve, reject) => {
      const complete = async () => {
        try {
          await detachArrayBuffer(audioData);
        } catch {
        }
      };
      const fail = (err) => {
        reject(err);
        complete();
      };
      try {
        nativeContext.decodeAudioData(audioData, (audioBuffer) => {
          if (typeof audioBuffer.copyFromChannel !== "function") {
            wrapAudioBufferCopyChannelMethods2(audioBuffer);
            wrapAudioBufferGetChannelDataMethod(audioBuffer);
          }
          audioBufferStore2.add(audioBuffer);
          complete().then(() => resolve(audioBuffer));
        }, (err) => {
          if (err === null) {
            fail(createEncodingError2());
          } else {
            fail(err);
          }
        });
      } catch (err) {
        fail(err);
      }
    });
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/decrement-cycle-counter.js
var createDecrementCycleCounter = (connectNativeAudioNodeToNativeAudioNode2, cycleCounters, getAudioNodeConnections2, getNativeAudioNode2, getNativeAudioParam2, getNativeContext2, isActiveAudioNode2, isNativeOfflineAudioContext2) => {
  return (audioNode, count) => {
    const cycleCounter = cycleCounters.get(audioNode);
    if (cycleCounter === void 0) {
      throw new Error("Missing the expected cycle count.");
    }
    const nativeContext = getNativeContext2(audioNode.context);
    const isOffline = isNativeOfflineAudioContext2(nativeContext);
    if (cycleCounter === count) {
      cycleCounters.delete(audioNode);
      if (!isOffline && isActiveAudioNode2(audioNode)) {
        const nativeSourceAudioNode = getNativeAudioNode2(audioNode);
        const { outputs } = getAudioNodeConnections2(audioNode);
        for (const output of outputs) {
          if (isAudioNodeOutputConnection(output)) {
            const nativeDestinationAudioNode = getNativeAudioNode2(output[0]);
            connectNativeAudioNodeToNativeAudioNode2(nativeSourceAudioNode, nativeDestinationAudioNode, output[1], output[2]);
          } else {
            const nativeDestinationAudioParam = getNativeAudioParam2(output[0]);
            nativeSourceAudioNode.connect(nativeDestinationAudioParam, output[1]);
          }
        }
      }
    } else {
      cycleCounters.set(audioNode, cycleCounter - count);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/delay-node-constructor.js
var DEFAULT_OPTIONS10 = {
  channelCount: 2,
  channelCountMode: "max",
  channelInterpretation: "speakers",
  delayTime: 0,
  maxDelayTime: 1
};
var createDelayNodeConstructor = (audioNodeConstructor2, createAudioParam2, createDelayNodeRenderer2, createNativeDelayNode2, getNativeContext2, isNativeOfflineAudioContext2, setAudioNodeTailTime2) => {
  return class DelayNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS10, ...options };
      const nativeDelayNode = createNativeDelayNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const delayNodeRenderer = isOffline ? createDelayNodeRenderer2(mergedOptions.maxDelayTime) : null;
      super(context2, false, nativeDelayNode, delayNodeRenderer);
      this._delayTime = createAudioParam2(this, isOffline, nativeDelayNode.delayTime);
      setAudioNodeTailTime2(this, mergedOptions.maxDelayTime);
    }
    get delayTime() {
      return this._delayTime;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/delay-node-renderer-factory.js
var createDelayNodeRendererFactory = (connectAudioParam2, createNativeDelayNode2, getNativeAudioNode2, renderAutomation2, renderInputsOfAudioNode2) => {
  return (maxDelayTime) => {
    const renderedNativeDelayNodes = /* @__PURE__ */ new WeakMap();
    const createDelayNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeDelayNode = getNativeAudioNode2(proxy);
      const nativeDelayNodeIsOwnedByContext = isOwnedByContext(nativeDelayNode, nativeOfflineAudioContext);
      if (!nativeDelayNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeDelayNode.channelCount,
          channelCountMode: nativeDelayNode.channelCountMode,
          channelInterpretation: nativeDelayNode.channelInterpretation,
          delayTime: nativeDelayNode.delayTime.value,
          maxDelayTime
        };
        nativeDelayNode = createNativeDelayNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeDelayNodes.set(nativeOfflineAudioContext, nativeDelayNode);
      if (!nativeDelayNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.delayTime, nativeDelayNode.delayTime);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.delayTime, nativeDelayNode.delayTime);
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeDelayNode);
      return nativeDelayNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeDelayNode = renderedNativeDelayNodes.get(nativeOfflineAudioContext);
        if (renderedNativeDelayNode !== void 0) {
          return Promise.resolve(renderedNativeDelayNode);
        }
        return createDelayNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/delete-active-input-connection-to-audio-node.js
var createDeleteActiveInputConnectionToAudioNode = (pickElementFromSet2) => {
  return (activeInputs, source, output, input) => {
    return pickElementFromSet2(activeInputs[input], (activeInputConnection) => activeInputConnection[0] === source && activeInputConnection[1] === output);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/delete-unrendered-audio-worklet-node.js
var createDeleteUnrenderedAudioWorkletNode = (getUnrenderedAudioWorkletNodes2) => {
  return (nativeContext, audioWorkletNode) => {
    getUnrenderedAudioWorkletNodes2(nativeContext).delete(audioWorkletNode);
  };
};

// node_modules/standardized-audio-context/build/es2019/guards/delay-node.js
var isDelayNode = (audioNode) => {
  return "delayTime" in audioNode;
};

// node_modules/standardized-audio-context/build/es2019/factories/detect-cycles.js
var createDetectCycles = (audioParamAudioNodeStore2, getAudioNodeConnections2, getValueForKey2) => {
  return function detectCycles(chain, nextLink) {
    const audioNode = isAudioNode(nextLink) ? nextLink : getValueForKey2(audioParamAudioNodeStore2, nextLink);
    if (isDelayNode(audioNode)) {
      return [];
    }
    if (chain[0] === audioNode) {
      return [chain];
    }
    if (chain.includes(audioNode)) {
      return [];
    }
    const { outputs } = getAudioNodeConnections2(audioNode);
    return Array.from(outputs).map((outputConnection) => detectCycles([...chain, audioNode], outputConnection[0])).reduce((mergedCycles, nestedCycles) => mergedCycles.concat(nestedCycles), []);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/disconnect-multiple-outputs.js
var getOutputAudioNodeAtIndex = (createIndexSizeError2, outputAudioNodes, output) => {
  const outputAudioNode = outputAudioNodes[output];
  if (outputAudioNode === void 0) {
    throw createIndexSizeError2();
  }
  return outputAudioNode;
};
var createDisconnectMultipleOutputs = (createIndexSizeError2) => {
  return (outputAudioNodes, destinationOrOutput = void 0, output = void 0, input = 0) => {
    if (destinationOrOutput === void 0) {
      return outputAudioNodes.forEach((outputAudioNode) => outputAudioNode.disconnect());
    }
    if (typeof destinationOrOutput === "number") {
      return getOutputAudioNodeAtIndex(createIndexSizeError2, outputAudioNodes, destinationOrOutput).disconnect();
    }
    if (isNativeAudioNode(destinationOrOutput)) {
      if (output === void 0) {
        return outputAudioNodes.forEach((outputAudioNode) => outputAudioNode.disconnect(destinationOrOutput));
      }
      if (input === void 0) {
        return getOutputAudioNodeAtIndex(createIndexSizeError2, outputAudioNodes, output).disconnect(destinationOrOutput, 0);
      }
      return getOutputAudioNodeAtIndex(createIndexSizeError2, outputAudioNodes, output).disconnect(destinationOrOutput, 0, input);
    }
    if (output === void 0) {
      return outputAudioNodes.forEach((outputAudioNode) => outputAudioNode.disconnect(destinationOrOutput));
    }
    return getOutputAudioNodeAtIndex(createIndexSizeError2, outputAudioNodes, output).disconnect(destinationOrOutput, 0);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/dynamics-compressor-node-constructor.js
var DEFAULT_OPTIONS11 = {
  attack: 3e-3,
  channelCount: 2,
  channelCountMode: "clamped-max",
  channelInterpretation: "speakers",
  knee: 30,
  ratio: 12,
  release: 0.25,
  threshold: -24
};
var createDynamicsCompressorNodeConstructor = (audioNodeConstructor2, createAudioParam2, createDynamicsCompressorNodeRenderer2, createNativeDynamicsCompressorNode2, createNotSupportedError2, getNativeContext2, isNativeOfflineAudioContext2, setAudioNodeTailTime2) => {
  return class DynamicsCompressorNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS11, ...options };
      const nativeDynamicsCompressorNode = createNativeDynamicsCompressorNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const dynamicsCompressorNodeRenderer = isOffline ? createDynamicsCompressorNodeRenderer2() : null;
      super(context2, false, nativeDynamicsCompressorNode, dynamicsCompressorNodeRenderer);
      this._attack = createAudioParam2(this, isOffline, nativeDynamicsCompressorNode.attack);
      this._knee = createAudioParam2(this, isOffline, nativeDynamicsCompressorNode.knee);
      this._nativeDynamicsCompressorNode = nativeDynamicsCompressorNode;
      this._ratio = createAudioParam2(this, isOffline, nativeDynamicsCompressorNode.ratio);
      this._release = createAudioParam2(this, isOffline, nativeDynamicsCompressorNode.release);
      this._threshold = createAudioParam2(this, isOffline, nativeDynamicsCompressorNode.threshold);
      setAudioNodeTailTime2(this, 6e-3);
    }
    get attack() {
      return this._attack;
    }
    // Bug #108: Safari allows a channelCount of three and above which is why the getter and setter needs to be overwritten here.
    get channelCount() {
      return this._nativeDynamicsCompressorNode.channelCount;
    }
    set channelCount(value) {
      const previousChannelCount = this._nativeDynamicsCompressorNode.channelCount;
      this._nativeDynamicsCompressorNode.channelCount = value;
      if (value > 2) {
        this._nativeDynamicsCompressorNode.channelCount = previousChannelCount;
        throw createNotSupportedError2();
      }
    }
    /*
     * Bug #109: Only Chrome and Firefox disallow a channelCountMode of 'max' yet which is why the getter and setter needs to be
     * overwritten here.
     */
    get channelCountMode() {
      return this._nativeDynamicsCompressorNode.channelCountMode;
    }
    set channelCountMode(value) {
      const previousChannelCount = this._nativeDynamicsCompressorNode.channelCountMode;
      this._nativeDynamicsCompressorNode.channelCountMode = value;
      if (value === "max") {
        this._nativeDynamicsCompressorNode.channelCountMode = previousChannelCount;
        throw createNotSupportedError2();
      }
    }
    get knee() {
      return this._knee;
    }
    get ratio() {
      return this._ratio;
    }
    get reduction() {
      if (typeof this._nativeDynamicsCompressorNode.reduction.value === "number") {
        return this._nativeDynamicsCompressorNode.reduction.value;
      }
      return this._nativeDynamicsCompressorNode.reduction;
    }
    get release() {
      return this._release;
    }
    get threshold() {
      return this._threshold;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/dynamics-compressor-node-renderer-factory.js
var createDynamicsCompressorNodeRendererFactory = (connectAudioParam2, createNativeDynamicsCompressorNode2, getNativeAudioNode2, renderAutomation2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeDynamicsCompressorNodes = /* @__PURE__ */ new WeakMap();
    const createDynamicsCompressorNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeDynamicsCompressorNode = getNativeAudioNode2(proxy);
      const nativeDynamicsCompressorNodeIsOwnedByContext = isOwnedByContext(nativeDynamicsCompressorNode, nativeOfflineAudioContext);
      if (!nativeDynamicsCompressorNodeIsOwnedByContext) {
        const options = {
          attack: nativeDynamicsCompressorNode.attack.value,
          channelCount: nativeDynamicsCompressorNode.channelCount,
          channelCountMode: nativeDynamicsCompressorNode.channelCountMode,
          channelInterpretation: nativeDynamicsCompressorNode.channelInterpretation,
          knee: nativeDynamicsCompressorNode.knee.value,
          ratio: nativeDynamicsCompressorNode.ratio.value,
          release: nativeDynamicsCompressorNode.release.value,
          threshold: nativeDynamicsCompressorNode.threshold.value
        };
        nativeDynamicsCompressorNode = createNativeDynamicsCompressorNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeDynamicsCompressorNodes.set(nativeOfflineAudioContext, nativeDynamicsCompressorNode);
      if (!nativeDynamicsCompressorNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.attack, nativeDynamicsCompressorNode.attack);
        await renderAutomation2(nativeOfflineAudioContext, proxy.knee, nativeDynamicsCompressorNode.knee);
        await renderAutomation2(nativeOfflineAudioContext, proxy.ratio, nativeDynamicsCompressorNode.ratio);
        await renderAutomation2(nativeOfflineAudioContext, proxy.release, nativeDynamicsCompressorNode.release);
        await renderAutomation2(nativeOfflineAudioContext, proxy.threshold, nativeDynamicsCompressorNode.threshold);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.attack, nativeDynamicsCompressorNode.attack);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.knee, nativeDynamicsCompressorNode.knee);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.ratio, nativeDynamicsCompressorNode.ratio);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.release, nativeDynamicsCompressorNode.release);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.threshold, nativeDynamicsCompressorNode.threshold);
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeDynamicsCompressorNode);
      return nativeDynamicsCompressorNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeDynamicsCompressorNode = renderedNativeDynamicsCompressorNodes.get(nativeOfflineAudioContext);
        if (renderedNativeDynamicsCompressorNode !== void 0) {
          return Promise.resolve(renderedNativeDynamicsCompressorNode);
        }
        return createDynamicsCompressorNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/encoding-error.js
var createEncodingError = () => new DOMException("", "EncodingError");

// node_modules/standardized-audio-context/build/es2019/factories/evaluate-source.js
var createEvaluateSource = (window3) => {
  return (source) => new Promise((resolve, reject) => {
    if (window3 === null) {
      reject(new SyntaxError());
      return;
    }
    const head = window3.document.head;
    if (head === null) {
      reject(new SyntaxError());
    } else {
      const script = window3.document.createElement("script");
      const blob = new Blob([source], { type: "application/javascript" });
      const url = URL.createObjectURL(blob);
      const originalOnErrorHandler = window3.onerror;
      const removeErrorEventListenerAndRevokeUrl = () => {
        window3.onerror = originalOnErrorHandler;
        URL.revokeObjectURL(url);
      };
      window3.onerror = (message, src, lineno, colno, error) => {
        if (src === url || src === window3.location.href && lineno === 1 && colno === 1) {
          removeErrorEventListenerAndRevokeUrl();
          reject(error);
          return false;
        }
        if (originalOnErrorHandler !== null) {
          return originalOnErrorHandler(message, src, lineno, colno, error);
        }
      };
      script.onerror = () => {
        removeErrorEventListenerAndRevokeUrl();
        reject(new SyntaxError());
      };
      script.onload = () => {
        removeErrorEventListenerAndRevokeUrl();
        resolve();
      };
      script.src = url;
      script.type = "module";
      head.appendChild(script);
    }
  });
};

// node_modules/standardized-audio-context/build/es2019/factories/event-target-constructor.js
var createEventTargetConstructor = (wrapEventListener2) => {
  return class EventTarget {
    constructor(_nativeEventTarget) {
      this._nativeEventTarget = _nativeEventTarget;
      this._listeners = /* @__PURE__ */ new WeakMap();
    }
    addEventListener(type, listener, options) {
      if (listener !== null) {
        let wrappedEventListener = this._listeners.get(listener);
        if (wrappedEventListener === void 0) {
          wrappedEventListener = wrapEventListener2(this, listener);
          if (typeof listener === "function") {
            this._listeners.set(listener, wrappedEventListener);
          }
        }
        this._nativeEventTarget.addEventListener(type, wrappedEventListener, options);
      }
    }
    dispatchEvent(event) {
      return this._nativeEventTarget.dispatchEvent(event);
    }
    removeEventListener(type, listener, options) {
      const wrappedEventListener = listener === null ? void 0 : this._listeners.get(listener);
      this._nativeEventTarget.removeEventListener(type, wrappedEventListener === void 0 ? null : wrappedEventListener, options);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/expose-current-frame-and-current-time.js
var createExposeCurrentFrameAndCurrentTime = (window3) => {
  return (currentTime, sampleRate, fn) => {
    Object.defineProperties(window3, {
      currentFrame: {
        configurable: true,
        get() {
          return Math.round(currentTime * sampleRate);
        }
      },
      currentTime: {
        configurable: true,
        get() {
          return currentTime;
        }
      }
    });
    try {
      return fn();
    } finally {
      if (window3 !== null) {
        delete window3.currentFrame;
        delete window3.currentTime;
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/fetch-source.js
var createFetchSource = (createAbortError2) => {
  return async (url) => {
    try {
      const response = await fetch(url);
      if (response.ok) {
        return [await response.text(), response.url];
      }
    } catch {
    }
    throw createAbortError2();
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/gain-node-constructor.js
var DEFAULT_OPTIONS12 = {
  channelCount: 2,
  channelCountMode: "max",
  channelInterpretation: "speakers",
  gain: 1
};
var createGainNodeConstructor = (audioNodeConstructor2, createAudioParam2, createGainNodeRenderer2, createNativeGainNode2, getNativeContext2, isNativeOfflineAudioContext2) => {
  return class GainNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS12, ...options };
      const nativeGainNode = createNativeGainNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const gainNodeRenderer = isOffline ? createGainNodeRenderer2() : null;
      super(context2, false, nativeGainNode, gainNodeRenderer);
      this._gain = createAudioParam2(this, isOffline, nativeGainNode.gain, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
    }
    get gain() {
      return this._gain;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/gain-node-renderer-factory.js
var createGainNodeRendererFactory = (connectAudioParam2, createNativeGainNode2, getNativeAudioNode2, renderAutomation2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeGainNodes = /* @__PURE__ */ new WeakMap();
    const createGainNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeGainNode = getNativeAudioNode2(proxy);
      const nativeGainNodeIsOwnedByContext = isOwnedByContext(nativeGainNode, nativeOfflineAudioContext);
      if (!nativeGainNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeGainNode.channelCount,
          channelCountMode: nativeGainNode.channelCountMode,
          channelInterpretation: nativeGainNode.channelInterpretation,
          gain: nativeGainNode.gain.value
        };
        nativeGainNode = createNativeGainNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeGainNodes.set(nativeOfflineAudioContext, nativeGainNode);
      if (!nativeGainNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.gain, nativeGainNode.gain);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.gain, nativeGainNode.gain);
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeGainNode);
      return nativeGainNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeGainNode = renderedNativeGainNodes.get(nativeOfflineAudioContext);
        if (renderedNativeGainNode !== void 0) {
          return Promise.resolve(renderedNativeGainNode);
        }
        return createGainNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/get-active-audio-worklet-node-inputs.js
var createGetActiveAudioWorkletNodeInputs = (activeAudioWorkletNodeInputsStore2, getValueForKey2) => {
  return (nativeAudioWorkletNode) => getValueForKey2(activeAudioWorkletNodeInputsStore2, nativeAudioWorkletNode);
};

// node_modules/standardized-audio-context/build/es2019/factories/get-audio-node-renderer.js
var createGetAudioNodeRenderer = (getAudioNodeConnections2) => {
  return (audioNode) => {
    const audioNodeConnections = getAudioNodeConnections2(audioNode);
    if (audioNodeConnections.renderer === null) {
      throw new Error("Missing the renderer of the given AudioNode in the audio graph.");
    }
    return audioNodeConnections.renderer;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/get-audio-node-tail-time.js
var createGetAudioNodeTailTime = (audioNodeTailTimeStore2) => {
  return (audioNode) => {
    var _a;
    return (_a = audioNodeTailTimeStore2.get(audioNode)) !== null && _a !== void 0 ? _a : 0;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/get-audio-param-renderer.js
var createGetAudioParamRenderer = (getAudioParamConnections2) => {
  return (audioParam) => {
    const audioParamConnections = getAudioParamConnections2(audioParam);
    if (audioParamConnections.renderer === null) {
      throw new Error("Missing the renderer of the given AudioParam in the audio graph.");
    }
    return audioParamConnections.renderer;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/get-backup-offline-audio-context.js
var createGetBackupOfflineAudioContext = (backupOfflineAudioContextStore2) => {
  return (nativeContext) => {
    return backupOfflineAudioContextStore2.get(nativeContext);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/invalid-state-error.js
var createInvalidStateError = () => new DOMException("", "InvalidStateError");

// node_modules/standardized-audio-context/build/es2019/factories/get-native-context.js
var createGetNativeContext = (contextStore) => {
  return (context2) => {
    const nativeContext = contextStore.get(context2);
    if (nativeContext === void 0) {
      throw createInvalidStateError();
    }
    return nativeContext;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/get-or-create-backup-offline-audio-context.js
var createGetOrCreateBackupOfflineAudioContext = (backupOfflineAudioContextStore2, nativeOfflineAudioContextConstructor2) => {
  return (nativeContext) => {
    let backupOfflineAudioContext = backupOfflineAudioContextStore2.get(nativeContext);
    if (backupOfflineAudioContext !== void 0) {
      return backupOfflineAudioContext;
    }
    if (nativeOfflineAudioContextConstructor2 === null) {
      throw new Error("Missing the native OfflineAudioContext constructor.");
    }
    backupOfflineAudioContext = new nativeOfflineAudioContextConstructor2(1, 1, 44100);
    backupOfflineAudioContextStore2.set(nativeContext, backupOfflineAudioContext);
    return backupOfflineAudioContext;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/get-unrendered-audio-worklet-nodes.js
var createGetUnrenderedAudioWorkletNodes = (unrenderedAudioWorkletNodeStore2) => {
  return (nativeContext) => {
    const unrenderedAudioWorkletNodes = unrenderedAudioWorkletNodeStore2.get(nativeContext);
    if (unrenderedAudioWorkletNodes === void 0) {
      throw new Error("The context has no set of AudioWorkletNodes.");
    }
    return unrenderedAudioWorkletNodes;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/invalid-access-error.js
var createInvalidAccessError = () => new DOMException("", "InvalidAccessError");

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-iir-filter-node-get-frequency-response-method.js
var wrapIIRFilterNodeGetFrequencyResponseMethod = (nativeIIRFilterNode) => {
  nativeIIRFilterNode.getFrequencyResponse = /* @__PURE__ */ ((getFrequencyResponse) => {
    return (frequencyHz, magResponse, phaseResponse) => {
      if (frequencyHz.length !== magResponse.length || magResponse.length !== phaseResponse.length) {
        throw createInvalidAccessError();
      }
      return getFrequencyResponse.call(nativeIIRFilterNode, frequencyHz, magResponse, phaseResponse);
    };
  })(nativeIIRFilterNode.getFrequencyResponse);
};

// node_modules/standardized-audio-context/build/es2019/factories/iir-filter-node-constructor.js
var DEFAULT_OPTIONS13 = {
  channelCount: 2,
  channelCountMode: "max",
  channelInterpretation: "speakers"
};
var createIIRFilterNodeConstructor = (audioNodeConstructor2, createNativeIIRFilterNode2, createIIRFilterNodeRenderer2, getNativeContext2, isNativeOfflineAudioContext2, setAudioNodeTailTime2) => {
  return class IIRFilterNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const mergedOptions = { ...DEFAULT_OPTIONS13, ...options };
      const nativeIIRFilterNode = createNativeIIRFilterNode2(nativeContext, isOffline ? null : context2.baseLatency, mergedOptions);
      const iirFilterNodeRenderer = isOffline ? createIIRFilterNodeRenderer2(mergedOptions.feedback, mergedOptions.feedforward) : null;
      super(context2, false, nativeIIRFilterNode, iirFilterNodeRenderer);
      wrapIIRFilterNodeGetFrequencyResponseMethod(nativeIIRFilterNode);
      this._nativeIIRFilterNode = nativeIIRFilterNode;
      setAudioNodeTailTime2(this, 1);
    }
    getFrequencyResponse(frequencyHz, magResponse, phaseResponse) {
      return this._nativeIIRFilterNode.getFrequencyResponse(frequencyHz, magResponse, phaseResponse);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/filter-buffer.js
var filterBuffer = (feedback, feedbackLength, feedforward, feedforwardLength, minLength, xBuffer, yBuffer, bufferIndex, bufferLength, input, output) => {
  const inputLength = input.length;
  let i = bufferIndex;
  for (let j = 0; j < inputLength; j += 1) {
    let y = feedforward[0] * input[j];
    for (let k = 1; k < minLength; k += 1) {
      const x = i - k & bufferLength - 1;
      y += feedforward[k] * xBuffer[x];
      y -= feedback[k] * yBuffer[x];
    }
    for (let k = minLength; k < feedforwardLength; k += 1) {
      y += feedforward[k] * xBuffer[i - k & bufferLength - 1];
    }
    for (let k = minLength; k < feedbackLength; k += 1) {
      y -= feedback[k] * yBuffer[i - k & bufferLength - 1];
    }
    xBuffer[i] = input[j];
    yBuffer[i] = y;
    i = i + 1 & bufferLength - 1;
    output[j] = y;
  }
  return i;
};

// node_modules/standardized-audio-context/build/es2019/factories/iir-filter-node-renderer-factory.js
var filterFullBuffer = (renderedBuffer, nativeOfflineAudioContext, feedback, feedforward) => {
  const convertedFeedback = feedback instanceof Float64Array ? feedback : new Float64Array(feedback);
  const convertedFeedforward = feedforward instanceof Float64Array ? feedforward : new Float64Array(feedforward);
  const feedbackLength = convertedFeedback.length;
  const feedforwardLength = convertedFeedforward.length;
  const minLength = Math.min(feedbackLength, feedforwardLength);
  if (convertedFeedback[0] !== 1) {
    for (let i = 0; i < feedbackLength; i += 1) {
      convertedFeedforward[i] /= convertedFeedback[0];
    }
    for (let i = 1; i < feedforwardLength; i += 1) {
      convertedFeedback[i] /= convertedFeedback[0];
    }
  }
  const bufferLength = 32;
  const xBuffer = new Float32Array(bufferLength);
  const yBuffer = new Float32Array(bufferLength);
  const filteredBuffer = nativeOfflineAudioContext.createBuffer(renderedBuffer.numberOfChannels, renderedBuffer.length, renderedBuffer.sampleRate);
  const numberOfChannels = renderedBuffer.numberOfChannels;
  for (let i = 0; i < numberOfChannels; i += 1) {
    const input = renderedBuffer.getChannelData(i);
    const output = filteredBuffer.getChannelData(i);
    xBuffer.fill(0);
    yBuffer.fill(0);
    filterBuffer(convertedFeedback, feedbackLength, convertedFeedforward, feedforwardLength, minLength, xBuffer, yBuffer, 0, bufferLength, input, output);
  }
  return filteredBuffer;
};
var createIIRFilterNodeRendererFactory = (createNativeAudioBufferSourceNode2, getNativeAudioNode2, nativeOfflineAudioContextConstructor2, renderInputsOfAudioNode2, renderNativeOfflineAudioContext2) => {
  return (feedback, feedforward) => {
    const renderedNativeAudioNodes = /* @__PURE__ */ new WeakMap();
    let filteredBufferPromise = null;
    const createAudioNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeAudioBufferSourceNode = null;
      let nativeIIRFilterNode = getNativeAudioNode2(proxy);
      const nativeIIRFilterNodeIsOwnedByContext = isOwnedByContext(nativeIIRFilterNode, nativeOfflineAudioContext);
      if (nativeOfflineAudioContext.createIIRFilter === void 0) {
        nativeAudioBufferSourceNode = createNativeAudioBufferSourceNode2(nativeOfflineAudioContext, {
          buffer: null,
          channelCount: 2,
          channelCountMode: "max",
          channelInterpretation: "speakers",
          loop: false,
          loopEnd: 0,
          loopStart: 0,
          playbackRate: 1
        });
      } else if (!nativeIIRFilterNodeIsOwnedByContext) {
        nativeIIRFilterNode = nativeOfflineAudioContext.createIIRFilter(feedforward, feedback);
      }
      renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeAudioBufferSourceNode === null ? nativeIIRFilterNode : nativeAudioBufferSourceNode);
      if (nativeAudioBufferSourceNode !== null) {
        if (filteredBufferPromise === null) {
          if (nativeOfflineAudioContextConstructor2 === null) {
            throw new Error("Missing the native OfflineAudioContext constructor.");
          }
          const partialOfflineAudioContext = new nativeOfflineAudioContextConstructor2(
            // Bug #47: The AudioDestinationNode in Safari gets not initialized correctly.
            proxy.context.destination.channelCount,
            // Bug #17: Safari does not yet expose the length.
            proxy.context.length,
            nativeOfflineAudioContext.sampleRate
          );
          filteredBufferPromise = (async () => {
            await renderInputsOfAudioNode2(proxy, partialOfflineAudioContext, partialOfflineAudioContext.destination);
            const renderedBuffer = await renderNativeOfflineAudioContext2(partialOfflineAudioContext);
            return filterFullBuffer(renderedBuffer, nativeOfflineAudioContext, feedback, feedforward);
          })();
        }
        const filteredBuffer = await filteredBufferPromise;
        nativeAudioBufferSourceNode.buffer = filteredBuffer;
        nativeAudioBufferSourceNode.start(0);
        return nativeAudioBufferSourceNode;
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeIIRFilterNode);
      return nativeIIRFilterNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeAudioNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);
        if (renderedNativeAudioNode !== void 0) {
          return Promise.resolve(renderedNativeAudioNode);
        }
        return createAudioNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/increment-cycle-counter-factory.js
var createIncrementCycleCounterFactory = (cycleCounters, disconnectNativeAudioNodeFromNativeAudioNode2, getAudioNodeConnections2, getNativeAudioNode2, getNativeAudioParam2, isActiveAudioNode2) => {
  return (isOffline) => {
    return (audioNode, count) => {
      const cycleCounter = cycleCounters.get(audioNode);
      if (cycleCounter === void 0) {
        if (!isOffline && isActiveAudioNode2(audioNode)) {
          const nativeSourceAudioNode = getNativeAudioNode2(audioNode);
          const { outputs } = getAudioNodeConnections2(audioNode);
          for (const output of outputs) {
            if (isAudioNodeOutputConnection(output)) {
              const nativeDestinationAudioNode = getNativeAudioNode2(output[0]);
              disconnectNativeAudioNodeFromNativeAudioNode2(nativeSourceAudioNode, nativeDestinationAudioNode, output[1], output[2]);
            } else {
              const nativeDestinationAudioParam = getNativeAudioParam2(output[0]);
              nativeSourceAudioNode.disconnect(nativeDestinationAudioParam, output[1]);
            }
          }
        }
        cycleCounters.set(audioNode, count);
      } else {
        cycleCounters.set(audioNode, cycleCounter + count);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/is-any-audio-context.js
var createIsAnyAudioContext = (contextStore, isNativeAudioContext2) => {
  return (anything) => {
    const nativeContext = contextStore.get(anything);
    return isNativeAudioContext2(nativeContext) || isNativeAudioContext2(anything);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/is-any-audio-node.js
var createIsAnyAudioNode = (audioNodeStore, isNativeAudioNode3) => {
  return (anything) => audioNodeStore.has(anything) || isNativeAudioNode3(anything);
};

// node_modules/standardized-audio-context/build/es2019/factories/is-any-audio-param.js
var createIsAnyAudioParam = (audioParamStore, isNativeAudioParam2) => {
  return (anything) => audioParamStore.has(anything) || isNativeAudioParam2(anything);
};

// node_modules/standardized-audio-context/build/es2019/factories/is-any-offline-audio-context.js
var createIsAnyOfflineAudioContext = (contextStore, isNativeOfflineAudioContext2) => {
  return (anything) => {
    const nativeContext = contextStore.get(anything);
    return isNativeOfflineAudioContext2(nativeContext) || isNativeOfflineAudioContext2(anything);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/is-native-audio-context.js
var createIsNativeAudioContext = (nativeAudioContextConstructor2) => {
  return (anything) => {
    return nativeAudioContextConstructor2 !== null && anything instanceof nativeAudioContextConstructor2;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/is-native-audio-node.js
var createIsNativeAudioNode = (window3) => {
  return (anything) => {
    return window3 !== null && typeof window3.AudioNode === "function" && anything instanceof window3.AudioNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/is-native-audio-param.js
var createIsNativeAudioParam = (window3) => {
  return (anything) => {
    return window3 !== null && typeof window3.AudioParam === "function" && anything instanceof window3.AudioParam;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/is-native-context.js
var createIsNativeContext = (isNativeAudioContext2, isNativeOfflineAudioContext2) => {
  return (anything) => {
    return isNativeAudioContext2(anything) || isNativeOfflineAudioContext2(anything);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/is-native-offline-audio-context.js
var createIsNativeOfflineAudioContext = (nativeOfflineAudioContextConstructor2) => {
  return (anything) => {
    return nativeOfflineAudioContextConstructor2 !== null && anything instanceof nativeOfflineAudioContextConstructor2;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/is-secure-context.js
var createIsSecureContext = (window3) => window3 !== null && window3.isSecureContext;

// node_modules/standardized-audio-context/build/es2019/factories/media-element-audio-source-node-constructor.js
var createMediaElementAudioSourceNodeConstructor = (audioNodeConstructor2, createNativeMediaElementAudioSourceNode2, getNativeContext2, isNativeOfflineAudioContext2) => {
  return class MediaElementAudioSourceNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const nativeMediaElementAudioSourceNode = createNativeMediaElementAudioSourceNode2(nativeContext, options);
      if (isNativeOfflineAudioContext2(nativeContext)) {
        throw TypeError();
      }
      super(context2, true, nativeMediaElementAudioSourceNode, null);
      this._nativeMediaElementAudioSourceNode = nativeMediaElementAudioSourceNode;
    }
    get mediaElement() {
      return this._nativeMediaElementAudioSourceNode.mediaElement;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/media-stream-audio-destination-node-constructor.js
var DEFAULT_OPTIONS14 = {
  channelCount: 2,
  channelCountMode: "explicit",
  channelInterpretation: "speakers"
};
var createMediaStreamAudioDestinationNodeConstructor = (audioNodeConstructor2, createNativeMediaStreamAudioDestinationNode2, getNativeContext2, isNativeOfflineAudioContext2) => {
  return class MediaStreamAudioDestinationNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      if (isNativeOfflineAudioContext2(nativeContext)) {
        throw new TypeError();
      }
      const mergedOptions = { ...DEFAULT_OPTIONS14, ...options };
      const nativeMediaStreamAudioDestinationNode = createNativeMediaStreamAudioDestinationNode2(nativeContext, mergedOptions);
      super(context2, false, nativeMediaStreamAudioDestinationNode, null);
      this._nativeMediaStreamAudioDestinationNode = nativeMediaStreamAudioDestinationNode;
    }
    get stream() {
      return this._nativeMediaStreamAudioDestinationNode.stream;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/media-stream-audio-source-node-constructor.js
var createMediaStreamAudioSourceNodeConstructor = (audioNodeConstructor2, createNativeMediaStreamAudioSourceNode2, getNativeContext2, isNativeOfflineAudioContext2) => {
  return class MediaStreamAudioSourceNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const nativeMediaStreamAudioSourceNode = createNativeMediaStreamAudioSourceNode2(nativeContext, options);
      if (isNativeOfflineAudioContext2(nativeContext)) {
        throw new TypeError();
      }
      super(context2, true, nativeMediaStreamAudioSourceNode, null);
      this._nativeMediaStreamAudioSourceNode = nativeMediaStreamAudioSourceNode;
    }
    get mediaStream() {
      return this._nativeMediaStreamAudioSourceNode.mediaStream;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/media-stream-track-audio-source-node-constructor.js
var createMediaStreamTrackAudioSourceNodeConstructor = (audioNodeConstructor2, createNativeMediaStreamTrackAudioSourceNode2, getNativeContext2) => {
  return class MediaStreamTrackAudioSourceNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const nativeMediaStreamTrackAudioSourceNode = createNativeMediaStreamTrackAudioSourceNode2(nativeContext, options);
      super(context2, true, nativeMediaStreamTrackAudioSourceNode, null);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/minimal-audio-context-constructor.js
var createMinimalAudioContextConstructor = (createInvalidStateError2, createNotSupportedError2, createUnknownError2, minimalBaseAudioContextConstructor2, nativeAudioContextConstructor2) => {
  return class MinimalAudioContext extends minimalBaseAudioContextConstructor2 {
    constructor(options = {}) {
      if (nativeAudioContextConstructor2 === null) {
        throw new Error("Missing the native AudioContext constructor.");
      }
      let nativeAudioContext;
      try {
        nativeAudioContext = new nativeAudioContextConstructor2(options);
      } catch (err) {
        if (err.code === 12 && err.message === "sampleRate is not in range") {
          throw createNotSupportedError2();
        }
        throw err;
      }
      if (nativeAudioContext === null) {
        throw createUnknownError2();
      }
      if (!isValidLatencyHint(options.latencyHint)) {
        throw new TypeError(`The provided value '${options.latencyHint}' is not a valid enum value of type AudioContextLatencyCategory.`);
      }
      if (options.sampleRate !== void 0 && nativeAudioContext.sampleRate !== options.sampleRate) {
        throw createNotSupportedError2();
      }
      super(nativeAudioContext, 2);
      const { latencyHint } = options;
      const { sampleRate } = nativeAudioContext;
      this._baseLatency = typeof nativeAudioContext.baseLatency === "number" ? nativeAudioContext.baseLatency : latencyHint === "balanced" ? 512 / sampleRate : latencyHint === "interactive" || latencyHint === void 0 ? 256 / sampleRate : latencyHint === "playback" ? 1024 / sampleRate : (
        /*
         * @todo The min (256) and max (16384) values are taken from the allowed bufferSize values of a
         * ScriptProcessorNode.
         */
        Math.max(2, Math.min(128, Math.round(latencyHint * sampleRate / 128))) * 128 / sampleRate
      );
      this._nativeAudioContext = nativeAudioContext;
      if (nativeAudioContextConstructor2.name === "webkitAudioContext") {
        this._nativeGainNode = nativeAudioContext.createGain();
        this._nativeOscillatorNode = nativeAudioContext.createOscillator();
        this._nativeGainNode.gain.value = 1e-37;
        this._nativeOscillatorNode.connect(this._nativeGainNode).connect(nativeAudioContext.destination);
        this._nativeOscillatorNode.start();
      } else {
        this._nativeGainNode = null;
        this._nativeOscillatorNode = null;
      }
      this._state = null;
      if (nativeAudioContext.state === "running") {
        this._state = "suspended";
        const revokeState = () => {
          if (this._state === "suspended") {
            this._state = null;
          }
          nativeAudioContext.removeEventListener("statechange", revokeState);
        };
        nativeAudioContext.addEventListener("statechange", revokeState);
      }
    }
    get baseLatency() {
      return this._baseLatency;
    }
    get state() {
      return this._state !== null ? this._state : this._nativeAudioContext.state;
    }
    close() {
      if (this.state === "closed") {
        return this._nativeAudioContext.close().then(() => {
          throw createInvalidStateError2();
        });
      }
      if (this._state === "suspended") {
        this._state = null;
      }
      return this._nativeAudioContext.close().then(() => {
        if (this._nativeGainNode !== null && this._nativeOscillatorNode !== null) {
          this._nativeOscillatorNode.stop();
          this._nativeGainNode.disconnect();
          this._nativeOscillatorNode.disconnect();
        }
        deactivateAudioGraph(this);
      });
    }
    resume() {
      if (this._state === "suspended") {
        return new Promise((resolve, reject) => {
          const resolvePromise = () => {
            this._nativeAudioContext.removeEventListener("statechange", resolvePromise);
            if (this._nativeAudioContext.state === "running") {
              resolve();
            } else {
              this.resume().then(resolve, reject);
            }
          };
          this._nativeAudioContext.addEventListener("statechange", resolvePromise);
        });
      }
      return this._nativeAudioContext.resume().catch((err) => {
        if (err === void 0 || err.code === 15) {
          throw createInvalidStateError2();
        }
        throw err;
      });
    }
    suspend() {
      return this._nativeAudioContext.suspend().catch((err) => {
        if (err === void 0) {
          throw createInvalidStateError2();
        }
        throw err;
      });
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/minimal-base-audio-context-constructor.js
var createMinimalBaseAudioContextConstructor = (audioDestinationNodeConstructor2, createAudioListener2, eventTargetConstructor2, isNativeOfflineAudioContext2, unrenderedAudioWorkletNodeStore2, wrapEventListener2) => {
  return class MinimalBaseAudioContext extends eventTargetConstructor2 {
    constructor(_nativeContext, numberOfChannels) {
      super(_nativeContext);
      this._nativeContext = _nativeContext;
      CONTEXT_STORE.set(this, _nativeContext);
      if (isNativeOfflineAudioContext2(_nativeContext)) {
        unrenderedAudioWorkletNodeStore2.set(_nativeContext, /* @__PURE__ */ new Set());
      }
      this._destination = new audioDestinationNodeConstructor2(this, numberOfChannels);
      this._listener = createAudioListener2(this, _nativeContext);
      this._onstatechange = null;
    }
    get currentTime() {
      return this._nativeContext.currentTime;
    }
    get destination() {
      return this._destination;
    }
    get listener() {
      return this._listener;
    }
    get onstatechange() {
      return this._onstatechange;
    }
    set onstatechange(value) {
      const wrappedListener = typeof value === "function" ? wrapEventListener2(this, value) : null;
      this._nativeContext.onstatechange = wrappedListener;
      const nativeOnStateChange = this._nativeContext.onstatechange;
      this._onstatechange = nativeOnStateChange !== null && nativeOnStateChange === wrappedListener ? value : nativeOnStateChange;
    }
    get sampleRate() {
      return this._nativeContext.sampleRate;
    }
    get state() {
      return this._nativeContext.state;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-promise-support.js
var testPromiseSupport = (nativeContext) => {
  const uint32Array = new Uint32Array([1179011410, 40, 1163280727, 544501094, 16, 131073, 44100, 176400, 1048580, 1635017060, 4, 0]);
  try {
    const promise = nativeContext.decodeAudioData(uint32Array.buffer, () => {
    });
    if (promise === void 0) {
      return false;
    }
    promise.catch(() => {
    });
    return true;
  } catch {
  }
  return false;
};

// node_modules/standardized-audio-context/build/es2019/factories/minimal-offline-audio-context-constructor.js
var DEFAULT_OPTIONS15 = {
  numberOfChannels: 1
};
var createMinimalOfflineAudioContextConstructor = (cacheTestResult2, createInvalidStateError2, createNativeOfflineAudioContext2, minimalBaseAudioContextConstructor2, startRendering2) => {
  return class MinimalOfflineAudioContext extends minimalBaseAudioContextConstructor2 {
    constructor(options) {
      const { length, numberOfChannels, sampleRate } = { ...DEFAULT_OPTIONS15, ...options };
      const nativeOfflineAudioContext = createNativeOfflineAudioContext2(numberOfChannels, length, sampleRate);
      if (!cacheTestResult2(testPromiseSupport, () => testPromiseSupport(nativeOfflineAudioContext))) {
        nativeOfflineAudioContext.addEventListener("statechange", /* @__PURE__ */ (() => {
          let i = 0;
          const delayStateChangeEvent = (event) => {
            if (this._state === "running") {
              if (i > 0) {
                nativeOfflineAudioContext.removeEventListener("statechange", delayStateChangeEvent);
                event.stopImmediatePropagation();
                this._waitForThePromiseToSettle(event);
              } else {
                i += 1;
              }
            }
          };
          return delayStateChangeEvent;
        })());
      }
      super(nativeOfflineAudioContext, numberOfChannels);
      this._length = length;
      this._nativeOfflineAudioContext = nativeOfflineAudioContext;
      this._state = null;
    }
    get length() {
      if (this._nativeOfflineAudioContext.length === void 0) {
        return this._length;
      }
      return this._nativeOfflineAudioContext.length;
    }
    get state() {
      return this._state === null ? this._nativeOfflineAudioContext.state : this._state;
    }
    startRendering() {
      if (this._state === "running") {
        return Promise.reject(createInvalidStateError2());
      }
      this._state = "running";
      return startRendering2(this.destination, this._nativeOfflineAudioContext).finally(() => {
        this._state = null;
        deactivateAudioGraph(this);
      });
    }
    _waitForThePromiseToSettle(event) {
      if (this._state === null) {
        this._nativeOfflineAudioContext.dispatchEvent(event);
      } else {
        setTimeout(() => this._waitForThePromiseToSettle(event));
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/monitor-connections.js
var createMonitorConnections = (insertElementInSet2, isNativeAudioNode3) => {
  return (nativeAudioNode, whenConnected, whenDisconnected) => {
    const connections = /* @__PURE__ */ new Set();
    nativeAudioNode.connect = /* @__PURE__ */ ((connect2) => {
      return (destination, output = 0, input = 0) => {
        const wasDisconnected = connections.size === 0;
        if (isNativeAudioNode3(destination)) {
          connect2.call(nativeAudioNode, destination, output, input);
          insertElementInSet2(connections, [destination, output, input], (connection) => connection[0] === destination && connection[1] === output && connection[2] === input, true);
          if (wasDisconnected) {
            whenConnected();
          }
          return destination;
        }
        connect2.call(nativeAudioNode, destination, output);
        insertElementInSet2(connections, [destination, output], (connection) => connection[0] === destination && connection[1] === output, true);
        if (wasDisconnected) {
          whenConnected();
        }
        return;
      };
    })(nativeAudioNode.connect);
    nativeAudioNode.disconnect = /* @__PURE__ */ ((disconnect2) => {
      return (destinationOrOutput, output, input) => {
        const wasConnected = connections.size > 0;
        if (destinationOrOutput === void 0) {
          disconnect2.apply(nativeAudioNode);
          connections.clear();
        } else if (typeof destinationOrOutput === "number") {
          disconnect2.call(nativeAudioNode, destinationOrOutput);
          for (const connection of connections) {
            if (connection[1] === destinationOrOutput) {
              connections.delete(connection);
            }
          }
        } else {
          if (isNativeAudioNode3(destinationOrOutput)) {
            disconnect2.call(nativeAudioNode, destinationOrOutput, output, input);
          } else {
            disconnect2.call(nativeAudioNode, destinationOrOutput, output);
          }
          for (const connection of connections) {
            if (connection[0] === destinationOrOutput && (output === void 0 || connection[1] === output) && (input === void 0 || connection[2] === input)) {
              connections.delete(connection);
            }
          }
        }
        const isDisconnected = connections.size === 0;
        if (wasConnected && isDisconnected) {
          whenDisconnected();
        }
      };
    })(nativeAudioNode.disconnect);
    return nativeAudioNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/assign-native-audio-node-option.js
var assignNativeAudioNodeOption = (nativeAudioNode, options, option) => {
  const value = options[option];
  if (value !== void 0 && value !== nativeAudioNode[option]) {
    nativeAudioNode[option] = value;
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/assign-native-audio-node-options.js
var assignNativeAudioNodeOptions = (nativeAudioNode, options) => {
  assignNativeAudioNodeOption(nativeAudioNode, options, "channelCount");
  assignNativeAudioNodeOption(nativeAudioNode, options, "channelCountMode");
  assignNativeAudioNodeOption(nativeAudioNode, options, "channelInterpretation");
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-analyser-node-get-float-time-domain-data-method-support.js
var testAnalyserNodeGetFloatTimeDomainDataMethodSupport = (nativeAnalyserNode) => {
  return typeof nativeAnalyserNode.getFloatTimeDomainData === "function";
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-analyser-node-get-float-time-domain-data-method.js
var wrapAnalyserNodeGetFloatTimeDomainDataMethod = (nativeAnalyserNode) => {
  nativeAnalyserNode.getFloatTimeDomainData = (array) => {
    const byteTimeDomainData = new Uint8Array(array.length);
    nativeAnalyserNode.getByteTimeDomainData(byteTimeDomainData);
    const length = Math.max(byteTimeDomainData.length, nativeAnalyserNode.fftSize);
    for (let i = 0; i < length; i += 1) {
      array[i] = (byteTimeDomainData[i] - 128) * 78125e-7;
    }
    return array;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-analyser-node-factory.js
var createNativeAnalyserNodeFactory = (cacheTestResult2, createIndexSizeError2) => {
  return (nativeContext, options) => {
    const nativeAnalyserNode = nativeContext.createAnalyser();
    assignNativeAudioNodeOptions(nativeAnalyserNode, options);
    if (!(options.maxDecibels > options.minDecibels)) {
      throw createIndexSizeError2();
    }
    assignNativeAudioNodeOption(nativeAnalyserNode, options, "fftSize");
    assignNativeAudioNodeOption(nativeAnalyserNode, options, "maxDecibels");
    assignNativeAudioNodeOption(nativeAnalyserNode, options, "minDecibels");
    assignNativeAudioNodeOption(nativeAnalyserNode, options, "smoothingTimeConstant");
    if (!cacheTestResult2(testAnalyserNodeGetFloatTimeDomainDataMethodSupport, () => testAnalyserNodeGetFloatTimeDomainDataMethodSupport(nativeAnalyserNode))) {
      wrapAnalyserNodeGetFloatTimeDomainDataMethod(nativeAnalyserNode);
    }
    return nativeAnalyserNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-audio-buffer-constructor.js
var createNativeAudioBufferConstructor = (window3) => {
  if (window3 === null) {
    return null;
  }
  if (window3.hasOwnProperty("AudioBuffer")) {
    return window3.AudioBuffer;
  }
  return null;
};

// node_modules/standardized-audio-context/build/es2019/helpers/assign-native-audio-node-audio-param-value.js
var assignNativeAudioNodeAudioParamValue = (nativeAudioNode, options, audioParam) => {
  const value = options[audioParam];
  if (value !== void 0 && value !== nativeAudioNode[audioParam].value) {
    nativeAudioNode[audioParam].value = value;
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-buffer-source-node-start-method-consecutive-calls.js
var wrapAudioBufferSourceNodeStartMethodConsecutiveCalls = (nativeAudioBufferSourceNode) => {
  nativeAudioBufferSourceNode.start = /* @__PURE__ */ ((start2) => {
    let isScheduled = false;
    return (when = 0, offset = 0, duration) => {
      if (isScheduled) {
        throw createInvalidStateError();
      }
      start2.call(nativeAudioBufferSourceNode, when, offset, duration);
      isScheduled = true;
    };
  })(nativeAudioBufferSourceNode.start);
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-scheduled-source-node-start-method-negative-parameters.js
var wrapAudioScheduledSourceNodeStartMethodNegativeParameters = (nativeAudioScheduledSourceNode) => {
  nativeAudioScheduledSourceNode.start = /* @__PURE__ */ ((start2) => {
    return (when = 0, offset = 0, duration) => {
      if (typeof duration === "number" && duration < 0 || offset < 0 || when < 0) {
        throw new RangeError("The parameters can't be negative.");
      }
      start2.call(nativeAudioScheduledSourceNode, when, offset, duration);
    };
  })(nativeAudioScheduledSourceNode.start);
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-scheduled-source-node-stop-method-negative-parameters.js
var wrapAudioScheduledSourceNodeStopMethodNegativeParameters = (nativeAudioScheduledSourceNode) => {
  nativeAudioScheduledSourceNode.stop = /* @__PURE__ */ ((stop) => {
    return (when = 0) => {
      if (when < 0) {
        throw new RangeError("The parameter can't be negative.");
      }
      stop.call(nativeAudioScheduledSourceNode, when);
    };
  })(nativeAudioScheduledSourceNode.stop);
};

// node_modules/standardized-audio-context/build/es2019/factories/native-audio-buffer-source-node-factory.js
var createNativeAudioBufferSourceNodeFactory = (addSilentConnection2, cacheTestResult2, testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport2, testAudioBufferSourceNodeStartMethodOffsetClampingSupport2, testAudioBufferSourceNodeStopMethodNullifiedBufferSupport2, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2, testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport2, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2, wrapAudioBufferSourceNodeStartMethodOffsetClampling, wrapAudioBufferSourceNodeStopMethodNullifiedBuffer, wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls2) => {
  return (nativeContext, options) => {
    const nativeAudioBufferSourceNode = nativeContext.createBufferSource();
    assignNativeAudioNodeOptions(nativeAudioBufferSourceNode, options);
    assignNativeAudioNodeAudioParamValue(nativeAudioBufferSourceNode, options, "playbackRate");
    assignNativeAudioNodeOption(nativeAudioBufferSourceNode, options, "buffer");
    assignNativeAudioNodeOption(nativeAudioBufferSourceNode, options, "loop");
    assignNativeAudioNodeOption(nativeAudioBufferSourceNode, options, "loopEnd");
    assignNativeAudioNodeOption(nativeAudioBufferSourceNode, options, "loopStart");
    if (!cacheTestResult2(testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport2, () => testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport2(nativeContext))) {
      wrapAudioBufferSourceNodeStartMethodConsecutiveCalls(nativeAudioBufferSourceNode);
    }
    if (!cacheTestResult2(testAudioBufferSourceNodeStartMethodOffsetClampingSupport2, () => testAudioBufferSourceNodeStartMethodOffsetClampingSupport2(nativeContext))) {
      wrapAudioBufferSourceNodeStartMethodOffsetClampling(nativeAudioBufferSourceNode);
    }
    if (!cacheTestResult2(testAudioBufferSourceNodeStopMethodNullifiedBufferSupport2, () => testAudioBufferSourceNodeStopMethodNullifiedBufferSupport2(nativeContext))) {
      wrapAudioBufferSourceNodeStopMethodNullifiedBuffer(nativeAudioBufferSourceNode, nativeContext);
    }
    if (!cacheTestResult2(testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2, () => testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2(nativeContext))) {
      wrapAudioScheduledSourceNodeStartMethodNegativeParameters(nativeAudioBufferSourceNode);
    }
    if (!cacheTestResult2(testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport2, () => testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport2(nativeContext))) {
      wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls2(nativeAudioBufferSourceNode, nativeContext);
    }
    if (!cacheTestResult2(testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2, () => testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2(nativeContext))) {
      wrapAudioScheduledSourceNodeStopMethodNegativeParameters(nativeAudioBufferSourceNode);
    }
    addSilentConnection2(nativeContext, nativeAudioBufferSourceNode);
    return nativeAudioBufferSourceNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-audio-context-constructor.js
var createNativeAudioContextConstructor = (window3) => {
  if (window3 === null) {
    return null;
  }
  if (window3.hasOwnProperty("AudioContext")) {
    return window3.AudioContext;
  }
  return window3.hasOwnProperty("webkitAudioContext") ? window3.webkitAudioContext : null;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-audio-destination-node.js
var createNativeAudioDestinationNodeFactory = (createNativeGainNode2, overwriteAccessors2) => {
  return (nativeContext, channelCount, isNodeOfNativeOfflineAudioContext) => {
    const nativeAudioDestinationNode = nativeContext.destination;
    if (nativeAudioDestinationNode.channelCount !== channelCount) {
      try {
        nativeAudioDestinationNode.channelCount = channelCount;
      } catch {
      }
    }
    if (isNodeOfNativeOfflineAudioContext && nativeAudioDestinationNode.channelCountMode !== "explicit") {
      nativeAudioDestinationNode.channelCountMode = "explicit";
    }
    if (nativeAudioDestinationNode.maxChannelCount === 0) {
      Object.defineProperty(nativeAudioDestinationNode, "maxChannelCount", {
        value: channelCount
      });
    }
    const gainNode = createNativeGainNode2(nativeContext, {
      channelCount,
      channelCountMode: nativeAudioDestinationNode.channelCountMode,
      channelInterpretation: nativeAudioDestinationNode.channelInterpretation,
      gain: 1
    });
    overwriteAccessors2(gainNode, "channelCount", (get) => () => get.call(gainNode), (set) => (value) => {
      set.call(gainNode, value);
      try {
        nativeAudioDestinationNode.channelCount = value;
      } catch (err) {
        if (value > nativeAudioDestinationNode.maxChannelCount) {
          throw err;
        }
      }
    });
    overwriteAccessors2(gainNode, "channelCountMode", (get) => () => get.call(gainNode), (set) => (value) => {
      set.call(gainNode, value);
      nativeAudioDestinationNode.channelCountMode = value;
    });
    overwriteAccessors2(gainNode, "channelInterpretation", (get) => () => get.call(gainNode), (set) => (value) => {
      set.call(gainNode, value);
      nativeAudioDestinationNode.channelInterpretation = value;
    });
    Object.defineProperty(gainNode, "maxChannelCount", {
      get: () => nativeAudioDestinationNode.maxChannelCount
    });
    gainNode.connect(nativeAudioDestinationNode);
    return gainNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-audio-worklet-node-constructor.js
var createNativeAudioWorkletNodeConstructor = (window3) => {
  if (window3 === null) {
    return null;
  }
  return window3.hasOwnProperty("AudioWorkletNode") ? window3.AudioWorkletNode : null;
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-clonability-of-audio-worklet-node-options.js
var testClonabilityOfAudioWorkletNodeOptions = (audioWorkletNodeOptions) => {
  const { port1 } = new MessageChannel();
  try {
    port1.postMessage(audioWorkletNodeOptions);
  } finally {
    port1.close();
  }
};

// node_modules/standardized-audio-context/build/es2019/factories/native-audio-worklet-node-factory.js
var createNativeAudioWorkletNodeFactory = (createInvalidStateError2, createNativeAudioWorkletNodeFaker2, createNativeGainNode2, createNotSupportedError2, monitorConnections2) => {
  return (nativeContext, baseLatency, nativeAudioWorkletNodeConstructor2, name, processorConstructor, options) => {
    if (nativeAudioWorkletNodeConstructor2 !== null) {
      try {
        const nativeAudioWorkletNode = new nativeAudioWorkletNodeConstructor2(nativeContext, name, options);
        const patchedEventListeners = /* @__PURE__ */ new Map();
        let onprocessorerror = null;
        Object.defineProperties(nativeAudioWorkletNode, {
          /*
           * Bug #61: Overwriting the property accessors for channelCount and channelCountMode is necessary as long as some
           * browsers have no native implementation to achieve a consistent behavior.
           */
          channelCount: {
            get: () => options.channelCount,
            set: () => {
              throw createInvalidStateError2();
            }
          },
          channelCountMode: {
            get: () => "explicit",
            set: () => {
              throw createInvalidStateError2();
            }
          },
          // Bug #156: Chrome and Edge do not yet fire an ErrorEvent.
          onprocessorerror: {
            get: () => onprocessorerror,
            set: (value) => {
              if (typeof onprocessorerror === "function") {
                nativeAudioWorkletNode.removeEventListener("processorerror", onprocessorerror);
              }
              onprocessorerror = typeof value === "function" ? value : null;
              if (typeof onprocessorerror === "function") {
                nativeAudioWorkletNode.addEventListener("processorerror", onprocessorerror);
              }
            }
          }
        });
        nativeAudioWorkletNode.addEventListener = /* @__PURE__ */ ((addEventListener) => {
          return (...args) => {
            if (args[0] === "processorerror") {
              const unpatchedEventListener = typeof args[1] === "function" ? args[1] : typeof args[1] === "object" && args[1] !== null && typeof args[1].handleEvent === "function" ? args[1].handleEvent : null;
              if (unpatchedEventListener !== null) {
                const patchedEventListener = patchedEventListeners.get(args[1]);
                if (patchedEventListener !== void 0) {
                  args[1] = patchedEventListener;
                } else {
                  args[1] = (event) => {
                    if (event.type === "error") {
                      Object.defineProperties(event, {
                        type: { value: "processorerror" }
                      });
                      unpatchedEventListener(event);
                    } else {
                      unpatchedEventListener(new ErrorEvent(args[0], { ...event }));
                    }
                  };
                  patchedEventListeners.set(unpatchedEventListener, args[1]);
                }
              }
            }
            addEventListener.call(nativeAudioWorkletNode, "error", args[1], args[2]);
            return addEventListener.call(nativeAudioWorkletNode, ...args);
          };
        })(nativeAudioWorkletNode.addEventListener);
        nativeAudioWorkletNode.removeEventListener = /* @__PURE__ */ ((removeEventListener) => {
          return (...args) => {
            if (args[0] === "processorerror") {
              const patchedEventListener = patchedEventListeners.get(args[1]);
              if (patchedEventListener !== void 0) {
                patchedEventListeners.delete(args[1]);
                args[1] = patchedEventListener;
              }
            }
            removeEventListener.call(nativeAudioWorkletNode, "error", args[1], args[2]);
            return removeEventListener.call(nativeAudioWorkletNode, args[0], args[1], args[2]);
          };
        })(nativeAudioWorkletNode.removeEventListener);
        if (options.numberOfOutputs !== 0) {
          const nativeGainNode = createNativeGainNode2(nativeContext, {
            channelCount: 1,
            channelCountMode: "explicit",
            channelInterpretation: "discrete",
            gain: 0
          });
          nativeAudioWorkletNode.connect(nativeGainNode).connect(nativeContext.destination);
          const whenConnected = () => nativeGainNode.disconnect();
          const whenDisconnected = () => nativeGainNode.connect(nativeContext.destination);
          return monitorConnections2(nativeAudioWorkletNode, whenConnected, whenDisconnected);
        }
        return nativeAudioWorkletNode;
      } catch (err) {
        if (err.code === 11) {
          throw createNotSupportedError2();
        }
        throw err;
      }
    }
    if (processorConstructor === void 0) {
      throw createNotSupportedError2();
    }
    testClonabilityOfAudioWorkletNodeOptions(options);
    return createNativeAudioWorkletNodeFaker2(nativeContext, baseLatency, processorConstructor, options);
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/compute-buffer-size.js
var computeBufferSize = (baseLatency, sampleRate) => {
  if (baseLatency === null) {
    return 512;
  }
  return Math.max(512, Math.min(16384, Math.pow(2, Math.round(Math.log2(baseLatency * sampleRate)))));
};

// node_modules/standardized-audio-context/build/es2019/helpers/clone-audio-worklet-node-options.js
var cloneAudioWorkletNodeOptions = (audioWorkletNodeOptions) => {
  return new Promise((resolve, reject) => {
    const { port1, port2 } = new MessageChannel();
    port1.onmessage = ({ data }) => {
      port1.close();
      port2.close();
      resolve(data);
    };
    port1.onmessageerror = ({ data }) => {
      port1.close();
      port2.close();
      reject(data);
    };
    port2.postMessage(audioWorkletNodeOptions);
  });
};

// node_modules/standardized-audio-context/build/es2019/helpers/create-audio-worklet-processor-promise.js
var createAudioWorkletProcessorPromise = async (processorConstructor, audioWorkletNodeOptions) => {
  const clonedAudioWorkletNodeOptions = await cloneAudioWorkletNodeOptions(audioWorkletNodeOptions);
  return new processorConstructor(clonedAudioWorkletNodeOptions);
};

// node_modules/standardized-audio-context/build/es2019/helpers/create-audio-worklet-processor.js
var createAudioWorkletProcessor = (nativeContext, nativeAudioWorkletNode, processorConstructor, audioWorkletNodeOptions) => {
  let nodeToProcessorMap = NODE_TO_PROCESSOR_MAPS.get(nativeContext);
  if (nodeToProcessorMap === void 0) {
    nodeToProcessorMap = /* @__PURE__ */ new WeakMap();
    NODE_TO_PROCESSOR_MAPS.set(nativeContext, nodeToProcessorMap);
  }
  const audioWorkletProcessorPromise = createAudioWorkletProcessorPromise(processorConstructor, audioWorkletNodeOptions);
  nodeToProcessorMap.set(nativeAudioWorkletNode, audioWorkletProcessorPromise);
  return audioWorkletProcessorPromise;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-audio-worklet-node-faker-factory.js
var createNativeAudioWorkletNodeFakerFactory = (connectMultipleOutputs2, createIndexSizeError2, createInvalidStateError2, createNativeChannelMergerNode2, createNativeChannelSplitterNode2, createNativeConstantSourceNode2, createNativeGainNode2, createNativeScriptProcessorNode2, createNotSupportedError2, disconnectMultipleOutputs2, exposeCurrentFrameAndCurrentTime2, getActiveAudioWorkletNodeInputs2, monitorConnections2) => {
  return (nativeContext, baseLatency, processorConstructor, options) => {
    if (options.numberOfInputs === 0 && options.numberOfOutputs === 0) {
      throw createNotSupportedError2();
    }
    const outputChannelCount = Array.isArray(options.outputChannelCount) ? options.outputChannelCount : Array.from(options.outputChannelCount);
    if (outputChannelCount.some((channelCount) => channelCount < 1)) {
      throw createNotSupportedError2();
    }
    if (outputChannelCount.length !== options.numberOfOutputs) {
      throw createIndexSizeError2();
    }
    if (options.channelCountMode !== "explicit") {
      throw createNotSupportedError2();
    }
    const numberOfInputChannels = options.channelCount * options.numberOfInputs;
    const numberOfOutputChannels = outputChannelCount.reduce((sum, value) => sum + value, 0);
    const numberOfParameters = processorConstructor.parameterDescriptors === void 0 ? 0 : processorConstructor.parameterDescriptors.length;
    if (numberOfInputChannels + numberOfParameters > 6 || numberOfOutputChannels > 6) {
      throw createNotSupportedError2();
    }
    const messageChannel = new MessageChannel();
    const gainNodes = [];
    const inputChannelSplitterNodes = [];
    for (let i = 0; i < options.numberOfInputs; i += 1) {
      gainNodes.push(createNativeGainNode2(nativeContext, {
        channelCount: options.channelCount,
        channelCountMode: options.channelCountMode,
        channelInterpretation: options.channelInterpretation,
        gain: 1
      }));
      inputChannelSplitterNodes.push(createNativeChannelSplitterNode2(nativeContext, {
        channelCount: options.channelCount,
        channelCountMode: "explicit",
        channelInterpretation: "discrete",
        numberOfOutputs: options.channelCount
      }));
    }
    const constantSourceNodes = [];
    if (processorConstructor.parameterDescriptors !== void 0) {
      for (const { defaultValue, maxValue, minValue, name } of processorConstructor.parameterDescriptors) {
        const constantSourceNode = createNativeConstantSourceNode2(nativeContext, {
          channelCount: 1,
          channelCountMode: "explicit",
          channelInterpretation: "discrete",
          offset: options.parameterData[name] !== void 0 ? options.parameterData[name] : defaultValue === void 0 ? 0 : defaultValue
        });
        Object.defineProperties(constantSourceNode.offset, {
          defaultValue: {
            get: () => defaultValue === void 0 ? 0 : defaultValue
          },
          maxValue: {
            get: () => maxValue === void 0 ? MOST_POSITIVE_SINGLE_FLOAT : maxValue
          },
          minValue: {
            get: () => minValue === void 0 ? MOST_NEGATIVE_SINGLE_FLOAT : minValue
          }
        });
        constantSourceNodes.push(constantSourceNode);
      }
    }
    const inputChannelMergerNode = createNativeChannelMergerNode2(nativeContext, {
      channelCount: 1,
      channelCountMode: "explicit",
      channelInterpretation: "speakers",
      numberOfInputs: Math.max(1, numberOfInputChannels + numberOfParameters)
    });
    const bufferSize = computeBufferSize(baseLatency, nativeContext.sampleRate);
    const scriptProcessorNode = createNativeScriptProcessorNode2(
      nativeContext,
      bufferSize,
      numberOfInputChannels + numberOfParameters,
      // Bug #87: Only Firefox will fire an AudioProcessingEvent if there is no connected output.
      Math.max(1, numberOfOutputChannels)
    );
    const outputChannelSplitterNode = createNativeChannelSplitterNode2(nativeContext, {
      channelCount: Math.max(1, numberOfOutputChannels),
      channelCountMode: "explicit",
      channelInterpretation: "discrete",
      numberOfOutputs: Math.max(1, numberOfOutputChannels)
    });
    const outputChannelMergerNodes = [];
    for (let i = 0; i < options.numberOfOutputs; i += 1) {
      outputChannelMergerNodes.push(createNativeChannelMergerNode2(nativeContext, {
        channelCount: 1,
        channelCountMode: "explicit",
        channelInterpretation: "speakers",
        numberOfInputs: outputChannelCount[i]
      }));
    }
    for (let i = 0; i < options.numberOfInputs; i += 1) {
      gainNodes[i].connect(inputChannelSplitterNodes[i]);
      for (let j = 0; j < options.channelCount; j += 1) {
        inputChannelSplitterNodes[i].connect(inputChannelMergerNode, j, i * options.channelCount + j);
      }
    }
    const parameterMap = new ReadOnlyMap(processorConstructor.parameterDescriptors === void 0 ? [] : processorConstructor.parameterDescriptors.map(({ name }, index) => {
      const constantSourceNode = constantSourceNodes[index];
      constantSourceNode.connect(inputChannelMergerNode, 0, numberOfInputChannels + index);
      constantSourceNode.start(0);
      return [name, constantSourceNode.offset];
    }));
    inputChannelMergerNode.connect(scriptProcessorNode);
    let channelInterpretation = options.channelInterpretation;
    let onprocessorerror = null;
    const outputAudioNodes = options.numberOfOutputs === 0 ? [scriptProcessorNode] : outputChannelMergerNodes;
    const nativeAudioWorkletNodeFaker = {
      get bufferSize() {
        return bufferSize;
      },
      get channelCount() {
        return options.channelCount;
      },
      set channelCount(_) {
        throw createInvalidStateError2();
      },
      get channelCountMode() {
        return options.channelCountMode;
      },
      set channelCountMode(_) {
        throw createInvalidStateError2();
      },
      get channelInterpretation() {
        return channelInterpretation;
      },
      set channelInterpretation(value) {
        for (const gainNode of gainNodes) {
          gainNode.channelInterpretation = value;
        }
        channelInterpretation = value;
      },
      get context() {
        return scriptProcessorNode.context;
      },
      get inputs() {
        return gainNodes;
      },
      get numberOfInputs() {
        return options.numberOfInputs;
      },
      get numberOfOutputs() {
        return options.numberOfOutputs;
      },
      get onprocessorerror() {
        return onprocessorerror;
      },
      set onprocessorerror(value) {
        if (typeof onprocessorerror === "function") {
          nativeAudioWorkletNodeFaker.removeEventListener("processorerror", onprocessorerror);
        }
        onprocessorerror = typeof value === "function" ? value : null;
        if (typeof onprocessorerror === "function") {
          nativeAudioWorkletNodeFaker.addEventListener("processorerror", onprocessorerror);
        }
      },
      get parameters() {
        return parameterMap;
      },
      get port() {
        return messageChannel.port2;
      },
      addEventListener(...args) {
        return scriptProcessorNode.addEventListener(args[0], args[1], args[2]);
      },
      connect: connectMultipleOutputs2.bind(null, outputAudioNodes),
      disconnect: disconnectMultipleOutputs2.bind(null, outputAudioNodes),
      dispatchEvent(...args) {
        return scriptProcessorNode.dispatchEvent(args[0]);
      },
      removeEventListener(...args) {
        return scriptProcessorNode.removeEventListener(args[0], args[1], args[2]);
      }
    };
    const patchedEventListeners = /* @__PURE__ */ new Map();
    messageChannel.port1.addEventListener = /* @__PURE__ */ ((addEventListener) => {
      return (...args) => {
        if (args[0] === "message") {
          const unpatchedEventListener = typeof args[1] === "function" ? args[1] : typeof args[1] === "object" && args[1] !== null && typeof args[1].handleEvent === "function" ? args[1].handleEvent : null;
          if (unpatchedEventListener !== null) {
            const patchedEventListener = patchedEventListeners.get(args[1]);
            if (patchedEventListener !== void 0) {
              args[1] = patchedEventListener;
            } else {
              args[1] = (event) => {
                exposeCurrentFrameAndCurrentTime2(nativeContext.currentTime, nativeContext.sampleRate, () => unpatchedEventListener(event));
              };
              patchedEventListeners.set(unpatchedEventListener, args[1]);
            }
          }
        }
        return addEventListener.call(messageChannel.port1, args[0], args[1], args[2]);
      };
    })(messageChannel.port1.addEventListener);
    messageChannel.port1.removeEventListener = /* @__PURE__ */ ((removeEventListener) => {
      return (...args) => {
        if (args[0] === "message") {
          const patchedEventListener = patchedEventListeners.get(args[1]);
          if (patchedEventListener !== void 0) {
            patchedEventListeners.delete(args[1]);
            args[1] = patchedEventListener;
          }
        }
        return removeEventListener.call(messageChannel.port1, args[0], args[1], args[2]);
      };
    })(messageChannel.port1.removeEventListener);
    let onmessage = null;
    Object.defineProperty(messageChannel.port1, "onmessage", {
      get: () => onmessage,
      set: (value) => {
        if (typeof onmessage === "function") {
          messageChannel.port1.removeEventListener("message", onmessage);
        }
        onmessage = typeof value === "function" ? value : null;
        if (typeof onmessage === "function") {
          messageChannel.port1.addEventListener("message", onmessage);
          messageChannel.port1.start();
        }
      }
    });
    processorConstructor.prototype.port = messageChannel.port1;
    let audioWorkletProcessor = null;
    const audioWorkletProcessorPromise = createAudioWorkletProcessor(nativeContext, nativeAudioWorkletNodeFaker, processorConstructor, options);
    audioWorkletProcessorPromise.then((dWrkltPrcssr) => audioWorkletProcessor = dWrkltPrcssr);
    const inputs = createNestedArrays(options.numberOfInputs, options.channelCount);
    const outputs = createNestedArrays(options.numberOfOutputs, outputChannelCount);
    const parameters = processorConstructor.parameterDescriptors === void 0 ? [] : processorConstructor.parameterDescriptors.reduce((prmtrs, { name }) => ({ ...prmtrs, [name]: new Float32Array(128) }), {});
    let isActive = true;
    const disconnectOutputsGraph = () => {
      if (options.numberOfOutputs > 0) {
        scriptProcessorNode.disconnect(outputChannelSplitterNode);
      }
      for (let i = 0, outputChannelSplitterNodeOutput = 0; i < options.numberOfOutputs; i += 1) {
        const outputChannelMergerNode = outputChannelMergerNodes[i];
        for (let j = 0; j < outputChannelCount[i]; j += 1) {
          outputChannelSplitterNode.disconnect(outputChannelMergerNode, outputChannelSplitterNodeOutput + j, j);
        }
        outputChannelSplitterNodeOutput += outputChannelCount[i];
      }
    };
    const activeInputIndexes = /* @__PURE__ */ new Map();
    scriptProcessorNode.onaudioprocess = ({ inputBuffer, outputBuffer }) => {
      if (audioWorkletProcessor !== null) {
        const activeInputs = getActiveAudioWorkletNodeInputs2(nativeAudioWorkletNodeFaker);
        for (let i = 0; i < bufferSize; i += 128) {
          for (let j = 0; j < options.numberOfInputs; j += 1) {
            for (let k = 0; k < options.channelCount; k += 1) {
              copyFromChannel(inputBuffer, inputs[j], k, k, i);
            }
          }
          if (processorConstructor.parameterDescriptors !== void 0) {
            processorConstructor.parameterDescriptors.forEach(({ name }, index) => {
              copyFromChannel(inputBuffer, parameters, name, numberOfInputChannels + index, i);
            });
          }
          for (let j = 0; j < options.numberOfInputs; j += 1) {
            for (let k = 0; k < outputChannelCount[j]; k += 1) {
              if (outputs[j][k].byteLength === 0) {
                outputs[j][k] = new Float32Array(128);
              }
            }
          }
          try {
            const potentiallyEmptyInputs = inputs.map((input, index) => {
              const activeInput = activeInputs[index];
              if (activeInput.size > 0) {
                activeInputIndexes.set(index, bufferSize / 128);
                return input;
              }
              const count = activeInputIndexes.get(index);
              if (count === void 0) {
                return [];
              }
              if (input.every((channelData) => channelData.every((sample) => sample === 0))) {
                if (count === 1) {
                  activeInputIndexes.delete(index);
                } else {
                  activeInputIndexes.set(index, count - 1);
                }
              }
              return input;
            });
            const activeSourceFlag = exposeCurrentFrameAndCurrentTime2(nativeContext.currentTime + i / nativeContext.sampleRate, nativeContext.sampleRate, () => audioWorkletProcessor.process(potentiallyEmptyInputs, outputs, parameters));
            isActive = activeSourceFlag;
            for (let j = 0, outputChannelSplitterNodeOutput = 0; j < options.numberOfOutputs; j += 1) {
              for (let k = 0; k < outputChannelCount[j]; k += 1) {
                copyToChannel(outputBuffer, outputs[j], k, outputChannelSplitterNodeOutput + k, i);
              }
              outputChannelSplitterNodeOutput += outputChannelCount[j];
            }
          } catch (error) {
            isActive = false;
            nativeAudioWorkletNodeFaker.dispatchEvent(new ErrorEvent("processorerror", {
              colno: error.colno,
              filename: error.filename,
              lineno: error.lineno,
              message: error.message
            }));
          }
          if (!isActive) {
            for (let j = 0; j < options.numberOfInputs; j += 1) {
              gainNodes[j].disconnect(inputChannelSplitterNodes[j]);
              for (let k = 0; k < options.channelCount; k += 1) {
                inputChannelSplitterNodes[i].disconnect(inputChannelMergerNode, k, j * options.channelCount + k);
              }
            }
            if (processorConstructor.parameterDescriptors !== void 0) {
              const length = processorConstructor.parameterDescriptors.length;
              for (let j = 0; j < length; j += 1) {
                const constantSourceNode = constantSourceNodes[j];
                constantSourceNode.disconnect(inputChannelMergerNode, 0, numberOfInputChannels + j);
                constantSourceNode.stop();
              }
            }
            inputChannelMergerNode.disconnect(scriptProcessorNode);
            scriptProcessorNode.onaudioprocess = null;
            if (isConnected) {
              disconnectOutputsGraph();
            } else {
              disconnectFakeGraph();
            }
            break;
          }
        }
      }
    };
    let isConnected = false;
    const nativeGainNode = createNativeGainNode2(nativeContext, {
      channelCount: 1,
      channelCountMode: "explicit",
      channelInterpretation: "discrete",
      gain: 0
    });
    const connectFakeGraph = () => scriptProcessorNode.connect(nativeGainNode).connect(nativeContext.destination);
    const disconnectFakeGraph = () => {
      scriptProcessorNode.disconnect(nativeGainNode);
      nativeGainNode.disconnect();
    };
    const whenConnected = () => {
      if (isActive) {
        disconnectFakeGraph();
        if (options.numberOfOutputs > 0) {
          scriptProcessorNode.connect(outputChannelSplitterNode);
        }
        for (let i = 0, outputChannelSplitterNodeOutput = 0; i < options.numberOfOutputs; i += 1) {
          const outputChannelMergerNode = outputChannelMergerNodes[i];
          for (let j = 0; j < outputChannelCount[i]; j += 1) {
            outputChannelSplitterNode.connect(outputChannelMergerNode, outputChannelSplitterNodeOutput + j, j);
          }
          outputChannelSplitterNodeOutput += outputChannelCount[i];
        }
      }
      isConnected = true;
    };
    const whenDisconnected = () => {
      if (isActive) {
        connectFakeGraph();
        disconnectOutputsGraph();
      }
      isConnected = false;
    };
    connectFakeGraph();
    return monitorConnections2(nativeAudioWorkletNodeFaker, whenConnected, whenDisconnected);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-biquad-filter-node.js
var createNativeBiquadFilterNode = (nativeContext, options) => {
  const nativeBiquadFilterNode = nativeContext.createBiquadFilter();
  assignNativeAudioNodeOptions(nativeBiquadFilterNode, options);
  assignNativeAudioNodeAudioParamValue(nativeBiquadFilterNode, options, "Q");
  assignNativeAudioNodeAudioParamValue(nativeBiquadFilterNode, options, "detune");
  assignNativeAudioNodeAudioParamValue(nativeBiquadFilterNode, options, "frequency");
  assignNativeAudioNodeAudioParamValue(nativeBiquadFilterNode, options, "gain");
  assignNativeAudioNodeOption(nativeBiquadFilterNode, options, "type");
  return nativeBiquadFilterNode;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-channel-merger-node-factory.js
var createNativeChannelMergerNodeFactory = (nativeAudioContextConstructor2, wrapChannelMergerNode2) => {
  return (nativeContext, options) => {
    const nativeChannelMergerNode = nativeContext.createChannelMerger(options.numberOfInputs);
    if (nativeAudioContextConstructor2 !== null && nativeAudioContextConstructor2.name === "webkitAudioContext") {
      wrapChannelMergerNode2(nativeContext, nativeChannelMergerNode);
    }
    assignNativeAudioNodeOptions(nativeChannelMergerNode, options);
    return nativeChannelMergerNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-channel-splitter-node.js
var wrapChannelSplitterNode = (channelSplitterNode) => {
  const channelCount = channelSplitterNode.numberOfOutputs;
  Object.defineProperty(channelSplitterNode, "channelCount", {
    get: () => channelCount,
    set: (value) => {
      if (value !== channelCount) {
        throw createInvalidStateError();
      }
    }
  });
  Object.defineProperty(channelSplitterNode, "channelCountMode", {
    get: () => "explicit",
    set: (value) => {
      if (value !== "explicit") {
        throw createInvalidStateError();
      }
    }
  });
  Object.defineProperty(channelSplitterNode, "channelInterpretation", {
    get: () => "discrete",
    set: (value) => {
      if (value !== "discrete") {
        throw createInvalidStateError();
      }
    }
  });
};

// node_modules/standardized-audio-context/build/es2019/factories/native-channel-splitter-node.js
var createNativeChannelSplitterNode = (nativeContext, options) => {
  const nativeChannelSplitterNode = nativeContext.createChannelSplitter(options.numberOfOutputs);
  assignNativeAudioNodeOptions(nativeChannelSplitterNode, options);
  wrapChannelSplitterNode(nativeChannelSplitterNode);
  return nativeChannelSplitterNode;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-constant-source-node-factory.js
var createNativeConstantSourceNodeFactory = (addSilentConnection2, cacheTestResult2, createNativeConstantSourceNodeFaker2, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2) => {
  return (nativeContext, options) => {
    if (nativeContext.createConstantSource === void 0) {
      return createNativeConstantSourceNodeFaker2(nativeContext, options);
    }
    const nativeConstantSourceNode = nativeContext.createConstantSource();
    assignNativeAudioNodeOptions(nativeConstantSourceNode, options);
    assignNativeAudioNodeAudioParamValue(nativeConstantSourceNode, options, "offset");
    if (!cacheTestResult2(testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2, () => testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2(nativeContext))) {
      wrapAudioScheduledSourceNodeStartMethodNegativeParameters(nativeConstantSourceNode);
    }
    if (!cacheTestResult2(testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2, () => testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2(nativeContext))) {
      wrapAudioScheduledSourceNodeStopMethodNegativeParameters(nativeConstantSourceNode);
    }
    addSilentConnection2(nativeContext, nativeConstantSourceNode);
    return nativeConstantSourceNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/intercept-connections.js
var interceptConnections = (original, interceptor) => {
  original.connect = interceptor.connect.bind(interceptor);
  original.disconnect = interceptor.disconnect.bind(interceptor);
  return original;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-constant-source-node-faker-factory.js
var createNativeConstantSourceNodeFakerFactory = (addSilentConnection2, createNativeAudioBufferSourceNode2, createNativeGainNode2, monitorConnections2) => {
  return (nativeContext, { offset, ...audioNodeOptions }) => {
    const audioBuffer = nativeContext.createBuffer(1, 2, 44100);
    const audioBufferSourceNode = createNativeAudioBufferSourceNode2(nativeContext, {
      buffer: null,
      channelCount: 2,
      channelCountMode: "max",
      channelInterpretation: "speakers",
      loop: false,
      loopEnd: 0,
      loopStart: 0,
      playbackRate: 1
    });
    const gainNode = createNativeGainNode2(nativeContext, { ...audioNodeOptions, gain: offset });
    const channelData = audioBuffer.getChannelData(0);
    channelData[0] = 1;
    channelData[1] = 1;
    audioBufferSourceNode.buffer = audioBuffer;
    audioBufferSourceNode.loop = true;
    const nativeConstantSourceNodeFaker = {
      get bufferSize() {
        return void 0;
      },
      get channelCount() {
        return gainNode.channelCount;
      },
      set channelCount(value) {
        gainNode.channelCount = value;
      },
      get channelCountMode() {
        return gainNode.channelCountMode;
      },
      set channelCountMode(value) {
        gainNode.channelCountMode = value;
      },
      get channelInterpretation() {
        return gainNode.channelInterpretation;
      },
      set channelInterpretation(value) {
        gainNode.channelInterpretation = value;
      },
      get context() {
        return gainNode.context;
      },
      get inputs() {
        return [];
      },
      get numberOfInputs() {
        return audioBufferSourceNode.numberOfInputs;
      },
      get numberOfOutputs() {
        return gainNode.numberOfOutputs;
      },
      get offset() {
        return gainNode.gain;
      },
      get onended() {
        return audioBufferSourceNode.onended;
      },
      set onended(value) {
        audioBufferSourceNode.onended = value;
      },
      addEventListener(...args) {
        return audioBufferSourceNode.addEventListener(args[0], args[1], args[2]);
      },
      dispatchEvent(...args) {
        return audioBufferSourceNode.dispatchEvent(args[0]);
      },
      removeEventListener(...args) {
        return audioBufferSourceNode.removeEventListener(args[0], args[1], args[2]);
      },
      start(when = 0) {
        audioBufferSourceNode.start.call(audioBufferSourceNode, when);
      },
      stop(when = 0) {
        audioBufferSourceNode.stop.call(audioBufferSourceNode, when);
      }
    };
    const whenConnected = () => audioBufferSourceNode.connect(gainNode);
    const whenDisconnected = () => audioBufferSourceNode.disconnect(gainNode);
    addSilentConnection2(nativeContext, audioBufferSourceNode);
    return monitorConnections2(interceptConnections(nativeConstantSourceNodeFaker, gainNode), whenConnected, whenDisconnected);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-convolver-node-factory.js
var createNativeConvolverNodeFactory = (createNotSupportedError2, overwriteAccessors2) => {
  return (nativeContext, options) => {
    const nativeConvolverNode = nativeContext.createConvolver();
    assignNativeAudioNodeOptions(nativeConvolverNode, options);
    if (options.disableNormalization === nativeConvolverNode.normalize) {
      nativeConvolverNode.normalize = !options.disableNormalization;
    }
    assignNativeAudioNodeOption(nativeConvolverNode, options, "buffer");
    if (options.channelCount > 2) {
      throw createNotSupportedError2();
    }
    overwriteAccessors2(nativeConvolverNode, "channelCount", (get) => () => get.call(nativeConvolverNode), (set) => (value) => {
      if (value > 2) {
        throw createNotSupportedError2();
      }
      return set.call(nativeConvolverNode, value);
    });
    if (options.channelCountMode === "max") {
      throw createNotSupportedError2();
    }
    overwriteAccessors2(nativeConvolverNode, "channelCountMode", (get) => () => get.call(nativeConvolverNode), (set) => (value) => {
      if (value === "max") {
        throw createNotSupportedError2();
      }
      return set.call(nativeConvolverNode, value);
    });
    return nativeConvolverNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-delay-node.js
var createNativeDelayNode = (nativeContext, options) => {
  const nativeDelayNode = nativeContext.createDelay(options.maxDelayTime);
  assignNativeAudioNodeOptions(nativeDelayNode, options);
  assignNativeAudioNodeAudioParamValue(nativeDelayNode, options, "delayTime");
  return nativeDelayNode;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-dynamics-compressor-node-factory.js
var createNativeDynamicsCompressorNodeFactory = (createNotSupportedError2) => {
  return (nativeContext, options) => {
    const nativeDynamicsCompressorNode = nativeContext.createDynamicsCompressor();
    assignNativeAudioNodeOptions(nativeDynamicsCompressorNode, options);
    if (options.channelCount > 2) {
      throw createNotSupportedError2();
    }
    if (options.channelCountMode === "max") {
      throw createNotSupportedError2();
    }
    assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, "attack");
    assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, "knee");
    assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, "ratio");
    assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, "release");
    assignNativeAudioNodeAudioParamValue(nativeDynamicsCompressorNode, options, "threshold");
    return nativeDynamicsCompressorNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-gain-node.js
var createNativeGainNode = (nativeContext, options) => {
  const nativeGainNode = nativeContext.createGain();
  assignNativeAudioNodeOptions(nativeGainNode, options);
  assignNativeAudioNodeAudioParamValue(nativeGainNode, options, "gain");
  return nativeGainNode;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-iir-filter-node-factory.js
var createNativeIIRFilterNodeFactory = (createNativeIIRFilterNodeFaker2) => {
  return (nativeContext, baseLatency, options) => {
    if (nativeContext.createIIRFilter === void 0) {
      return createNativeIIRFilterNodeFaker2(nativeContext, baseLatency, options);
    }
    const nativeIIRFilterNode = nativeContext.createIIRFilter(options.feedforward, options.feedback);
    assignNativeAudioNodeOptions(nativeIIRFilterNode, options);
    return nativeIIRFilterNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-iir-filter-node-faker-factory.js
function divide(a, b) {
  const denominator = b[0] * b[0] + b[1] * b[1];
  return [(a[0] * b[0] + a[1] * b[1]) / denominator, (a[1] * b[0] - a[0] * b[1]) / denominator];
}
function multiply(a, b) {
  return [a[0] * b[0] - a[1] * b[1], a[0] * b[1] + a[1] * b[0]];
}
function evaluatePolynomial(coefficient, z) {
  let result = [0, 0];
  for (let i = coefficient.length - 1; i >= 0; i -= 1) {
    result = multiply(result, z);
    result[0] += coefficient[i];
  }
  return result;
}
var createNativeIIRFilterNodeFakerFactory = (createInvalidAccessError2, createInvalidStateError2, createNativeScriptProcessorNode2, createNotSupportedError2) => {
  return (nativeContext, baseLatency, { channelCount, channelCountMode, channelInterpretation, feedback, feedforward }) => {
    const bufferSize = computeBufferSize(baseLatency, nativeContext.sampleRate);
    const convertedFeedback = feedback instanceof Float64Array ? feedback : new Float64Array(feedback);
    const convertedFeedforward = feedforward instanceof Float64Array ? feedforward : new Float64Array(feedforward);
    const feedbackLength = convertedFeedback.length;
    const feedforwardLength = convertedFeedforward.length;
    const minLength = Math.min(feedbackLength, feedforwardLength);
    if (feedbackLength === 0 || feedbackLength > 20) {
      throw createNotSupportedError2();
    }
    if (convertedFeedback[0] === 0) {
      throw createInvalidStateError2();
    }
    if (feedforwardLength === 0 || feedforwardLength > 20) {
      throw createNotSupportedError2();
    }
    if (convertedFeedforward[0] === 0) {
      throw createInvalidStateError2();
    }
    if (convertedFeedback[0] !== 1) {
      for (let i = 0; i < feedforwardLength; i += 1) {
        convertedFeedforward[i] /= convertedFeedback[0];
      }
      for (let i = 1; i < feedbackLength; i += 1) {
        convertedFeedback[i] /= convertedFeedback[0];
      }
    }
    const scriptProcessorNode = createNativeScriptProcessorNode2(nativeContext, bufferSize, channelCount, channelCount);
    scriptProcessorNode.channelCount = channelCount;
    scriptProcessorNode.channelCountMode = channelCountMode;
    scriptProcessorNode.channelInterpretation = channelInterpretation;
    const bufferLength = 32;
    const bufferIndexes = [];
    const xBuffers = [];
    const yBuffers = [];
    for (let i = 0; i < channelCount; i += 1) {
      bufferIndexes.push(0);
      const xBuffer = new Float32Array(bufferLength);
      const yBuffer = new Float32Array(bufferLength);
      xBuffer.fill(0);
      yBuffer.fill(0);
      xBuffers.push(xBuffer);
      yBuffers.push(yBuffer);
    }
    scriptProcessorNode.onaudioprocess = (event) => {
      const inputBuffer = event.inputBuffer;
      const outputBuffer = event.outputBuffer;
      const numberOfChannels = inputBuffer.numberOfChannels;
      for (let i = 0; i < numberOfChannels; i += 1) {
        const input = inputBuffer.getChannelData(i);
        const output = outputBuffer.getChannelData(i);
        bufferIndexes[i] = filterBuffer(convertedFeedback, feedbackLength, convertedFeedforward, feedforwardLength, minLength, xBuffers[i], yBuffers[i], bufferIndexes[i], bufferLength, input, output);
      }
    };
    const nyquist = nativeContext.sampleRate / 2;
    const nativeIIRFilterNodeFaker = {
      get bufferSize() {
        return bufferSize;
      },
      get channelCount() {
        return scriptProcessorNode.channelCount;
      },
      set channelCount(value) {
        scriptProcessorNode.channelCount = value;
      },
      get channelCountMode() {
        return scriptProcessorNode.channelCountMode;
      },
      set channelCountMode(value) {
        scriptProcessorNode.channelCountMode = value;
      },
      get channelInterpretation() {
        return scriptProcessorNode.channelInterpretation;
      },
      set channelInterpretation(value) {
        scriptProcessorNode.channelInterpretation = value;
      },
      get context() {
        return scriptProcessorNode.context;
      },
      get inputs() {
        return [scriptProcessorNode];
      },
      get numberOfInputs() {
        return scriptProcessorNode.numberOfInputs;
      },
      get numberOfOutputs() {
        return scriptProcessorNode.numberOfOutputs;
      },
      addEventListener(...args) {
        return scriptProcessorNode.addEventListener(args[0], args[1], args[2]);
      },
      dispatchEvent(...args) {
        return scriptProcessorNode.dispatchEvent(args[0]);
      },
      getFrequencyResponse(frequencyHz, magResponse, phaseResponse) {
        if (frequencyHz.length !== magResponse.length || magResponse.length !== phaseResponse.length) {
          throw createInvalidAccessError2();
        }
        const length = frequencyHz.length;
        for (let i = 0; i < length; i += 1) {
          const omega = -Math.PI * (frequencyHz[i] / nyquist);
          const z = [Math.cos(omega), Math.sin(omega)];
          const numerator = evaluatePolynomial(convertedFeedforward, z);
          const denominator = evaluatePolynomial(convertedFeedback, z);
          const response = divide(numerator, denominator);
          magResponse[i] = Math.sqrt(response[0] * response[0] + response[1] * response[1]);
          phaseResponse[i] = Math.atan2(response[1], response[0]);
        }
      },
      removeEventListener(...args) {
        return scriptProcessorNode.removeEventListener(args[0], args[1], args[2]);
      }
    };
    return interceptConnections(nativeIIRFilterNodeFaker, scriptProcessorNode);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-media-element-audio-source-node.js
var createNativeMediaElementAudioSourceNode = (nativeAudioContext, options) => {
  return nativeAudioContext.createMediaElementSource(options.mediaElement);
};

// node_modules/standardized-audio-context/build/es2019/factories/native-media-stream-audio-destination-node.js
var createNativeMediaStreamAudioDestinationNode = (nativeAudioContext, options) => {
  const nativeMediaStreamAudioDestinationNode = nativeAudioContext.createMediaStreamDestination();
  assignNativeAudioNodeOptions(nativeMediaStreamAudioDestinationNode, options);
  if (nativeMediaStreamAudioDestinationNode.numberOfOutputs === 1) {
    Object.defineProperty(nativeMediaStreamAudioDestinationNode, "numberOfOutputs", { get: () => 0 });
  }
  return nativeMediaStreamAudioDestinationNode;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-media-stream-audio-source-node.js
var createNativeMediaStreamAudioSourceNode = (nativeAudioContext, { mediaStream }) => {
  const audioStreamTracks = mediaStream.getAudioTracks();
  audioStreamTracks.sort((a, b) => a.id < b.id ? -1 : a.id > b.id ? 1 : 0);
  const filteredAudioStreamTracks = audioStreamTracks.slice(0, 1);
  const nativeMediaStreamAudioSourceNode = nativeAudioContext.createMediaStreamSource(new MediaStream(filteredAudioStreamTracks));
  Object.defineProperty(nativeMediaStreamAudioSourceNode, "mediaStream", { value: mediaStream });
  return nativeMediaStreamAudioSourceNode;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-media-stream-track-audio-source-node-factory.js
var createNativeMediaStreamTrackAudioSourceNodeFactory = (createInvalidStateError2, isNativeOfflineAudioContext2) => {
  return (nativeAudioContext, { mediaStreamTrack }) => {
    if (typeof nativeAudioContext.createMediaStreamTrackSource === "function") {
      return nativeAudioContext.createMediaStreamTrackSource(mediaStreamTrack);
    }
    const mediaStream = new MediaStream([mediaStreamTrack]);
    const nativeMediaStreamAudioSourceNode = nativeAudioContext.createMediaStreamSource(mediaStream);
    if (mediaStreamTrack.kind !== "audio") {
      throw createInvalidStateError2();
    }
    if (isNativeOfflineAudioContext2(nativeAudioContext)) {
      throw new TypeError();
    }
    return nativeMediaStreamAudioSourceNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-offline-audio-context-constructor.js
var createNativeOfflineAudioContextConstructor = (window3) => {
  if (window3 === null) {
    return null;
  }
  if (window3.hasOwnProperty("OfflineAudioContext")) {
    return window3.OfflineAudioContext;
  }
  return window3.hasOwnProperty("webkitOfflineAudioContext") ? window3.webkitOfflineAudioContext : null;
};

// node_modules/standardized-audio-context/build/es2019/factories/native-oscillator-node-factory.js
var createNativeOscillatorNodeFactory = (addSilentConnection2, cacheTestResult2, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2, testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport2, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2, wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls2) => {
  return (nativeContext, options) => {
    const nativeOscillatorNode = nativeContext.createOscillator();
    assignNativeAudioNodeOptions(nativeOscillatorNode, options);
    assignNativeAudioNodeAudioParamValue(nativeOscillatorNode, options, "detune");
    assignNativeAudioNodeAudioParamValue(nativeOscillatorNode, options, "frequency");
    if (options.periodicWave !== void 0) {
      nativeOscillatorNode.setPeriodicWave(options.periodicWave);
    } else {
      assignNativeAudioNodeOption(nativeOscillatorNode, options, "type");
    }
    if (!cacheTestResult2(testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2, () => testAudioScheduledSourceNodeStartMethodNegativeParametersSupport2(nativeContext))) {
      wrapAudioScheduledSourceNodeStartMethodNegativeParameters(nativeOscillatorNode);
    }
    if (!cacheTestResult2(testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport2, () => testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport2(nativeContext))) {
      wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls2(nativeOscillatorNode, nativeContext);
    }
    if (!cacheTestResult2(testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2, () => testAudioScheduledSourceNodeStopMethodNegativeParametersSupport2(nativeContext))) {
      wrapAudioScheduledSourceNodeStopMethodNegativeParameters(nativeOscillatorNode);
    }
    addSilentConnection2(nativeContext, nativeOscillatorNode);
    return nativeOscillatorNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-panner-node-factory.js
var createNativePannerNodeFactory = (createNativePannerNodeFaker2) => {
  return (nativeContext, options) => {
    const nativePannerNode = nativeContext.createPanner();
    if (nativePannerNode.orientationX === void 0) {
      return createNativePannerNodeFaker2(nativeContext, options);
    }
    assignNativeAudioNodeOptions(nativePannerNode, options);
    assignNativeAudioNodeAudioParamValue(nativePannerNode, options, "orientationX");
    assignNativeAudioNodeAudioParamValue(nativePannerNode, options, "orientationY");
    assignNativeAudioNodeAudioParamValue(nativePannerNode, options, "orientationZ");
    assignNativeAudioNodeAudioParamValue(nativePannerNode, options, "positionX");
    assignNativeAudioNodeAudioParamValue(nativePannerNode, options, "positionY");
    assignNativeAudioNodeAudioParamValue(nativePannerNode, options, "positionZ");
    assignNativeAudioNodeOption(nativePannerNode, options, "coneInnerAngle");
    assignNativeAudioNodeOption(nativePannerNode, options, "coneOuterAngle");
    assignNativeAudioNodeOption(nativePannerNode, options, "coneOuterGain");
    assignNativeAudioNodeOption(nativePannerNode, options, "distanceModel");
    assignNativeAudioNodeOption(nativePannerNode, options, "maxDistance");
    assignNativeAudioNodeOption(nativePannerNode, options, "panningModel");
    assignNativeAudioNodeOption(nativePannerNode, options, "refDistance");
    assignNativeAudioNodeOption(nativePannerNode, options, "rolloffFactor");
    return nativePannerNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-panner-node-faker-factory.js
var createNativePannerNodeFakerFactory = (connectNativeAudioNodeToNativeAudioNode2, createInvalidStateError2, createNativeChannelMergerNode2, createNativeGainNode2, createNativeScriptProcessorNode2, createNativeWaveShaperNode2, createNotSupportedError2, disconnectNativeAudioNodeFromNativeAudioNode2, getFirstSample2, monitorConnections2) => {
  return (nativeContext, { coneInnerAngle, coneOuterAngle, coneOuterGain, distanceModel, maxDistance, orientationX, orientationY, orientationZ, panningModel, positionX, positionY, positionZ, refDistance, rolloffFactor, ...audioNodeOptions }) => {
    const pannerNode = nativeContext.createPanner();
    if (audioNodeOptions.channelCount > 2) {
      throw createNotSupportedError2();
    }
    if (audioNodeOptions.channelCountMode === "max") {
      throw createNotSupportedError2();
    }
    assignNativeAudioNodeOptions(pannerNode, audioNodeOptions);
    const SINGLE_CHANNEL_OPTIONS = {
      channelCount: 1,
      channelCountMode: "explicit",
      channelInterpretation: "discrete"
    };
    const channelMergerNode = createNativeChannelMergerNode2(nativeContext, {
      ...SINGLE_CHANNEL_OPTIONS,
      channelInterpretation: "speakers",
      numberOfInputs: 6
    });
    const inputGainNode = createNativeGainNode2(nativeContext, { ...audioNodeOptions, gain: 1 });
    const orientationXGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 1 });
    const orientationYGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const orientationZGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const positionXGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const positionYGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const positionZGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const scriptProcessorNode = createNativeScriptProcessorNode2(nativeContext, 256, 6, 1);
    const waveShaperNode = createNativeWaveShaperNode2(nativeContext, {
      ...SINGLE_CHANNEL_OPTIONS,
      curve: new Float32Array([1, 1]),
      oversample: "none"
    });
    let lastOrientation = [orientationX, orientationY, orientationZ];
    let lastPosition = [positionX, positionY, positionZ];
    const buffer = new Float32Array(1);
    scriptProcessorNode.onaudioprocess = ({ inputBuffer }) => {
      const orientation = [
        getFirstSample2(inputBuffer, buffer, 0),
        getFirstSample2(inputBuffer, buffer, 1),
        getFirstSample2(inputBuffer, buffer, 2)
      ];
      if (orientation.some((value, index) => value !== lastOrientation[index])) {
        pannerNode.setOrientation(...orientation);
        lastOrientation = orientation;
      }
      const positon = [
        getFirstSample2(inputBuffer, buffer, 3),
        getFirstSample2(inputBuffer, buffer, 4),
        getFirstSample2(inputBuffer, buffer, 5)
      ];
      if (positon.some((value, index) => value !== lastPosition[index])) {
        pannerNode.setPosition(...positon);
        lastPosition = positon;
      }
    };
    Object.defineProperty(orientationYGainNode.gain, "defaultValue", { get: () => 0 });
    Object.defineProperty(orientationZGainNode.gain, "defaultValue", { get: () => 0 });
    Object.defineProperty(positionXGainNode.gain, "defaultValue", { get: () => 0 });
    Object.defineProperty(positionYGainNode.gain, "defaultValue", { get: () => 0 });
    Object.defineProperty(positionZGainNode.gain, "defaultValue", { get: () => 0 });
    const nativePannerNodeFaker = {
      get bufferSize() {
        return void 0;
      },
      get channelCount() {
        return pannerNode.channelCount;
      },
      set channelCount(value) {
        if (value > 2) {
          throw createNotSupportedError2();
        }
        inputGainNode.channelCount = value;
        pannerNode.channelCount = value;
      },
      get channelCountMode() {
        return pannerNode.channelCountMode;
      },
      set channelCountMode(value) {
        if (value === "max") {
          throw createNotSupportedError2();
        }
        inputGainNode.channelCountMode = value;
        pannerNode.channelCountMode = value;
      },
      get channelInterpretation() {
        return pannerNode.channelInterpretation;
      },
      set channelInterpretation(value) {
        inputGainNode.channelInterpretation = value;
        pannerNode.channelInterpretation = value;
      },
      get coneInnerAngle() {
        return pannerNode.coneInnerAngle;
      },
      set coneInnerAngle(value) {
        pannerNode.coneInnerAngle = value;
      },
      get coneOuterAngle() {
        return pannerNode.coneOuterAngle;
      },
      set coneOuterAngle(value) {
        pannerNode.coneOuterAngle = value;
      },
      get coneOuterGain() {
        return pannerNode.coneOuterGain;
      },
      set coneOuterGain(value) {
        if (value < 0 || value > 1) {
          throw createInvalidStateError2();
        }
        pannerNode.coneOuterGain = value;
      },
      get context() {
        return pannerNode.context;
      },
      get distanceModel() {
        return pannerNode.distanceModel;
      },
      set distanceModel(value) {
        pannerNode.distanceModel = value;
      },
      get inputs() {
        return [inputGainNode];
      },
      get maxDistance() {
        return pannerNode.maxDistance;
      },
      set maxDistance(value) {
        if (value < 0) {
          throw new RangeError();
        }
        pannerNode.maxDistance = value;
      },
      get numberOfInputs() {
        return pannerNode.numberOfInputs;
      },
      get numberOfOutputs() {
        return pannerNode.numberOfOutputs;
      },
      get orientationX() {
        return orientationXGainNode.gain;
      },
      get orientationY() {
        return orientationYGainNode.gain;
      },
      get orientationZ() {
        return orientationZGainNode.gain;
      },
      get panningModel() {
        return pannerNode.panningModel;
      },
      set panningModel(value) {
        pannerNode.panningModel = value;
      },
      get positionX() {
        return positionXGainNode.gain;
      },
      get positionY() {
        return positionYGainNode.gain;
      },
      get positionZ() {
        return positionZGainNode.gain;
      },
      get refDistance() {
        return pannerNode.refDistance;
      },
      set refDistance(value) {
        if (value < 0) {
          throw new RangeError();
        }
        pannerNode.refDistance = value;
      },
      get rolloffFactor() {
        return pannerNode.rolloffFactor;
      },
      set rolloffFactor(value) {
        if (value < 0) {
          throw new RangeError();
        }
        pannerNode.rolloffFactor = value;
      },
      addEventListener(...args) {
        return inputGainNode.addEventListener(args[0], args[1], args[2]);
      },
      dispatchEvent(...args) {
        return inputGainNode.dispatchEvent(args[0]);
      },
      removeEventListener(...args) {
        return inputGainNode.removeEventListener(args[0], args[1], args[2]);
      }
    };
    if (coneInnerAngle !== nativePannerNodeFaker.coneInnerAngle) {
      nativePannerNodeFaker.coneInnerAngle = coneInnerAngle;
    }
    if (coneOuterAngle !== nativePannerNodeFaker.coneOuterAngle) {
      nativePannerNodeFaker.coneOuterAngle = coneOuterAngle;
    }
    if (coneOuterGain !== nativePannerNodeFaker.coneOuterGain) {
      nativePannerNodeFaker.coneOuterGain = coneOuterGain;
    }
    if (distanceModel !== nativePannerNodeFaker.distanceModel) {
      nativePannerNodeFaker.distanceModel = distanceModel;
    }
    if (maxDistance !== nativePannerNodeFaker.maxDistance) {
      nativePannerNodeFaker.maxDistance = maxDistance;
    }
    if (orientationX !== nativePannerNodeFaker.orientationX.value) {
      nativePannerNodeFaker.orientationX.value = orientationX;
    }
    if (orientationY !== nativePannerNodeFaker.orientationY.value) {
      nativePannerNodeFaker.orientationY.value = orientationY;
    }
    if (orientationZ !== nativePannerNodeFaker.orientationZ.value) {
      nativePannerNodeFaker.orientationZ.value = orientationZ;
    }
    if (panningModel !== nativePannerNodeFaker.panningModel) {
      nativePannerNodeFaker.panningModel = panningModel;
    }
    if (positionX !== nativePannerNodeFaker.positionX.value) {
      nativePannerNodeFaker.positionX.value = positionX;
    }
    if (positionY !== nativePannerNodeFaker.positionY.value) {
      nativePannerNodeFaker.positionY.value = positionY;
    }
    if (positionZ !== nativePannerNodeFaker.positionZ.value) {
      nativePannerNodeFaker.positionZ.value = positionZ;
    }
    if (refDistance !== nativePannerNodeFaker.refDistance) {
      nativePannerNodeFaker.refDistance = refDistance;
    }
    if (rolloffFactor !== nativePannerNodeFaker.rolloffFactor) {
      nativePannerNodeFaker.rolloffFactor = rolloffFactor;
    }
    if (lastOrientation[0] !== 1 || lastOrientation[1] !== 0 || lastOrientation[2] !== 0) {
      pannerNode.setOrientation(...lastOrientation);
    }
    if (lastPosition[0] !== 0 || lastPosition[1] !== 0 || lastPosition[2] !== 0) {
      pannerNode.setPosition(...lastPosition);
    }
    const whenConnected = () => {
      inputGainNode.connect(pannerNode);
      connectNativeAudioNodeToNativeAudioNode2(inputGainNode, waveShaperNode, 0, 0);
      waveShaperNode.connect(orientationXGainNode).connect(channelMergerNode, 0, 0);
      waveShaperNode.connect(orientationYGainNode).connect(channelMergerNode, 0, 1);
      waveShaperNode.connect(orientationZGainNode).connect(channelMergerNode, 0, 2);
      waveShaperNode.connect(positionXGainNode).connect(channelMergerNode, 0, 3);
      waveShaperNode.connect(positionYGainNode).connect(channelMergerNode, 0, 4);
      waveShaperNode.connect(positionZGainNode).connect(channelMergerNode, 0, 5);
      channelMergerNode.connect(scriptProcessorNode).connect(nativeContext.destination);
    };
    const whenDisconnected = () => {
      inputGainNode.disconnect(pannerNode);
      disconnectNativeAudioNodeFromNativeAudioNode2(inputGainNode, waveShaperNode, 0, 0);
      waveShaperNode.disconnect(orientationXGainNode);
      orientationXGainNode.disconnect(channelMergerNode);
      waveShaperNode.disconnect(orientationYGainNode);
      orientationYGainNode.disconnect(channelMergerNode);
      waveShaperNode.disconnect(orientationZGainNode);
      orientationZGainNode.disconnect(channelMergerNode);
      waveShaperNode.disconnect(positionXGainNode);
      positionXGainNode.disconnect(channelMergerNode);
      waveShaperNode.disconnect(positionYGainNode);
      positionYGainNode.disconnect(channelMergerNode);
      waveShaperNode.disconnect(positionZGainNode);
      positionZGainNode.disconnect(channelMergerNode);
      channelMergerNode.disconnect(scriptProcessorNode);
      scriptProcessorNode.disconnect(nativeContext.destination);
    };
    return monitorConnections2(interceptConnections(nativePannerNodeFaker, pannerNode), whenConnected, whenDisconnected);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-periodic-wave-factory.js
var createNativePeriodicWaveFactory = (createIndexSizeError2) => {
  return (nativeContext, { disableNormalization, imag, real }) => {
    const convertedImag = imag instanceof Float32Array ? imag : new Float32Array(imag);
    const convertedReal = real instanceof Float32Array ? real : new Float32Array(real);
    const nativePeriodicWave = nativeContext.createPeriodicWave(convertedReal, convertedImag, { disableNormalization });
    if (Array.from(imag).length < 2) {
      throw createIndexSizeError2();
    }
    return nativePeriodicWave;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-script-processor-node.js
var createNativeScriptProcessorNode = (nativeContext, bufferSize, numberOfInputChannels, numberOfOutputChannels) => {
  return nativeContext.createScriptProcessor(bufferSize, numberOfInputChannels, numberOfOutputChannels);
};

// node_modules/standardized-audio-context/build/es2019/factories/native-stereo-panner-node-factory.js
var createNativeStereoPannerNodeFactory = (createNativeStereoPannerNodeFaker, createNotSupportedError2) => {
  return (nativeContext, options) => {
    const channelCountMode = options.channelCountMode;
    if (channelCountMode === "clamped-max") {
      throw createNotSupportedError2();
    }
    if (nativeContext.createStereoPanner === void 0) {
      return createNativeStereoPannerNodeFaker(nativeContext, options);
    }
    const nativeStereoPannerNode = nativeContext.createStereoPanner();
    assignNativeAudioNodeOptions(nativeStereoPannerNode, options);
    assignNativeAudioNodeAudioParamValue(nativeStereoPannerNode, options, "pan");
    Object.defineProperty(nativeStereoPannerNode, "channelCountMode", {
      get: () => channelCountMode,
      set: (value) => {
        if (value !== channelCountMode) {
          throw createNotSupportedError2();
        }
      }
    });
    return nativeStereoPannerNode;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-stereo-panner-node-faker-factory.js
var createNativeStereoPannerNodeFakerFactory = (createNativeChannelMergerNode2, createNativeChannelSplitterNode2, createNativeGainNode2, createNativeWaveShaperNode2, createNotSupportedError2, monitorConnections2) => {
  const CURVE_SIZE = 16385;
  const DC_CURVE = new Float32Array([1, 1]);
  const HALF_PI = Math.PI / 2;
  const SINGLE_CHANNEL_OPTIONS = { channelCount: 1, channelCountMode: "explicit", channelInterpretation: "discrete" };
  const SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS = { ...SINGLE_CHANNEL_OPTIONS, oversample: "none" };
  const buildInternalGraphForMono = (nativeContext, inputGainNode, panGainNode, channelMergerNode) => {
    const leftWaveShaperCurve = new Float32Array(CURVE_SIZE);
    const rightWaveShaperCurve = new Float32Array(CURVE_SIZE);
    for (let i = 0; i < CURVE_SIZE; i += 1) {
      const x = i / (CURVE_SIZE - 1) * HALF_PI;
      leftWaveShaperCurve[i] = Math.cos(x);
      rightWaveShaperCurve[i] = Math.sin(x);
    }
    const leftGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const leftWaveShaperNode = createNativeWaveShaperNode2(nativeContext, { ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS, curve: leftWaveShaperCurve });
    const panWaveShaperNode = createNativeWaveShaperNode2(nativeContext, { ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS, curve: DC_CURVE });
    const rightGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const rightWaveShaperNode = createNativeWaveShaperNode2(nativeContext, { ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS, curve: rightWaveShaperCurve });
    return {
      connectGraph() {
        inputGainNode.connect(leftGainNode);
        inputGainNode.connect(panWaveShaperNode.inputs === void 0 ? panWaveShaperNode : panWaveShaperNode.inputs[0]);
        inputGainNode.connect(rightGainNode);
        panWaveShaperNode.connect(panGainNode);
        panGainNode.connect(leftWaveShaperNode.inputs === void 0 ? leftWaveShaperNode : leftWaveShaperNode.inputs[0]);
        panGainNode.connect(rightWaveShaperNode.inputs === void 0 ? rightWaveShaperNode : rightWaveShaperNode.inputs[0]);
        leftWaveShaperNode.connect(leftGainNode.gain);
        rightWaveShaperNode.connect(rightGainNode.gain);
        leftGainNode.connect(channelMergerNode, 0, 0);
        rightGainNode.connect(channelMergerNode, 0, 1);
      },
      disconnectGraph() {
        inputGainNode.disconnect(leftGainNode);
        inputGainNode.disconnect(panWaveShaperNode.inputs === void 0 ? panWaveShaperNode : panWaveShaperNode.inputs[0]);
        inputGainNode.disconnect(rightGainNode);
        panWaveShaperNode.disconnect(panGainNode);
        panGainNode.disconnect(leftWaveShaperNode.inputs === void 0 ? leftWaveShaperNode : leftWaveShaperNode.inputs[0]);
        panGainNode.disconnect(rightWaveShaperNode.inputs === void 0 ? rightWaveShaperNode : rightWaveShaperNode.inputs[0]);
        leftWaveShaperNode.disconnect(leftGainNode.gain);
        rightWaveShaperNode.disconnect(rightGainNode.gain);
        leftGainNode.disconnect(channelMergerNode, 0, 0);
        rightGainNode.disconnect(channelMergerNode, 0, 1);
      }
    };
  };
  const buildInternalGraphForStereo = (nativeContext, inputGainNode, panGainNode, channelMergerNode) => {
    const leftInputForLeftOutputWaveShaperCurve = new Float32Array(CURVE_SIZE);
    const leftInputForRightOutputWaveShaperCurve = new Float32Array(CURVE_SIZE);
    const rightInputForLeftOutputWaveShaperCurve = new Float32Array(CURVE_SIZE);
    const rightInputForRightOutputWaveShaperCurve = new Float32Array(CURVE_SIZE);
    const centerIndex = Math.floor(CURVE_SIZE / 2);
    for (let i = 0; i < CURVE_SIZE; i += 1) {
      if (i > centerIndex) {
        const x = (i - centerIndex) / (CURVE_SIZE - 1 - centerIndex) * HALF_PI;
        leftInputForLeftOutputWaveShaperCurve[i] = Math.cos(x);
        leftInputForRightOutputWaveShaperCurve[i] = Math.sin(x);
        rightInputForLeftOutputWaveShaperCurve[i] = 0;
        rightInputForRightOutputWaveShaperCurve[i] = 1;
      } else {
        const x = i / (CURVE_SIZE - 1 - centerIndex) * HALF_PI;
        leftInputForLeftOutputWaveShaperCurve[i] = 1;
        leftInputForRightOutputWaveShaperCurve[i] = 0;
        rightInputForLeftOutputWaveShaperCurve[i] = Math.cos(x);
        rightInputForRightOutputWaveShaperCurve[i] = Math.sin(x);
      }
    }
    const channelSplitterNode = createNativeChannelSplitterNode2(nativeContext, {
      channelCount: 2,
      channelCountMode: "explicit",
      channelInterpretation: "discrete",
      numberOfOutputs: 2
    });
    const leftInputForLeftOutputGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const leftInputForLeftOutputWaveShaperNode = createNativeWaveShaperNode2(nativeContext, {
      ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS,
      curve: leftInputForLeftOutputWaveShaperCurve
    });
    const leftInputForRightOutputGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const leftInputForRightOutputWaveShaperNode = createNativeWaveShaperNode2(nativeContext, {
      ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS,
      curve: leftInputForRightOutputWaveShaperCurve
    });
    const panWaveShaperNode = createNativeWaveShaperNode2(nativeContext, { ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS, curve: DC_CURVE });
    const rightInputForLeftOutputGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const rightInputForLeftOutputWaveShaperNode = createNativeWaveShaperNode2(nativeContext, {
      ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS,
      curve: rightInputForLeftOutputWaveShaperCurve
    });
    const rightInputForRightOutputGainNode = createNativeGainNode2(nativeContext, { ...SINGLE_CHANNEL_OPTIONS, gain: 0 });
    const rightInputForRightOutputWaveShaperNode = createNativeWaveShaperNode2(nativeContext, {
      ...SINGLE_CHANNEL_WAVE_SHAPER_OPTIONS,
      curve: rightInputForRightOutputWaveShaperCurve
    });
    return {
      connectGraph() {
        inputGainNode.connect(channelSplitterNode);
        inputGainNode.connect(panWaveShaperNode.inputs === void 0 ? panWaveShaperNode : panWaveShaperNode.inputs[0]);
        channelSplitterNode.connect(leftInputForLeftOutputGainNode, 0);
        channelSplitterNode.connect(leftInputForRightOutputGainNode, 0);
        channelSplitterNode.connect(rightInputForLeftOutputGainNode, 1);
        channelSplitterNode.connect(rightInputForRightOutputGainNode, 1);
        panWaveShaperNode.connect(panGainNode);
        panGainNode.connect(leftInputForLeftOutputWaveShaperNode.inputs === void 0 ? leftInputForLeftOutputWaveShaperNode : leftInputForLeftOutputWaveShaperNode.inputs[0]);
        panGainNode.connect(leftInputForRightOutputWaveShaperNode.inputs === void 0 ? leftInputForRightOutputWaveShaperNode : leftInputForRightOutputWaveShaperNode.inputs[0]);
        panGainNode.connect(rightInputForLeftOutputWaveShaperNode.inputs === void 0 ? rightInputForLeftOutputWaveShaperNode : rightInputForLeftOutputWaveShaperNode.inputs[0]);
        panGainNode.connect(rightInputForRightOutputWaveShaperNode.inputs === void 0 ? rightInputForRightOutputWaveShaperNode : rightInputForRightOutputWaveShaperNode.inputs[0]);
        leftInputForLeftOutputWaveShaperNode.connect(leftInputForLeftOutputGainNode.gain);
        leftInputForRightOutputWaveShaperNode.connect(leftInputForRightOutputGainNode.gain);
        rightInputForLeftOutputWaveShaperNode.connect(rightInputForLeftOutputGainNode.gain);
        rightInputForRightOutputWaveShaperNode.connect(rightInputForRightOutputGainNode.gain);
        leftInputForLeftOutputGainNode.connect(channelMergerNode, 0, 0);
        rightInputForLeftOutputGainNode.connect(channelMergerNode, 0, 0);
        leftInputForRightOutputGainNode.connect(channelMergerNode, 0, 1);
        rightInputForRightOutputGainNode.connect(channelMergerNode, 0, 1);
      },
      disconnectGraph() {
        inputGainNode.disconnect(channelSplitterNode);
        inputGainNode.disconnect(panWaveShaperNode.inputs === void 0 ? panWaveShaperNode : panWaveShaperNode.inputs[0]);
        channelSplitterNode.disconnect(leftInputForLeftOutputGainNode, 0);
        channelSplitterNode.disconnect(leftInputForRightOutputGainNode, 0);
        channelSplitterNode.disconnect(rightInputForLeftOutputGainNode, 1);
        channelSplitterNode.disconnect(rightInputForRightOutputGainNode, 1);
        panWaveShaperNode.disconnect(panGainNode);
        panGainNode.disconnect(leftInputForLeftOutputWaveShaperNode.inputs === void 0 ? leftInputForLeftOutputWaveShaperNode : leftInputForLeftOutputWaveShaperNode.inputs[0]);
        panGainNode.disconnect(leftInputForRightOutputWaveShaperNode.inputs === void 0 ? leftInputForRightOutputWaveShaperNode : leftInputForRightOutputWaveShaperNode.inputs[0]);
        panGainNode.disconnect(rightInputForLeftOutputWaveShaperNode.inputs === void 0 ? rightInputForLeftOutputWaveShaperNode : rightInputForLeftOutputWaveShaperNode.inputs[0]);
        panGainNode.disconnect(rightInputForRightOutputWaveShaperNode.inputs === void 0 ? rightInputForRightOutputWaveShaperNode : rightInputForRightOutputWaveShaperNode.inputs[0]);
        leftInputForLeftOutputWaveShaperNode.disconnect(leftInputForLeftOutputGainNode.gain);
        leftInputForRightOutputWaveShaperNode.disconnect(leftInputForRightOutputGainNode.gain);
        rightInputForLeftOutputWaveShaperNode.disconnect(rightInputForLeftOutputGainNode.gain);
        rightInputForRightOutputWaveShaperNode.disconnect(rightInputForRightOutputGainNode.gain);
        leftInputForLeftOutputGainNode.disconnect(channelMergerNode, 0, 0);
        rightInputForLeftOutputGainNode.disconnect(channelMergerNode, 0, 0);
        leftInputForRightOutputGainNode.disconnect(channelMergerNode, 0, 1);
        rightInputForRightOutputGainNode.disconnect(channelMergerNode, 0, 1);
      }
    };
  };
  const buildInternalGraph = (nativeContext, channelCount, inputGainNode, panGainNode, channelMergerNode) => {
    if (channelCount === 1) {
      return buildInternalGraphForMono(nativeContext, inputGainNode, panGainNode, channelMergerNode);
    }
    if (channelCount === 2) {
      return buildInternalGraphForStereo(nativeContext, inputGainNode, panGainNode, channelMergerNode);
    }
    throw createNotSupportedError2();
  };
  return (nativeContext, { channelCount, channelCountMode, pan, ...audioNodeOptions }) => {
    if (channelCountMode === "max") {
      throw createNotSupportedError2();
    }
    const channelMergerNode = createNativeChannelMergerNode2(nativeContext, {
      ...audioNodeOptions,
      channelCount: 1,
      channelCountMode,
      numberOfInputs: 2
    });
    const inputGainNode = createNativeGainNode2(nativeContext, { ...audioNodeOptions, channelCount, channelCountMode, gain: 1 });
    const panGainNode = createNativeGainNode2(nativeContext, {
      channelCount: 1,
      channelCountMode: "explicit",
      channelInterpretation: "discrete",
      gain: pan
    });
    let { connectGraph, disconnectGraph } = buildInternalGraph(nativeContext, channelCount, inputGainNode, panGainNode, channelMergerNode);
    Object.defineProperty(panGainNode.gain, "defaultValue", { get: () => 0 });
    Object.defineProperty(panGainNode.gain, "maxValue", { get: () => 1 });
    Object.defineProperty(panGainNode.gain, "minValue", { get: () => -1 });
    const nativeStereoPannerNodeFakerFactory2 = {
      get bufferSize() {
        return void 0;
      },
      get channelCount() {
        return inputGainNode.channelCount;
      },
      set channelCount(value) {
        if (inputGainNode.channelCount !== value) {
          if (isConnected) {
            disconnectGraph();
          }
          ({ connectGraph, disconnectGraph } = buildInternalGraph(nativeContext, value, inputGainNode, panGainNode, channelMergerNode));
          if (isConnected) {
            connectGraph();
          }
        }
        inputGainNode.channelCount = value;
      },
      get channelCountMode() {
        return inputGainNode.channelCountMode;
      },
      set channelCountMode(value) {
        if (value === "clamped-max" || value === "max") {
          throw createNotSupportedError2();
        }
        inputGainNode.channelCountMode = value;
      },
      get channelInterpretation() {
        return inputGainNode.channelInterpretation;
      },
      set channelInterpretation(value) {
        inputGainNode.channelInterpretation = value;
      },
      get context() {
        return inputGainNode.context;
      },
      get inputs() {
        return [inputGainNode];
      },
      get numberOfInputs() {
        return inputGainNode.numberOfInputs;
      },
      get numberOfOutputs() {
        return inputGainNode.numberOfOutputs;
      },
      get pan() {
        return panGainNode.gain;
      },
      addEventListener(...args) {
        return inputGainNode.addEventListener(args[0], args[1], args[2]);
      },
      dispatchEvent(...args) {
        return inputGainNode.dispatchEvent(args[0]);
      },
      removeEventListener(...args) {
        return inputGainNode.removeEventListener(args[0], args[1], args[2]);
      }
    };
    let isConnected = false;
    const whenConnected = () => {
      connectGraph();
      isConnected = true;
    };
    const whenDisconnected = () => {
      disconnectGraph();
      isConnected = false;
    };
    return monitorConnections2(interceptConnections(nativeStereoPannerNodeFakerFactory2, channelMergerNode), whenConnected, whenDisconnected);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-wave-shaper-node-factory.js
var createNativeWaveShaperNodeFactory = (createConnectedNativeAudioBufferSourceNode2, createInvalidStateError2, createNativeWaveShaperNodeFaker2, isDCCurve2, monitorConnections2, nativeAudioContextConstructor2, overwriteAccessors2) => {
  return (nativeContext, options) => {
    const nativeWaveShaperNode = nativeContext.createWaveShaper();
    if (nativeAudioContextConstructor2 !== null && nativeAudioContextConstructor2.name === "webkitAudioContext" && nativeContext.createGain().gain.automationRate === void 0) {
      return createNativeWaveShaperNodeFaker2(nativeContext, options);
    }
    assignNativeAudioNodeOptions(nativeWaveShaperNode, options);
    const curve = options.curve === null || options.curve instanceof Float32Array ? options.curve : new Float32Array(options.curve);
    if (curve !== null && curve.length < 2) {
      throw createInvalidStateError2();
    }
    assignNativeAudioNodeOption(nativeWaveShaperNode, { curve }, "curve");
    assignNativeAudioNodeOption(nativeWaveShaperNode, options, "oversample");
    let disconnectNativeAudioBufferSourceNode = null;
    let isConnected = false;
    overwriteAccessors2(nativeWaveShaperNode, "curve", (get) => () => get.call(nativeWaveShaperNode), (set) => (value) => {
      set.call(nativeWaveShaperNode, value);
      if (isConnected) {
        if (isDCCurve2(value) && disconnectNativeAudioBufferSourceNode === null) {
          disconnectNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNode2(nativeContext, nativeWaveShaperNode);
        } else if (!isDCCurve2(value) && disconnectNativeAudioBufferSourceNode !== null) {
          disconnectNativeAudioBufferSourceNode();
          disconnectNativeAudioBufferSourceNode = null;
        }
      }
      return value;
    });
    const whenConnected = () => {
      isConnected = true;
      if (isDCCurve2(nativeWaveShaperNode.curve)) {
        disconnectNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNode2(nativeContext, nativeWaveShaperNode);
      }
    };
    const whenDisconnected = () => {
      isConnected = false;
      if (disconnectNativeAudioBufferSourceNode !== null) {
        disconnectNativeAudioBufferSourceNode();
        disconnectNativeAudioBufferSourceNode = null;
      }
    };
    return monitorConnections2(nativeWaveShaperNode, whenConnected, whenDisconnected);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/native-wave-shaper-node-faker-factory.js
var createNativeWaveShaperNodeFakerFactory = (createConnectedNativeAudioBufferSourceNode2, createInvalidStateError2, createNativeGainNode2, isDCCurve2, monitorConnections2) => {
  return (nativeContext, { curve, oversample, ...audioNodeOptions }) => {
    const negativeWaveShaperNode = nativeContext.createWaveShaper();
    const positiveWaveShaperNode = nativeContext.createWaveShaper();
    assignNativeAudioNodeOptions(negativeWaveShaperNode, audioNodeOptions);
    assignNativeAudioNodeOptions(positiveWaveShaperNode, audioNodeOptions);
    const inputGainNode = createNativeGainNode2(nativeContext, { ...audioNodeOptions, gain: 1 });
    const invertGainNode = createNativeGainNode2(nativeContext, { ...audioNodeOptions, gain: -1 });
    const outputGainNode = createNativeGainNode2(nativeContext, { ...audioNodeOptions, gain: 1 });
    const revertGainNode = createNativeGainNode2(nativeContext, { ...audioNodeOptions, gain: -1 });
    let disconnectNativeAudioBufferSourceNode = null;
    let isConnected = false;
    let unmodifiedCurve = null;
    const nativeWaveShaperNodeFaker = {
      get bufferSize() {
        return void 0;
      },
      get channelCount() {
        return negativeWaveShaperNode.channelCount;
      },
      set channelCount(value) {
        inputGainNode.channelCount = value;
        invertGainNode.channelCount = value;
        negativeWaveShaperNode.channelCount = value;
        outputGainNode.channelCount = value;
        positiveWaveShaperNode.channelCount = value;
        revertGainNode.channelCount = value;
      },
      get channelCountMode() {
        return negativeWaveShaperNode.channelCountMode;
      },
      set channelCountMode(value) {
        inputGainNode.channelCountMode = value;
        invertGainNode.channelCountMode = value;
        negativeWaveShaperNode.channelCountMode = value;
        outputGainNode.channelCountMode = value;
        positiveWaveShaperNode.channelCountMode = value;
        revertGainNode.channelCountMode = value;
      },
      get channelInterpretation() {
        return negativeWaveShaperNode.channelInterpretation;
      },
      set channelInterpretation(value) {
        inputGainNode.channelInterpretation = value;
        invertGainNode.channelInterpretation = value;
        negativeWaveShaperNode.channelInterpretation = value;
        outputGainNode.channelInterpretation = value;
        positiveWaveShaperNode.channelInterpretation = value;
        revertGainNode.channelInterpretation = value;
      },
      get context() {
        return negativeWaveShaperNode.context;
      },
      get curve() {
        return unmodifiedCurve;
      },
      set curve(value) {
        if (value !== null && value.length < 2) {
          throw createInvalidStateError2();
        }
        if (value === null) {
          negativeWaveShaperNode.curve = value;
          positiveWaveShaperNode.curve = value;
        } else {
          const curveLength = value.length;
          const negativeCurve = new Float32Array(curveLength + 2 - curveLength % 2);
          const positiveCurve = new Float32Array(curveLength + 2 - curveLength % 2);
          negativeCurve[0] = value[0];
          positiveCurve[0] = -value[curveLength - 1];
          const length = Math.ceil((curveLength + 1) / 2);
          const centerIndex = (curveLength + 1) / 2 - 1;
          for (let i = 1; i < length; i += 1) {
            const theoreticIndex = i / length * centerIndex;
            const lowerIndex = Math.floor(theoreticIndex);
            const upperIndex = Math.ceil(theoreticIndex);
            negativeCurve[i] = lowerIndex === upperIndex ? value[lowerIndex] : (1 - (theoreticIndex - lowerIndex)) * value[lowerIndex] + (1 - (upperIndex - theoreticIndex)) * value[upperIndex];
            positiveCurve[i] = lowerIndex === upperIndex ? -value[curveLength - 1 - lowerIndex] : -((1 - (theoreticIndex - lowerIndex)) * value[curveLength - 1 - lowerIndex]) - (1 - (upperIndex - theoreticIndex)) * value[curveLength - 1 - upperIndex];
          }
          negativeCurve[length] = curveLength % 2 === 1 ? value[length - 1] : (value[length - 2] + value[length - 1]) / 2;
          negativeWaveShaperNode.curve = negativeCurve;
          positiveWaveShaperNode.curve = positiveCurve;
        }
        unmodifiedCurve = value;
        if (isConnected) {
          if (isDCCurve2(unmodifiedCurve) && disconnectNativeAudioBufferSourceNode === null) {
            disconnectNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNode2(nativeContext, inputGainNode);
          } else if (disconnectNativeAudioBufferSourceNode !== null) {
            disconnectNativeAudioBufferSourceNode();
            disconnectNativeAudioBufferSourceNode = null;
          }
        }
      },
      get inputs() {
        return [inputGainNode];
      },
      get numberOfInputs() {
        return negativeWaveShaperNode.numberOfInputs;
      },
      get numberOfOutputs() {
        return negativeWaveShaperNode.numberOfOutputs;
      },
      get oversample() {
        return negativeWaveShaperNode.oversample;
      },
      set oversample(value) {
        negativeWaveShaperNode.oversample = value;
        positiveWaveShaperNode.oversample = value;
      },
      addEventListener(...args) {
        return inputGainNode.addEventListener(args[0], args[1], args[2]);
      },
      dispatchEvent(...args) {
        return inputGainNode.dispatchEvent(args[0]);
      },
      removeEventListener(...args) {
        return inputGainNode.removeEventListener(args[0], args[1], args[2]);
      }
    };
    if (curve !== null) {
      nativeWaveShaperNodeFaker.curve = curve instanceof Float32Array ? curve : new Float32Array(curve);
    }
    if (oversample !== nativeWaveShaperNodeFaker.oversample) {
      nativeWaveShaperNodeFaker.oversample = oversample;
    }
    const whenConnected = () => {
      inputGainNode.connect(negativeWaveShaperNode).connect(outputGainNode);
      inputGainNode.connect(invertGainNode).connect(positiveWaveShaperNode).connect(revertGainNode).connect(outputGainNode);
      isConnected = true;
      if (isDCCurve2(unmodifiedCurve)) {
        disconnectNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNode2(nativeContext, inputGainNode);
      }
    };
    const whenDisconnected = () => {
      inputGainNode.disconnect(negativeWaveShaperNode);
      negativeWaveShaperNode.disconnect(outputGainNode);
      inputGainNode.disconnect(invertGainNode);
      invertGainNode.disconnect(positiveWaveShaperNode);
      positiveWaveShaperNode.disconnect(revertGainNode);
      revertGainNode.disconnect(outputGainNode);
      isConnected = false;
      if (disconnectNativeAudioBufferSourceNode !== null) {
        disconnectNativeAudioBufferSourceNode();
        disconnectNativeAudioBufferSourceNode = null;
      }
    };
    return monitorConnections2(interceptConnections(nativeWaveShaperNodeFaker, outputGainNode), whenConnected, whenDisconnected);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/not-supported-error.js
var createNotSupportedError = () => new DOMException("", "NotSupportedError");

// node_modules/standardized-audio-context/build/es2019/factories/offline-audio-context-constructor.js
var DEFAULT_OPTIONS16 = {
  numberOfChannels: 1
};
var createOfflineAudioContextConstructor = (baseAudioContextConstructor2, cacheTestResult2, createInvalidStateError2, createNativeOfflineAudioContext2, startRendering2) => {
  return class OfflineAudioContext extends baseAudioContextConstructor2 {
    constructor(a, b, c) {
      let options;
      if (typeof a === "number" && b !== void 0 && c !== void 0) {
        options = { length: b, numberOfChannels: a, sampleRate: c };
      } else if (typeof a === "object") {
        options = a;
      } else {
        throw new Error("The given parameters are not valid.");
      }
      const { length, numberOfChannels, sampleRate } = { ...DEFAULT_OPTIONS16, ...options };
      const nativeOfflineAudioContext = createNativeOfflineAudioContext2(numberOfChannels, length, sampleRate);
      if (!cacheTestResult2(testPromiseSupport, () => testPromiseSupport(nativeOfflineAudioContext))) {
        nativeOfflineAudioContext.addEventListener("statechange", /* @__PURE__ */ (() => {
          let i = 0;
          const delayStateChangeEvent = (event) => {
            if (this._state === "running") {
              if (i > 0) {
                nativeOfflineAudioContext.removeEventListener("statechange", delayStateChangeEvent);
                event.stopImmediatePropagation();
                this._waitForThePromiseToSettle(event);
              } else {
                i += 1;
              }
            }
          };
          return delayStateChangeEvent;
        })());
      }
      super(nativeOfflineAudioContext, numberOfChannels);
      this._length = length;
      this._nativeOfflineAudioContext = nativeOfflineAudioContext;
      this._state = null;
    }
    get length() {
      if (this._nativeOfflineAudioContext.length === void 0) {
        return this._length;
      }
      return this._nativeOfflineAudioContext.length;
    }
    get state() {
      return this._state === null ? this._nativeOfflineAudioContext.state : this._state;
    }
    startRendering() {
      if (this._state === "running") {
        return Promise.reject(createInvalidStateError2());
      }
      this._state = "running";
      return startRendering2(this.destination, this._nativeOfflineAudioContext).finally(() => {
        this._state = null;
        deactivateAudioGraph(this);
      });
    }
    _waitForThePromiseToSettle(event) {
      if (this._state === null) {
        this._nativeOfflineAudioContext.dispatchEvent(event);
      } else {
        setTimeout(() => this._waitForThePromiseToSettle(event));
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/oscillator-node-constructor.js
var DEFAULT_OPTIONS17 = {
  channelCount: 2,
  channelCountMode: "max",
  // This attribute has no effect for nodes with no inputs.
  channelInterpretation: "speakers",
  // This attribute has no effect for nodes with no inputs.
  detune: 0,
  frequency: 440,
  periodicWave: void 0,
  type: "sine"
};
var createOscillatorNodeConstructor = (audioNodeConstructor2, createAudioParam2, createNativeOscillatorNode2, createOscillatorNodeRenderer2, getNativeContext2, isNativeOfflineAudioContext2, wrapEventListener2) => {
  return class OscillatorNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS17, ...options };
      const nativeOscillatorNode = createNativeOscillatorNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const oscillatorNodeRenderer = isOffline ? createOscillatorNodeRenderer2() : null;
      const nyquist = context2.sampleRate / 2;
      super(context2, false, nativeOscillatorNode, oscillatorNodeRenderer);
      this._detune = createAudioParam2(this, isOffline, nativeOscillatorNode.detune, 153600, -153600);
      this._frequency = createAudioParam2(this, isOffline, nativeOscillatorNode.frequency, nyquist, -nyquist);
      this._nativeOscillatorNode = nativeOscillatorNode;
      this._onended = null;
      this._oscillatorNodeRenderer = oscillatorNodeRenderer;
      if (this._oscillatorNodeRenderer !== null && mergedOptions.periodicWave !== void 0) {
        this._oscillatorNodeRenderer.periodicWave = mergedOptions.periodicWave;
      }
    }
    get detune() {
      return this._detune;
    }
    get frequency() {
      return this._frequency;
    }
    get onended() {
      return this._onended;
    }
    set onended(value) {
      const wrappedListener = typeof value === "function" ? wrapEventListener2(this, value) : null;
      this._nativeOscillatorNode.onended = wrappedListener;
      const nativeOnEnded = this._nativeOscillatorNode.onended;
      this._onended = nativeOnEnded !== null && nativeOnEnded === wrappedListener ? value : nativeOnEnded;
    }
    get type() {
      return this._nativeOscillatorNode.type;
    }
    set type(value) {
      this._nativeOscillatorNode.type = value;
      if (this._oscillatorNodeRenderer !== null) {
        this._oscillatorNodeRenderer.periodicWave = null;
      }
    }
    setPeriodicWave(periodicWave) {
      this._nativeOscillatorNode.setPeriodicWave(periodicWave);
      if (this._oscillatorNodeRenderer !== null) {
        this._oscillatorNodeRenderer.periodicWave = periodicWave;
      }
    }
    start(when = 0) {
      this._nativeOscillatorNode.start(when);
      if (this._oscillatorNodeRenderer !== null) {
        this._oscillatorNodeRenderer.start = when;
      }
      if (this.context.state !== "closed") {
        setInternalStateToActive(this);
        const resetInternalStateToPassive = () => {
          this._nativeOscillatorNode.removeEventListener("ended", resetInternalStateToPassive);
          if (isActiveAudioNode(this)) {
            setInternalStateToPassive(this);
          }
        };
        this._nativeOscillatorNode.addEventListener("ended", resetInternalStateToPassive);
      }
    }
    stop(when = 0) {
      this._nativeOscillatorNode.stop(when);
      if (this._oscillatorNodeRenderer !== null) {
        this._oscillatorNodeRenderer.stop = when;
      }
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/oscillator-node-renderer-factory.js
var createOscillatorNodeRendererFactory = (connectAudioParam2, createNativeOscillatorNode2, getNativeAudioNode2, renderAutomation2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeOscillatorNodes = /* @__PURE__ */ new WeakMap();
    let periodicWave = null;
    let start2 = null;
    let stop = null;
    const createOscillatorNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeOscillatorNode = getNativeAudioNode2(proxy);
      const nativeOscillatorNodeIsOwnedByContext = isOwnedByContext(nativeOscillatorNode, nativeOfflineAudioContext);
      if (!nativeOscillatorNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeOscillatorNode.channelCount,
          channelCountMode: nativeOscillatorNode.channelCountMode,
          channelInterpretation: nativeOscillatorNode.channelInterpretation,
          detune: nativeOscillatorNode.detune.value,
          frequency: nativeOscillatorNode.frequency.value,
          periodicWave: periodicWave === null ? void 0 : periodicWave,
          type: nativeOscillatorNode.type
        };
        nativeOscillatorNode = createNativeOscillatorNode2(nativeOfflineAudioContext, options);
        if (start2 !== null) {
          nativeOscillatorNode.start(start2);
        }
        if (stop !== null) {
          nativeOscillatorNode.stop(stop);
        }
      }
      renderedNativeOscillatorNodes.set(nativeOfflineAudioContext, nativeOscillatorNode);
      if (!nativeOscillatorNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.detune, nativeOscillatorNode.detune);
        await renderAutomation2(nativeOfflineAudioContext, proxy.frequency, nativeOscillatorNode.frequency);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.detune, nativeOscillatorNode.detune);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.frequency, nativeOscillatorNode.frequency);
      }
      await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeOscillatorNode);
      return nativeOscillatorNode;
    };
    return {
      set periodicWave(value) {
        periodicWave = value;
      },
      set start(value) {
        start2 = value;
      },
      set stop(value) {
        stop = value;
      },
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeOscillatorNode = renderedNativeOscillatorNodes.get(nativeOfflineAudioContext);
        if (renderedNativeOscillatorNode !== void 0) {
          return Promise.resolve(renderedNativeOscillatorNode);
        }
        return createOscillatorNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/panner-node-constructor.js
var DEFAULT_OPTIONS18 = {
  channelCount: 2,
  channelCountMode: "clamped-max",
  channelInterpretation: "speakers",
  coneInnerAngle: 360,
  coneOuterAngle: 360,
  coneOuterGain: 0,
  distanceModel: "inverse",
  maxDistance: 1e4,
  orientationX: 1,
  orientationY: 0,
  orientationZ: 0,
  panningModel: "equalpower",
  positionX: 0,
  positionY: 0,
  positionZ: 0,
  refDistance: 1,
  rolloffFactor: 1
};
var createPannerNodeConstructor = (audioNodeConstructor2, createAudioParam2, createNativePannerNode2, createPannerNodeRenderer2, getNativeContext2, isNativeOfflineAudioContext2, setAudioNodeTailTime2) => {
  return class PannerNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS18, ...options };
      const nativePannerNode = createNativePannerNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const pannerNodeRenderer = isOffline ? createPannerNodeRenderer2() : null;
      super(context2, false, nativePannerNode, pannerNodeRenderer);
      this._nativePannerNode = nativePannerNode;
      this._orientationX = createAudioParam2(this, isOffline, nativePannerNode.orientationX, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
      this._orientationY = createAudioParam2(this, isOffline, nativePannerNode.orientationY, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
      this._orientationZ = createAudioParam2(this, isOffline, nativePannerNode.orientationZ, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
      this._positionX = createAudioParam2(this, isOffline, nativePannerNode.positionX, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
      this._positionY = createAudioParam2(this, isOffline, nativePannerNode.positionY, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
      this._positionZ = createAudioParam2(this, isOffline, nativePannerNode.positionZ, MOST_POSITIVE_SINGLE_FLOAT, MOST_NEGATIVE_SINGLE_FLOAT);
      setAudioNodeTailTime2(this, 1);
    }
    get coneInnerAngle() {
      return this._nativePannerNode.coneInnerAngle;
    }
    set coneInnerAngle(value) {
      this._nativePannerNode.coneInnerAngle = value;
    }
    get coneOuterAngle() {
      return this._nativePannerNode.coneOuterAngle;
    }
    set coneOuterAngle(value) {
      this._nativePannerNode.coneOuterAngle = value;
    }
    get coneOuterGain() {
      return this._nativePannerNode.coneOuterGain;
    }
    set coneOuterGain(value) {
      this._nativePannerNode.coneOuterGain = value;
    }
    get distanceModel() {
      return this._nativePannerNode.distanceModel;
    }
    set distanceModel(value) {
      this._nativePannerNode.distanceModel = value;
    }
    get maxDistance() {
      return this._nativePannerNode.maxDistance;
    }
    set maxDistance(value) {
      this._nativePannerNode.maxDistance = value;
    }
    get orientationX() {
      return this._orientationX;
    }
    get orientationY() {
      return this._orientationY;
    }
    get orientationZ() {
      return this._orientationZ;
    }
    get panningModel() {
      return this._nativePannerNode.panningModel;
    }
    set panningModel(value) {
      this._nativePannerNode.panningModel = value;
    }
    get positionX() {
      return this._positionX;
    }
    get positionY() {
      return this._positionY;
    }
    get positionZ() {
      return this._positionZ;
    }
    get refDistance() {
      return this._nativePannerNode.refDistance;
    }
    set refDistance(value) {
      this._nativePannerNode.refDistance = value;
    }
    get rolloffFactor() {
      return this._nativePannerNode.rolloffFactor;
    }
    set rolloffFactor(value) {
      this._nativePannerNode.rolloffFactor = value;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/panner-node-renderer-factory.js
var createPannerNodeRendererFactory = (connectAudioParam2, createNativeChannelMergerNode2, createNativeConstantSourceNode2, createNativeGainNode2, createNativePannerNode2, getNativeAudioNode2, nativeOfflineAudioContextConstructor2, renderAutomation2, renderInputsOfAudioNode2, renderNativeOfflineAudioContext2) => {
  return () => {
    const renderedNativeAudioNodes = /* @__PURE__ */ new WeakMap();
    let renderedBufferPromise = null;
    const createAudioNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeGainNode = null;
      let nativePannerNode = getNativeAudioNode2(proxy);
      const commonAudioNodeOptions = {
        channelCount: nativePannerNode.channelCount,
        channelCountMode: nativePannerNode.channelCountMode,
        channelInterpretation: nativePannerNode.channelInterpretation
      };
      const commonNativePannerNodeOptions = {
        ...commonAudioNodeOptions,
        coneInnerAngle: nativePannerNode.coneInnerAngle,
        coneOuterAngle: nativePannerNode.coneOuterAngle,
        coneOuterGain: nativePannerNode.coneOuterGain,
        distanceModel: nativePannerNode.distanceModel,
        maxDistance: nativePannerNode.maxDistance,
        panningModel: nativePannerNode.panningModel,
        refDistance: nativePannerNode.refDistance,
        rolloffFactor: nativePannerNode.rolloffFactor
      };
      const nativePannerNodeIsOwnedByContext = isOwnedByContext(nativePannerNode, nativeOfflineAudioContext);
      if ("bufferSize" in nativePannerNode) {
        nativeGainNode = createNativeGainNode2(nativeOfflineAudioContext, { ...commonAudioNodeOptions, gain: 1 });
      } else if (!nativePannerNodeIsOwnedByContext) {
        const options = {
          ...commonNativePannerNodeOptions,
          orientationX: nativePannerNode.orientationX.value,
          orientationY: nativePannerNode.orientationY.value,
          orientationZ: nativePannerNode.orientationZ.value,
          positionX: nativePannerNode.positionX.value,
          positionY: nativePannerNode.positionY.value,
          positionZ: nativePannerNode.positionZ.value
        };
        nativePannerNode = createNativePannerNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeAudioNodes.set(nativeOfflineAudioContext, nativeGainNode === null ? nativePannerNode : nativeGainNode);
      if (nativeGainNode !== null) {
        if (renderedBufferPromise === null) {
          if (nativeOfflineAudioContextConstructor2 === null) {
            throw new Error("Missing the native OfflineAudioContext constructor.");
          }
          const partialOfflineAudioContext = new nativeOfflineAudioContextConstructor2(
            6,
            // Bug #17: Safari does not yet expose the length.
            proxy.context.length,
            nativeOfflineAudioContext.sampleRate
          );
          const nativeChannelMergerNode = createNativeChannelMergerNode2(partialOfflineAudioContext, {
            channelCount: 1,
            channelCountMode: "explicit",
            channelInterpretation: "speakers",
            numberOfInputs: 6
          });
          nativeChannelMergerNode.connect(partialOfflineAudioContext.destination);
          renderedBufferPromise = (async () => {
            const nativeConstantSourceNodes = await Promise.all([
              proxy.orientationX,
              proxy.orientationY,
              proxy.orientationZ,
              proxy.positionX,
              proxy.positionY,
              proxy.positionZ
            ].map(async (audioParam, index) => {
              const nativeConstantSourceNode = createNativeConstantSourceNode2(partialOfflineAudioContext, {
                channelCount: 1,
                channelCountMode: "explicit",
                channelInterpretation: "discrete",
                offset: index === 0 ? 1 : 0
              });
              await renderAutomation2(partialOfflineAudioContext, audioParam, nativeConstantSourceNode.offset);
              return nativeConstantSourceNode;
            }));
            for (let i = 0; i < 6; i += 1) {
              nativeConstantSourceNodes[i].connect(nativeChannelMergerNode, 0, i);
              nativeConstantSourceNodes[i].start(0);
            }
            return renderNativeOfflineAudioContext2(partialOfflineAudioContext);
          })();
        }
        const renderedBuffer = await renderedBufferPromise;
        const inputGainNode = createNativeGainNode2(nativeOfflineAudioContext, { ...commonAudioNodeOptions, gain: 1 });
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, inputGainNode);
        const channelDatas = [];
        for (let i = 0; i < renderedBuffer.numberOfChannels; i += 1) {
          channelDatas.push(renderedBuffer.getChannelData(i));
        }
        let lastOrientation = [channelDatas[0][0], channelDatas[1][0], channelDatas[2][0]];
        let lastPosition = [channelDatas[3][0], channelDatas[4][0], channelDatas[5][0]];
        let gateGainNode = createNativeGainNode2(nativeOfflineAudioContext, { ...commonAudioNodeOptions, gain: 1 });
        let partialPannerNode = createNativePannerNode2(nativeOfflineAudioContext, {
          ...commonNativePannerNodeOptions,
          orientationX: lastOrientation[0],
          orientationY: lastOrientation[1],
          orientationZ: lastOrientation[2],
          positionX: lastPosition[0],
          positionY: lastPosition[1],
          positionZ: lastPosition[2]
        });
        inputGainNode.connect(gateGainNode).connect(partialPannerNode.inputs[0]);
        partialPannerNode.connect(nativeGainNode);
        for (let i = 128; i < renderedBuffer.length; i += 128) {
          const orientation = [channelDatas[0][i], channelDatas[1][i], channelDatas[2][i]];
          const positon = [channelDatas[3][i], channelDatas[4][i], channelDatas[5][i]];
          if (orientation.some((value, index) => value !== lastOrientation[index]) || positon.some((value, index) => value !== lastPosition[index])) {
            lastOrientation = orientation;
            lastPosition = positon;
            const currentTime = i / nativeOfflineAudioContext.sampleRate;
            gateGainNode.gain.setValueAtTime(0, currentTime);
            gateGainNode = createNativeGainNode2(nativeOfflineAudioContext, { ...commonAudioNodeOptions, gain: 0 });
            partialPannerNode = createNativePannerNode2(nativeOfflineAudioContext, {
              ...commonNativePannerNodeOptions,
              orientationX: lastOrientation[0],
              orientationY: lastOrientation[1],
              orientationZ: lastOrientation[2],
              positionX: lastPosition[0],
              positionY: lastPosition[1],
              positionZ: lastPosition[2]
            });
            gateGainNode.gain.setValueAtTime(1, currentTime);
            inputGainNode.connect(gateGainNode).connect(partialPannerNode.inputs[0]);
            partialPannerNode.connect(nativeGainNode);
          }
        }
        return nativeGainNode;
      }
      if (!nativePannerNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.orientationX, nativePannerNode.orientationX);
        await renderAutomation2(nativeOfflineAudioContext, proxy.orientationY, nativePannerNode.orientationY);
        await renderAutomation2(nativeOfflineAudioContext, proxy.orientationZ, nativePannerNode.orientationZ);
        await renderAutomation2(nativeOfflineAudioContext, proxy.positionX, nativePannerNode.positionX);
        await renderAutomation2(nativeOfflineAudioContext, proxy.positionY, nativePannerNode.positionY);
        await renderAutomation2(nativeOfflineAudioContext, proxy.positionZ, nativePannerNode.positionZ);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.orientationX, nativePannerNode.orientationX);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.orientationY, nativePannerNode.orientationY);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.orientationZ, nativePannerNode.orientationZ);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.positionX, nativePannerNode.positionX);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.positionY, nativePannerNode.positionY);
        await connectAudioParam2(nativeOfflineAudioContext, proxy.positionZ, nativePannerNode.positionZ);
      }
      if (isNativeAudioNodeFaker(nativePannerNode)) {
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativePannerNode.inputs[0]);
      } else {
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativePannerNode);
      }
      return nativePannerNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeGainNodeOrNativePannerNode = renderedNativeAudioNodes.get(nativeOfflineAudioContext);
        if (renderedNativeGainNodeOrNativePannerNode !== void 0) {
          return Promise.resolve(renderedNativeGainNodeOrNativePannerNode);
        }
        return createAudioNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/periodic-wave-constructor.js
var DEFAULT_OPTIONS19 = {
  disableNormalization: false
};
var createPeriodicWaveConstructor = (createNativePeriodicWave2, getNativeContext2, periodicWaveStore, sanitizePeriodicWaveOptions2) => {
  return class PeriodicWave {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = sanitizePeriodicWaveOptions2({ ...DEFAULT_OPTIONS19, ...options });
      const periodicWave = createNativePeriodicWave2(nativeContext, mergedOptions);
      periodicWaveStore.add(periodicWave);
      return periodicWave;
    }
    static [Symbol.hasInstance](instance) {
      return instance !== null && typeof instance === "object" && Object.getPrototypeOf(instance) === PeriodicWave.prototype || periodicWaveStore.has(instance);
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/render-automation.js
var createRenderAutomation = (getAudioParamRenderer, renderInputsOfAudioParam2) => {
  return (nativeOfflineAudioContext, audioParam, nativeAudioParam) => {
    const audioParamRenderer = getAudioParamRenderer(audioParam);
    audioParamRenderer.replay(nativeAudioParam);
    return renderInputsOfAudioParam2(audioParam, nativeOfflineAudioContext, nativeAudioParam);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/render-inputs-of-audio-node.js
var createRenderInputsOfAudioNode = (getAudioNodeConnections2, getAudioNodeRenderer2, isPartOfACycle2) => {
  return async (audioNode, nativeOfflineAudioContext, nativeAudioNode) => {
    const audioNodeConnections = getAudioNodeConnections2(audioNode);
    await Promise.all(audioNodeConnections.activeInputs.map((connections, input) => Array.from(connections).map(async ([source, output]) => {
      const audioNodeRenderer = getAudioNodeRenderer2(source);
      const renderedNativeAudioNode = await audioNodeRenderer.render(source, nativeOfflineAudioContext);
      const destination = audioNode.context.destination;
      if (!isPartOfACycle2(source) && (audioNode !== destination || !isPartOfACycle2(audioNode))) {
        renderedNativeAudioNode.connect(nativeAudioNode, output, input);
      }
    })).reduce((allRenderingPromises, renderingPromises) => [...allRenderingPromises, ...renderingPromises], []));
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/render-inputs-of-audio-param.js
var createRenderInputsOfAudioParam = (getAudioNodeRenderer2, getAudioParamConnections2, isPartOfACycle2) => {
  return async (audioParam, nativeOfflineAudioContext, nativeAudioParam) => {
    const audioParamConnections = getAudioParamConnections2(audioParam);
    await Promise.all(Array.from(audioParamConnections.activeInputs).map(async ([source, output]) => {
      const audioNodeRenderer = getAudioNodeRenderer2(source);
      const renderedNativeAudioNode = await audioNodeRenderer.render(source, nativeOfflineAudioContext);
      if (!isPartOfACycle2(source)) {
        renderedNativeAudioNode.connect(nativeAudioParam, output);
      }
    }));
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/render-native-offline-audio-context.js
var createRenderNativeOfflineAudioContext = (cacheTestResult2, createNativeGainNode2, createNativeScriptProcessorNode2, testOfflineAudioContextCurrentTimeSupport) => {
  return (nativeOfflineAudioContext) => {
    if (cacheTestResult2(testPromiseSupport, () => testPromiseSupport(nativeOfflineAudioContext))) {
      return Promise.resolve(cacheTestResult2(testOfflineAudioContextCurrentTimeSupport, testOfflineAudioContextCurrentTimeSupport)).then((isOfflineAudioContextCurrentTimeSupported) => {
        if (!isOfflineAudioContextCurrentTimeSupported) {
          const scriptProcessorNode = createNativeScriptProcessorNode2(nativeOfflineAudioContext, 512, 0, 1);
          nativeOfflineAudioContext.oncomplete = () => {
            scriptProcessorNode.onaudioprocess = null;
            scriptProcessorNode.disconnect();
          };
          scriptProcessorNode.onaudioprocess = () => nativeOfflineAudioContext.currentTime;
          scriptProcessorNode.connect(nativeOfflineAudioContext.destination);
        }
        return nativeOfflineAudioContext.startRendering();
      });
    }
    return new Promise((resolve) => {
      const gainNode = createNativeGainNode2(nativeOfflineAudioContext, {
        channelCount: 1,
        channelCountMode: "explicit",
        channelInterpretation: "discrete",
        gain: 0
      });
      nativeOfflineAudioContext.oncomplete = (event) => {
        gainNode.disconnect();
        resolve(event.renderedBuffer);
      };
      gainNode.connect(nativeOfflineAudioContext.destination);
      nativeOfflineAudioContext.startRendering();
    });
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/set-active-audio-worklet-node-inputs.js
var createSetActiveAudioWorkletNodeInputs = (activeAudioWorkletNodeInputsStore2) => {
  return (nativeAudioWorkletNode, activeInputs) => {
    activeAudioWorkletNodeInputsStore2.set(nativeAudioWorkletNode, activeInputs);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/set-audio-node-tail-time.js
var createSetAudioNodeTailTime = (audioNodeTailTimeStore2) => {
  return (audioNode, tailTime) => audioNodeTailTimeStore2.set(audioNode, tailTime);
};

// node_modules/standardized-audio-context/build/es2019/factories/start-rendering.js
var createStartRendering = (audioBufferStore2, cacheTestResult2, getAudioNodeRenderer2, getUnrenderedAudioWorkletNodes2, renderNativeOfflineAudioContext2, testAudioBufferCopyChannelMethodsOutOfBoundsSupport2, wrapAudioBufferCopyChannelMethods2, wrapAudioBufferCopyChannelMethodsOutOfBounds2) => {
  return (destination, nativeOfflineAudioContext) => getAudioNodeRenderer2(destination).render(destination, nativeOfflineAudioContext).then(() => Promise.all(Array.from(getUnrenderedAudioWorkletNodes2(nativeOfflineAudioContext)).map((audioWorkletNode) => getAudioNodeRenderer2(audioWorkletNode).render(audioWorkletNode, nativeOfflineAudioContext)))).then(() => renderNativeOfflineAudioContext2(nativeOfflineAudioContext)).then((audioBuffer) => {
    if (typeof audioBuffer.copyFromChannel !== "function") {
      wrapAudioBufferCopyChannelMethods2(audioBuffer);
      wrapAudioBufferGetChannelDataMethod(audioBuffer);
    } else if (!cacheTestResult2(testAudioBufferCopyChannelMethodsOutOfBoundsSupport2, () => testAudioBufferCopyChannelMethodsOutOfBoundsSupport2(audioBuffer))) {
      wrapAudioBufferCopyChannelMethodsOutOfBounds2(audioBuffer);
    }
    audioBufferStore2.add(audioBuffer);
    return audioBuffer;
  });
};

// node_modules/standardized-audio-context/build/es2019/factories/stereo-panner-node-constructor.js
var DEFAULT_OPTIONS20 = {
  channelCount: 2,
  /*
   * Bug #105: The channelCountMode should be 'clamped-max' according to the spec but is set to 'explicit' to achieve consistent
   * behavior.
   */
  channelCountMode: "explicit",
  channelInterpretation: "speakers",
  pan: 0
};
var createStereoPannerNodeConstructor = (audioNodeConstructor2, createAudioParam2, createNativeStereoPannerNode2, createStereoPannerNodeRenderer2, getNativeContext2, isNativeOfflineAudioContext2) => {
  return class StereoPannerNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS20, ...options };
      const nativeStereoPannerNode = createNativeStereoPannerNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const stereoPannerNodeRenderer = isOffline ? createStereoPannerNodeRenderer2() : null;
      super(context2, false, nativeStereoPannerNode, stereoPannerNodeRenderer);
      this._pan = createAudioParam2(this, isOffline, nativeStereoPannerNode.pan);
    }
    get pan() {
      return this._pan;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/stereo-panner-node-renderer-factory.js
var createStereoPannerNodeRendererFactory = (connectAudioParam2, createNativeStereoPannerNode2, getNativeAudioNode2, renderAutomation2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeStereoPannerNodes = /* @__PURE__ */ new WeakMap();
    const createStereoPannerNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeStereoPannerNode = getNativeAudioNode2(proxy);
      const nativeStereoPannerNodeIsOwnedByContext = isOwnedByContext(nativeStereoPannerNode, nativeOfflineAudioContext);
      if (!nativeStereoPannerNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeStereoPannerNode.channelCount,
          channelCountMode: nativeStereoPannerNode.channelCountMode,
          channelInterpretation: nativeStereoPannerNode.channelInterpretation,
          pan: nativeStereoPannerNode.pan.value
        };
        nativeStereoPannerNode = createNativeStereoPannerNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeStereoPannerNodes.set(nativeOfflineAudioContext, nativeStereoPannerNode);
      if (!nativeStereoPannerNodeIsOwnedByContext) {
        await renderAutomation2(nativeOfflineAudioContext, proxy.pan, nativeStereoPannerNode.pan);
      } else {
        await connectAudioParam2(nativeOfflineAudioContext, proxy.pan, nativeStereoPannerNode.pan);
      }
      if (isNativeAudioNodeFaker(nativeStereoPannerNode)) {
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeStereoPannerNode.inputs[0]);
      } else {
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeStereoPannerNode);
      }
      return nativeStereoPannerNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeStereoPannerNode = renderedNativeStereoPannerNodes.get(nativeOfflineAudioContext);
        if (renderedNativeStereoPannerNode !== void 0) {
          return Promise.resolve(renderedNativeStereoPannerNode);
        }
        return createStereoPannerNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/test-audio-buffer-constructor-support.js
var createTestAudioBufferConstructorSupport = (nativeAudioBufferConstructor2) => {
  return () => {
    if (nativeAudioBufferConstructor2 === null) {
      return false;
    }
    try {
      new nativeAudioBufferConstructor2({ length: 1, sampleRate: 44100 });
    } catch {
      return false;
    }
    return true;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/test-audio-worklet-processor-post-message-support.js
var createTestAudioWorkletProcessorPostMessageSupport = (nativeAudioWorkletNodeConstructor2, nativeOfflineAudioContextConstructor2) => {
  return async () => {
    if (nativeAudioWorkletNodeConstructor2 === null) {
      return true;
    }
    if (nativeOfflineAudioContextConstructor2 === null) {
      return false;
    }
    const blob = new Blob(['class A extends AudioWorkletProcessor{process(i){this.port.postMessage(i,[i[0][0].buffer])}}registerProcessor("a",A)'], {
      type: "application/javascript; charset=utf-8"
    });
    const offlineAudioContext = new nativeOfflineAudioContextConstructor2(1, 128, 44100);
    const url = URL.createObjectURL(blob);
    let isEmittingMessageEvents = false;
    let isEmittingProcessorErrorEvents = false;
    try {
      await offlineAudioContext.audioWorklet.addModule(url);
      const audioWorkletNode = new nativeAudioWorkletNodeConstructor2(offlineAudioContext, "a", { numberOfOutputs: 0 });
      const oscillator = offlineAudioContext.createOscillator();
      audioWorkletNode.port.onmessage = () => isEmittingMessageEvents = true;
      audioWorkletNode.onprocessorerror = () => isEmittingProcessorErrorEvents = true;
      oscillator.connect(audioWorkletNode);
      oscillator.start(0);
      await offlineAudioContext.startRendering();
      await new Promise((resolve) => setTimeout(resolve));
    } catch {
    } finally {
      URL.revokeObjectURL(url);
    }
    return isEmittingMessageEvents && !isEmittingProcessorErrorEvents;
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/test-offline-audio-context-current-time-support.js
var createTestOfflineAudioContextCurrentTimeSupport = (createNativeGainNode2, nativeOfflineAudioContextConstructor2) => {
  return () => {
    if (nativeOfflineAudioContextConstructor2 === null) {
      return Promise.resolve(false);
    }
    const nativeOfflineAudioContext = new nativeOfflineAudioContextConstructor2(1, 1, 44100);
    const gainNode = createNativeGainNode2(nativeOfflineAudioContext, {
      channelCount: 1,
      channelCountMode: "explicit",
      channelInterpretation: "discrete",
      gain: 0
    });
    return new Promise((resolve) => {
      nativeOfflineAudioContext.oncomplete = () => {
        gainNode.disconnect();
        resolve(nativeOfflineAudioContext.currentTime !== 0);
      };
      nativeOfflineAudioContext.startRendering();
    });
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/unknown-error.js
var createUnknownError = () => new DOMException("", "UnknownError");

// node_modules/standardized-audio-context/build/es2019/factories/wave-shaper-node-constructor.js
var DEFAULT_OPTIONS21 = {
  channelCount: 2,
  channelCountMode: "max",
  channelInterpretation: "speakers",
  curve: null,
  oversample: "none"
};
var createWaveShaperNodeConstructor = (audioNodeConstructor2, createInvalidStateError2, createNativeWaveShaperNode2, createWaveShaperNodeRenderer2, getNativeContext2, isNativeOfflineAudioContext2, setAudioNodeTailTime2) => {
  return class WaveShaperNode extends audioNodeConstructor2 {
    constructor(context2, options) {
      const nativeContext = getNativeContext2(context2);
      const mergedOptions = { ...DEFAULT_OPTIONS21, ...options };
      const nativeWaveShaperNode = createNativeWaveShaperNode2(nativeContext, mergedOptions);
      const isOffline = isNativeOfflineAudioContext2(nativeContext);
      const waveShaperNodeRenderer = isOffline ? createWaveShaperNodeRenderer2() : null;
      super(context2, true, nativeWaveShaperNode, waveShaperNodeRenderer);
      this._isCurveNullified = false;
      this._nativeWaveShaperNode = nativeWaveShaperNode;
      setAudioNodeTailTime2(this, 1);
    }
    get curve() {
      if (this._isCurveNullified) {
        return null;
      }
      return this._nativeWaveShaperNode.curve;
    }
    set curve(value) {
      if (value === null) {
        this._isCurveNullified = true;
        this._nativeWaveShaperNode.curve = new Float32Array([0, 0]);
      } else {
        if (value.length < 2) {
          throw createInvalidStateError2();
        }
        this._isCurveNullified = false;
        this._nativeWaveShaperNode.curve = value;
      }
    }
    get oversample() {
      return this._nativeWaveShaperNode.oversample;
    }
    set oversample(value) {
      this._nativeWaveShaperNode.oversample = value;
    }
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/wave-shaper-node-renderer-factory.js
var createWaveShaperNodeRendererFactory = (createNativeWaveShaperNode2, getNativeAudioNode2, renderInputsOfAudioNode2) => {
  return () => {
    const renderedNativeWaveShaperNodes = /* @__PURE__ */ new WeakMap();
    const createWaveShaperNode = async (proxy, nativeOfflineAudioContext) => {
      let nativeWaveShaperNode = getNativeAudioNode2(proxy);
      const nativeWaveShaperNodeIsOwnedByContext = isOwnedByContext(nativeWaveShaperNode, nativeOfflineAudioContext);
      if (!nativeWaveShaperNodeIsOwnedByContext) {
        const options = {
          channelCount: nativeWaveShaperNode.channelCount,
          channelCountMode: nativeWaveShaperNode.channelCountMode,
          channelInterpretation: nativeWaveShaperNode.channelInterpretation,
          curve: nativeWaveShaperNode.curve,
          oversample: nativeWaveShaperNode.oversample
        };
        nativeWaveShaperNode = createNativeWaveShaperNode2(nativeOfflineAudioContext, options);
      }
      renderedNativeWaveShaperNodes.set(nativeOfflineAudioContext, nativeWaveShaperNode);
      if (isNativeAudioNodeFaker(nativeWaveShaperNode)) {
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeWaveShaperNode.inputs[0]);
      } else {
        await renderInputsOfAudioNode2(proxy, nativeOfflineAudioContext, nativeWaveShaperNode);
      }
      return nativeWaveShaperNode;
    };
    return {
      render(proxy, nativeOfflineAudioContext) {
        const renderedNativeWaveShaperNode = renderedNativeWaveShaperNodes.get(nativeOfflineAudioContext);
        if (renderedNativeWaveShaperNode !== void 0) {
          return Promise.resolve(renderedNativeWaveShaperNode);
        }
        return createWaveShaperNode(proxy, nativeOfflineAudioContext);
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/window.js
var createWindow = () => typeof window === "undefined" ? null : window;

// node_modules/standardized-audio-context/build/es2019/factories/wrap-audio-buffer-copy-channel-methods.js
var createWrapAudioBufferCopyChannelMethods = (convertNumberToUnsignedLong2, createIndexSizeError2) => {
  return (audioBuffer) => {
    audioBuffer.copyFromChannel = (destination, channelNumberAsNumber, bufferOffsetAsNumber = 0) => {
      const bufferOffset = convertNumberToUnsignedLong2(bufferOffsetAsNumber);
      const channelNumber = convertNumberToUnsignedLong2(channelNumberAsNumber);
      if (channelNumber >= audioBuffer.numberOfChannels) {
        throw createIndexSizeError2();
      }
      const audioBufferLength = audioBuffer.length;
      const channelData = audioBuffer.getChannelData(channelNumber);
      const destinationLength = destination.length;
      for (let i = bufferOffset < 0 ? -bufferOffset : 0; i + bufferOffset < audioBufferLength && i < destinationLength; i += 1) {
        destination[i] = channelData[i + bufferOffset];
      }
    };
    audioBuffer.copyToChannel = (source, channelNumberAsNumber, bufferOffsetAsNumber = 0) => {
      const bufferOffset = convertNumberToUnsignedLong2(bufferOffsetAsNumber);
      const channelNumber = convertNumberToUnsignedLong2(channelNumberAsNumber);
      if (channelNumber >= audioBuffer.numberOfChannels) {
        throw createIndexSizeError2();
      }
      const audioBufferLength = audioBuffer.length;
      const channelData = audioBuffer.getChannelData(channelNumber);
      const sourceLength = source.length;
      for (let i = bufferOffset < 0 ? -bufferOffset : 0; i + bufferOffset < audioBufferLength && i < sourceLength; i += 1) {
        channelData[i + bufferOffset] = source[i];
      }
    };
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/wrap-audio-buffer-copy-channel-methods-out-of-bounds.js
var createWrapAudioBufferCopyChannelMethodsOutOfBounds = (convertNumberToUnsignedLong2) => {
  return (audioBuffer) => {
    audioBuffer.copyFromChannel = /* @__PURE__ */ ((copyFromChannel2) => {
      return (destination, channelNumberAsNumber, bufferOffsetAsNumber = 0) => {
        const bufferOffset = convertNumberToUnsignedLong2(bufferOffsetAsNumber);
        const channelNumber = convertNumberToUnsignedLong2(channelNumberAsNumber);
        if (bufferOffset < audioBuffer.length) {
          return copyFromChannel2.call(audioBuffer, destination, channelNumber, bufferOffset);
        }
      };
    })(audioBuffer.copyFromChannel);
    audioBuffer.copyToChannel = /* @__PURE__ */ ((copyToChannel2) => {
      return (source, channelNumberAsNumber, bufferOffsetAsNumber = 0) => {
        const bufferOffset = convertNumberToUnsignedLong2(bufferOffsetAsNumber);
        const channelNumber = convertNumberToUnsignedLong2(channelNumberAsNumber);
        if (bufferOffset < audioBuffer.length) {
          return copyToChannel2.call(audioBuffer, source, channelNumber, bufferOffset);
        }
      };
    })(audioBuffer.copyToChannel);
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/wrap-audio-buffer-source-node-stop-method-nullified-buffer.js
var createWrapAudioBufferSourceNodeStopMethodNullifiedBuffer = (overwriteAccessors2) => {
  return (nativeAudioBufferSourceNode, nativeContext) => {
    const nullifiedBuffer = nativeContext.createBuffer(1, 1, 44100);
    if (nativeAudioBufferSourceNode.buffer === null) {
      nativeAudioBufferSourceNode.buffer = nullifiedBuffer;
    }
    overwriteAccessors2(nativeAudioBufferSourceNode, "buffer", (get) => () => {
      const value = get.call(nativeAudioBufferSourceNode);
      return value === nullifiedBuffer ? null : value;
    }, (set) => (value) => {
      return set.call(nativeAudioBufferSourceNode, value === null ? nullifiedBuffer : value);
    });
  };
};

// node_modules/standardized-audio-context/build/es2019/factories/wrap-channel-merger-node.js
var createWrapChannelMergerNode = (createInvalidStateError2, monitorConnections2) => {
  return (nativeContext, channelMergerNode) => {
    channelMergerNode.channelCount = 1;
    channelMergerNode.channelCountMode = "explicit";
    Object.defineProperty(channelMergerNode, "channelCount", {
      get: () => 1,
      set: () => {
        throw createInvalidStateError2();
      }
    });
    Object.defineProperty(channelMergerNode, "channelCountMode", {
      get: () => "explicit",
      set: () => {
        throw createInvalidStateError2();
      }
    });
    const audioBufferSourceNode = nativeContext.createBufferSource();
    const whenConnected = () => {
      const length = channelMergerNode.numberOfInputs;
      for (let i = 0; i < length; i += 1) {
        audioBufferSourceNode.connect(channelMergerNode, 0, i);
      }
    };
    const whenDisconnected = () => audioBufferSourceNode.disconnect(channelMergerNode);
    monitorConnections2(channelMergerNode, whenConnected, whenDisconnected);
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/get-first-sample.js
var getFirstSample = (audioBuffer, buffer, channelNumber) => {
  if (audioBuffer.copyFromChannel === void 0) {
    return audioBuffer.getChannelData(channelNumber)[0];
  }
  audioBuffer.copyFromChannel(buffer, channelNumber);
  return buffer[0];
};

// node_modules/standardized-audio-context/build/es2019/helpers/is-dc-curve.js
var isDCCurve = (curve) => {
  if (curve === null) {
    return false;
  }
  const length = curve.length;
  if (length % 2 !== 0) {
    return curve[Math.floor(length / 2)] !== 0;
  }
  return curve[length / 2 - 1] + curve[length / 2] !== 0;
};

// node_modules/standardized-audio-context/build/es2019/helpers/overwrite-accessors.js
var overwriteAccessors = (object, property, createGetter, createSetter) => {
  let prototype = object;
  while (!prototype.hasOwnProperty(property)) {
    prototype = Object.getPrototypeOf(prototype);
  }
  const { get, set } = Object.getOwnPropertyDescriptor(prototype, property);
  Object.defineProperty(object, property, { get: createGetter(get), set: createSetter(set) });
};

// node_modules/standardized-audio-context/build/es2019/helpers/sanitize-audio-worklet-node-options.js
var sanitizeAudioWorkletNodeOptions = (options) => {
  return {
    ...options,
    outputChannelCount: options.outputChannelCount !== void 0 ? options.outputChannelCount : options.numberOfInputs === 1 && options.numberOfOutputs === 1 ? (
      /*
       * Bug #61: This should be the computedNumberOfChannels, but unfortunately that is almost impossible to fake. That's why
       * the channelCountMode is required to be 'explicit' as long as there is not a native implementation in every browser. That
       * makes sure the computedNumberOfChannels is equivilant to the channelCount which makes it much easier to compute.
       */
      [options.channelCount]
    ) : Array.from({ length: options.numberOfOutputs }, () => 1)
  };
};

// node_modules/standardized-audio-context/build/es2019/helpers/sanitize-channel-splitter-options.js
var sanitizeChannelSplitterOptions = (options) => {
  return { ...options, channelCount: options.numberOfOutputs };
};

// node_modules/standardized-audio-context/build/es2019/helpers/sanitize-periodic-wave-options.js
var sanitizePeriodicWaveOptions = (options) => {
  const { imag, real } = options;
  if (imag === void 0) {
    if (real === void 0) {
      return { ...options, imag: [0, 0], real: [0, 0] };
    }
    return { ...options, imag: Array.from(real, () => 0), real };
  }
  if (real === void 0) {
    return { ...options, imag, real: Array.from(imag, () => 0) };
  }
  return { ...options, imag, real };
};

// node_modules/standardized-audio-context/build/es2019/helpers/set-value-at-time-until-possible.js
var setValueAtTimeUntilPossible = (audioParam, value, startTime) => {
  try {
    audioParam.setValueAtTime(value, startTime);
  } catch (err) {
    if (err.code !== 9) {
      throw err;
    }
    setValueAtTimeUntilPossible(audioParam, value, startTime + 1e-7);
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-buffer-source-node-start-method-consecutive-calls-support.js
var testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport = (nativeContext) => {
  const nativeAudioBufferSourceNode = nativeContext.createBufferSource();
  nativeAudioBufferSourceNode.start();
  try {
    nativeAudioBufferSourceNode.start();
  } catch {
    return true;
  }
  return false;
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-buffer-source-node-start-method-offset-clamping-support.js
var testAudioBufferSourceNodeStartMethodOffsetClampingSupport = (nativeContext) => {
  const nativeAudioBufferSourceNode = nativeContext.createBufferSource();
  const nativeAudioBuffer = nativeContext.createBuffer(1, 1, 44100);
  nativeAudioBufferSourceNode.buffer = nativeAudioBuffer;
  try {
    nativeAudioBufferSourceNode.start(0, 1);
  } catch {
    return false;
  }
  return true;
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-buffer-source-node-stop-method-nullified-buffer-support.js
var testAudioBufferSourceNodeStopMethodNullifiedBufferSupport = (nativeContext) => {
  const nativeAudioBufferSourceNode = nativeContext.createBufferSource();
  nativeAudioBufferSourceNode.start();
  try {
    nativeAudioBufferSourceNode.stop();
  } catch {
    return false;
  }
  return true;
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-scheduled-source-node-start-method-negative-parameters-support.js
var testAudioScheduledSourceNodeStartMethodNegativeParametersSupport = (nativeContext) => {
  const nativeAudioBufferSourceNode = nativeContext.createOscillator();
  try {
    nativeAudioBufferSourceNode.start(-1);
  } catch (err) {
    return err instanceof RangeError;
  }
  return false;
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-scheduled-source-node-stop-method-consecutive-calls-support.js
var testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport = (nativeContext) => {
  const nativeAudioBuffer = nativeContext.createBuffer(1, 1, 44100);
  const nativeAudioBufferSourceNode = nativeContext.createBufferSource();
  nativeAudioBufferSourceNode.buffer = nativeAudioBuffer;
  nativeAudioBufferSourceNode.start();
  nativeAudioBufferSourceNode.stop();
  try {
    nativeAudioBufferSourceNode.stop();
    return true;
  } catch {
    return false;
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-scheduled-source-node-stop-method-negative-parameters-support.js
var testAudioScheduledSourceNodeStopMethodNegativeParametersSupport = (nativeContext) => {
  const nativeAudioBufferSourceNode = nativeContext.createOscillator();
  try {
    nativeAudioBufferSourceNode.stop(-1);
  } catch (err) {
    return err instanceof RangeError;
  }
  return false;
};

// node_modules/standardized-audio-context/build/es2019/helpers/test-audio-worklet-node-options-clonability.js
var testAudioWorkletNodeOptionsClonability = (audioWorkletNodeOptions) => {
  const { port1, port2 } = new MessageChannel();
  try {
    port1.postMessage(audioWorkletNodeOptions);
  } finally {
    port1.close();
    port2.close();
  }
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-buffer-source-node-start-method-offset-clamping.js
var wrapAudioBufferSourceNodeStartMethodOffsetClamping = (nativeAudioBufferSourceNode) => {
  nativeAudioBufferSourceNode.start = /* @__PURE__ */ ((start2) => {
    return (when = 0, offset = 0, duration) => {
      const buffer = nativeAudioBufferSourceNode.buffer;
      const clampedOffset = buffer === null ? offset : Math.min(buffer.duration, offset);
      if (buffer !== null && clampedOffset > buffer.duration - 0.5 / nativeAudioBufferSourceNode.context.sampleRate) {
        start2.call(nativeAudioBufferSourceNode, when, 0, 0);
      } else {
        start2.call(nativeAudioBufferSourceNode, when, clampedOffset, duration);
      }
    };
  })(nativeAudioBufferSourceNode.start);
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-audio-scheduled-source-node-stop-method-consecutive-calls.js
var wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls = (nativeAudioScheduledSourceNode, nativeContext) => {
  const nativeGainNode = nativeContext.createGain();
  nativeAudioScheduledSourceNode.connect(nativeGainNode);
  const disconnectGainNode = /* @__PURE__ */ ((disconnect2) => {
    return () => {
      disconnect2.call(nativeAudioScheduledSourceNode, nativeGainNode);
      nativeAudioScheduledSourceNode.removeEventListener("ended", disconnectGainNode);
    };
  })(nativeAudioScheduledSourceNode.disconnect);
  nativeAudioScheduledSourceNode.addEventListener("ended", disconnectGainNode);
  interceptConnections(nativeAudioScheduledSourceNode, nativeGainNode);
  nativeAudioScheduledSourceNode.stop = /* @__PURE__ */ ((stop) => {
    let isStopped = false;
    return (when = 0) => {
      if (isStopped) {
        try {
          stop.call(nativeAudioScheduledSourceNode, when);
        } catch {
          nativeGainNode.gain.setValueAtTime(0, when);
        }
      } else {
        stop.call(nativeAudioScheduledSourceNode, when);
        isStopped = true;
      }
    };
  })(nativeAudioScheduledSourceNode.stop);
};

// node_modules/standardized-audio-context/build/es2019/helpers/wrap-event-listener.js
var wrapEventListener = (target, eventListener) => {
  return (event) => {
    const descriptor = { value: target };
    Object.defineProperties(event, {
      currentTarget: descriptor,
      target: descriptor
    });
    if (typeof eventListener === "function") {
      return eventListener.call(target, event);
    }
    return eventListener.handleEvent.call(target, event);
  };
};

// node_modules/standardized-audio-context/build/es2019/module.js
var addActiveInputConnectionToAudioNode = createAddActiveInputConnectionToAudioNode(insertElementInSet);
var addPassiveInputConnectionToAudioNode = createAddPassiveInputConnectionToAudioNode(insertElementInSet);
var deleteActiveInputConnectionToAudioNode = createDeleteActiveInputConnectionToAudioNode(pickElementFromSet);
var audioNodeTailTimeStore = /* @__PURE__ */ new WeakMap();
var getAudioNodeTailTime = createGetAudioNodeTailTime(audioNodeTailTimeStore);
var cacheTestResult = createCacheTestResult(/* @__PURE__ */ new Map(), /* @__PURE__ */ new WeakMap());
var window2 = createWindow();
var createNativeAnalyserNode = createNativeAnalyserNodeFactory(cacheTestResult, createIndexSizeError);
var getAudioNodeRenderer = createGetAudioNodeRenderer(getAudioNodeConnections);
var renderInputsOfAudioNode = createRenderInputsOfAudioNode(getAudioNodeConnections, getAudioNodeRenderer, isPartOfACycle);
var createAnalyserNodeRenderer = createAnalyserNodeRendererFactory(createNativeAnalyserNode, getNativeAudioNode, renderInputsOfAudioNode);
var getNativeContext = createGetNativeContext(CONTEXT_STORE);
var nativeOfflineAudioContextConstructor = createNativeOfflineAudioContextConstructor(window2);
var isNativeOfflineAudioContext = createIsNativeOfflineAudioContext(nativeOfflineAudioContextConstructor);
var audioParamAudioNodeStore = /* @__PURE__ */ new WeakMap();
var eventTargetConstructor = createEventTargetConstructor(wrapEventListener);
var nativeAudioContextConstructor = createNativeAudioContextConstructor(window2);
var isNativeAudioContext = createIsNativeAudioContext(nativeAudioContextConstructor);
var isNativeAudioNode2 = createIsNativeAudioNode(window2);
var isNativeAudioParam = createIsNativeAudioParam(window2);
var nativeAudioWorkletNodeConstructor = createNativeAudioWorkletNodeConstructor(window2);
var audioNodeConstructor = createAudioNodeConstructor(createAddAudioNodeConnections(AUDIO_NODE_CONNECTIONS_STORE), createAddConnectionToAudioNode(addActiveInputConnectionToAudioNode, addPassiveInputConnectionToAudioNode, connectNativeAudioNodeToNativeAudioNode, deleteActiveInputConnectionToAudioNode, disconnectNativeAudioNodeFromNativeAudioNode, getAudioNodeConnections, getAudioNodeTailTime, getEventListenersOfAudioNode, getNativeAudioNode, insertElementInSet, isActiveAudioNode, isPartOfACycle, isPassiveAudioNode), cacheTestResult, createIncrementCycleCounterFactory(CYCLE_COUNTERS, disconnectNativeAudioNodeFromNativeAudioNode, getAudioNodeConnections, getNativeAudioNode, getNativeAudioParam, isActiveAudioNode), createIndexSizeError, createInvalidAccessError, createNotSupportedError, createDecrementCycleCounter(connectNativeAudioNodeToNativeAudioNode, CYCLE_COUNTERS, getAudioNodeConnections, getNativeAudioNode, getNativeAudioParam, getNativeContext, isActiveAudioNode, isNativeOfflineAudioContext), createDetectCycles(audioParamAudioNodeStore, getAudioNodeConnections, getValueForKey), eventTargetConstructor, getNativeContext, isNativeAudioContext, isNativeAudioNode2, isNativeAudioParam, isNativeOfflineAudioContext, nativeAudioWorkletNodeConstructor);
var analyserNodeConstructor = createAnalyserNodeConstructor(audioNodeConstructor, createAnalyserNodeRenderer, createIndexSizeError, createNativeAnalyserNode, getNativeContext, isNativeOfflineAudioContext);
var audioBufferStore = /* @__PURE__ */ new WeakSet();
var nativeAudioBufferConstructor = createNativeAudioBufferConstructor(window2);
var convertNumberToUnsignedLong = createConvertNumberToUnsignedLong(new Uint32Array(1));
var wrapAudioBufferCopyChannelMethods = createWrapAudioBufferCopyChannelMethods(convertNumberToUnsignedLong, createIndexSizeError);
var wrapAudioBufferCopyChannelMethodsOutOfBounds = createWrapAudioBufferCopyChannelMethodsOutOfBounds(convertNumberToUnsignedLong);
var audioBufferConstructor = createAudioBufferConstructor(audioBufferStore, cacheTestResult, createNotSupportedError, nativeAudioBufferConstructor, nativeOfflineAudioContextConstructor, createTestAudioBufferConstructorSupport(nativeAudioBufferConstructor), wrapAudioBufferCopyChannelMethods, wrapAudioBufferCopyChannelMethodsOutOfBounds);
var addSilentConnection = createAddSilentConnection(createNativeGainNode);
var renderInputsOfAudioParam = createRenderInputsOfAudioParam(getAudioNodeRenderer, getAudioParamConnections, isPartOfACycle);
var connectAudioParam = createConnectAudioParam(renderInputsOfAudioParam);
var createNativeAudioBufferSourceNode = createNativeAudioBufferSourceNodeFactory(addSilentConnection, cacheTestResult, testAudioBufferSourceNodeStartMethodConsecutiveCallsSupport, testAudioBufferSourceNodeStartMethodOffsetClampingSupport, testAudioBufferSourceNodeStopMethodNullifiedBufferSupport, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, wrapAudioBufferSourceNodeStartMethodOffsetClamping, createWrapAudioBufferSourceNodeStopMethodNullifiedBuffer(overwriteAccessors), wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls);
var renderAutomation = createRenderAutomation(createGetAudioParamRenderer(getAudioParamConnections), renderInputsOfAudioParam);
var createAudioBufferSourceNodeRenderer = createAudioBufferSourceNodeRendererFactory(connectAudioParam, createNativeAudioBufferSourceNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode);
var createAudioParam = createAudioParamFactory(createAddAudioParamConnections(AUDIO_PARAM_CONNECTIONS_STORE), audioParamAudioNodeStore, AUDIO_PARAM_STORE, createAudioParamRenderer, import_automation_events2.createCancelAndHoldAutomationEvent, import_automation_events2.createCancelScheduledValuesAutomationEvent, import_automation_events2.createExponentialRampToValueAutomationEvent, import_automation_events2.createLinearRampToValueAutomationEvent, import_automation_events2.createSetTargetAutomationEvent, import_automation_events2.createSetValueAutomationEvent, import_automation_events2.createSetValueCurveAutomationEvent, nativeAudioContextConstructor, setValueAtTimeUntilPossible);
var audioBufferSourceNodeConstructor = createAudioBufferSourceNodeConstructor(audioNodeConstructor, createAudioBufferSourceNodeRenderer, createAudioParam, createInvalidStateError, createNativeAudioBufferSourceNode, getNativeContext, isNativeOfflineAudioContext, wrapEventListener);
var audioDestinationNodeConstructor = createAudioDestinationNodeConstructor(audioNodeConstructor, createAudioDestinationNodeRenderer, createIndexSizeError, createInvalidStateError, createNativeAudioDestinationNodeFactory(createNativeGainNode, overwriteAccessors), getNativeContext, isNativeOfflineAudioContext, renderInputsOfAudioNode);
var createBiquadFilterNodeRenderer = createBiquadFilterNodeRendererFactory(connectAudioParam, createNativeBiquadFilterNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode);
var setAudioNodeTailTime = createSetAudioNodeTailTime(audioNodeTailTimeStore);
var biquadFilterNodeConstructor = createBiquadFilterNodeConstructor(audioNodeConstructor, createAudioParam, createBiquadFilterNodeRenderer, createInvalidAccessError, createNativeBiquadFilterNode, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime);
var monitorConnections = createMonitorConnections(insertElementInSet, isNativeAudioNode2);
var wrapChannelMergerNode = createWrapChannelMergerNode(createInvalidStateError, monitorConnections);
var createNativeChannelMergerNode = createNativeChannelMergerNodeFactory(nativeAudioContextConstructor, wrapChannelMergerNode);
var createChannelMergerNodeRenderer = createChannelMergerNodeRendererFactory(createNativeChannelMergerNode, getNativeAudioNode, renderInputsOfAudioNode);
var channelMergerNodeConstructor = createChannelMergerNodeConstructor(audioNodeConstructor, createChannelMergerNodeRenderer, createNativeChannelMergerNode, getNativeContext, isNativeOfflineAudioContext);
var createChannelSplitterNodeRenderer = createChannelSplitterNodeRendererFactory(createNativeChannelSplitterNode, getNativeAudioNode, renderInputsOfAudioNode);
var channelSplitterNodeConstructor = createChannelSplitterNodeConstructor(audioNodeConstructor, createChannelSplitterNodeRenderer, createNativeChannelSplitterNode, getNativeContext, isNativeOfflineAudioContext, sanitizeChannelSplitterOptions);
var createNativeConstantSourceNodeFaker = createNativeConstantSourceNodeFakerFactory(addSilentConnection, createNativeAudioBufferSourceNode, createNativeGainNode, monitorConnections);
var createNativeConstantSourceNode = createNativeConstantSourceNodeFactory(addSilentConnection, cacheTestResult, createNativeConstantSourceNodeFaker, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport);
var createConstantSourceNodeRenderer = createConstantSourceNodeRendererFactory(connectAudioParam, createNativeConstantSourceNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode);
var constantSourceNodeConstructor = createConstantSourceNodeConstructor(audioNodeConstructor, createAudioParam, createConstantSourceNodeRenderer, createNativeConstantSourceNode, getNativeContext, isNativeOfflineAudioContext, wrapEventListener);
var createNativeConvolverNode = createNativeConvolverNodeFactory(createNotSupportedError, overwriteAccessors);
var createConvolverNodeRenderer = createConvolverNodeRendererFactory(createNativeConvolverNode, getNativeAudioNode, renderInputsOfAudioNode);
var convolverNodeConstructor = createConvolverNodeConstructor(audioNodeConstructor, createConvolverNodeRenderer, createNativeConvolverNode, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime);
var createDelayNodeRenderer = createDelayNodeRendererFactory(connectAudioParam, createNativeDelayNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode);
var delayNodeConstructor = createDelayNodeConstructor(audioNodeConstructor, createAudioParam, createDelayNodeRenderer, createNativeDelayNode, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime);
var createNativeDynamicsCompressorNode = createNativeDynamicsCompressorNodeFactory(createNotSupportedError);
var createDynamicsCompressorNodeRenderer = createDynamicsCompressorNodeRendererFactory(connectAudioParam, createNativeDynamicsCompressorNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode);
var dynamicsCompressorNodeConstructor = createDynamicsCompressorNodeConstructor(audioNodeConstructor, createAudioParam, createDynamicsCompressorNodeRenderer, createNativeDynamicsCompressorNode, createNotSupportedError, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime);
var createGainNodeRenderer = createGainNodeRendererFactory(connectAudioParam, createNativeGainNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode);
var gainNodeConstructor = createGainNodeConstructor(audioNodeConstructor, createAudioParam, createGainNodeRenderer, createNativeGainNode, getNativeContext, isNativeOfflineAudioContext);
var createNativeIIRFilterNodeFaker = createNativeIIRFilterNodeFakerFactory(createInvalidAccessError, createInvalidStateError, createNativeScriptProcessorNode, createNotSupportedError);
var renderNativeOfflineAudioContext = createRenderNativeOfflineAudioContext(cacheTestResult, createNativeGainNode, createNativeScriptProcessorNode, createTestOfflineAudioContextCurrentTimeSupport(createNativeGainNode, nativeOfflineAudioContextConstructor));
var createIIRFilterNodeRenderer = createIIRFilterNodeRendererFactory(createNativeAudioBufferSourceNode, getNativeAudioNode, nativeOfflineAudioContextConstructor, renderInputsOfAudioNode, renderNativeOfflineAudioContext);
var createNativeIIRFilterNode = createNativeIIRFilterNodeFactory(createNativeIIRFilterNodeFaker);
var iIRFilterNodeConstructor = createIIRFilterNodeConstructor(audioNodeConstructor, createNativeIIRFilterNode, createIIRFilterNodeRenderer, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime);
var createAudioListener = createAudioListenerFactory(createAudioParam, createNativeChannelMergerNode, createNativeConstantSourceNode, createNativeScriptProcessorNode, createNotSupportedError, getFirstSample, isNativeOfflineAudioContext, overwriteAccessors);
var unrenderedAudioWorkletNodeStore = /* @__PURE__ */ new WeakMap();
var minimalBaseAudioContextConstructor = createMinimalBaseAudioContextConstructor(audioDestinationNodeConstructor, createAudioListener, eventTargetConstructor, isNativeOfflineAudioContext, unrenderedAudioWorkletNodeStore, wrapEventListener);
var createNativeOscillatorNode = createNativeOscillatorNodeFactory(addSilentConnection, cacheTestResult, testAudioScheduledSourceNodeStartMethodNegativeParametersSupport, testAudioScheduledSourceNodeStopMethodConsecutiveCallsSupport, testAudioScheduledSourceNodeStopMethodNegativeParametersSupport, wrapAudioScheduledSourceNodeStopMethodConsecutiveCalls);
var createOscillatorNodeRenderer = createOscillatorNodeRendererFactory(connectAudioParam, createNativeOscillatorNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode);
var oscillatorNodeConstructor = createOscillatorNodeConstructor(audioNodeConstructor, createAudioParam, createNativeOscillatorNode, createOscillatorNodeRenderer, getNativeContext, isNativeOfflineAudioContext, wrapEventListener);
var createConnectedNativeAudioBufferSourceNode = createConnectedNativeAudioBufferSourceNodeFactory(createNativeAudioBufferSourceNode);
var createNativeWaveShaperNodeFaker = createNativeWaveShaperNodeFakerFactory(createConnectedNativeAudioBufferSourceNode, createInvalidStateError, createNativeGainNode, isDCCurve, monitorConnections);
var createNativeWaveShaperNode = createNativeWaveShaperNodeFactory(createConnectedNativeAudioBufferSourceNode, createInvalidStateError, createNativeWaveShaperNodeFaker, isDCCurve, monitorConnections, nativeAudioContextConstructor, overwriteAccessors);
var createNativePannerNodeFaker = createNativePannerNodeFakerFactory(connectNativeAudioNodeToNativeAudioNode, createInvalidStateError, createNativeChannelMergerNode, createNativeGainNode, createNativeScriptProcessorNode, createNativeWaveShaperNode, createNotSupportedError, disconnectNativeAudioNodeFromNativeAudioNode, getFirstSample, monitorConnections);
var createNativePannerNode = createNativePannerNodeFactory(createNativePannerNodeFaker);
var createPannerNodeRenderer = createPannerNodeRendererFactory(connectAudioParam, createNativeChannelMergerNode, createNativeConstantSourceNode, createNativeGainNode, createNativePannerNode, getNativeAudioNode, nativeOfflineAudioContextConstructor, renderAutomation, renderInputsOfAudioNode, renderNativeOfflineAudioContext);
var pannerNodeConstructor = createPannerNodeConstructor(audioNodeConstructor, createAudioParam, createNativePannerNode, createPannerNodeRenderer, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime);
var createNativePeriodicWave = createNativePeriodicWaveFactory(createIndexSizeError);
var periodicWaveConstructor = createPeriodicWaveConstructor(createNativePeriodicWave, getNativeContext, /* @__PURE__ */ new WeakSet(), sanitizePeriodicWaveOptions);
var nativeStereoPannerNodeFakerFactory = createNativeStereoPannerNodeFakerFactory(createNativeChannelMergerNode, createNativeChannelSplitterNode, createNativeGainNode, createNativeWaveShaperNode, createNotSupportedError, monitorConnections);
var createNativeStereoPannerNode = createNativeStereoPannerNodeFactory(nativeStereoPannerNodeFakerFactory, createNotSupportedError);
var createStereoPannerNodeRenderer = createStereoPannerNodeRendererFactory(connectAudioParam, createNativeStereoPannerNode, getNativeAudioNode, renderAutomation, renderInputsOfAudioNode);
var stereoPannerNodeConstructor = createStereoPannerNodeConstructor(audioNodeConstructor, createAudioParam, createNativeStereoPannerNode, createStereoPannerNodeRenderer, getNativeContext, isNativeOfflineAudioContext);
var createWaveShaperNodeRenderer = createWaveShaperNodeRendererFactory(createNativeWaveShaperNode, getNativeAudioNode, renderInputsOfAudioNode);
var waveShaperNodeConstructor = createWaveShaperNodeConstructor(audioNodeConstructor, createInvalidStateError, createNativeWaveShaperNode, createWaveShaperNodeRenderer, getNativeContext, isNativeOfflineAudioContext, setAudioNodeTailTime);
var isSecureContext = createIsSecureContext(window2);
var exposeCurrentFrameAndCurrentTime = createExposeCurrentFrameAndCurrentTime(window2);
var backupOfflineAudioContextStore = /* @__PURE__ */ new WeakMap();
var getOrCreateBackupOfflineAudioContext = createGetOrCreateBackupOfflineAudioContext(backupOfflineAudioContextStore, nativeOfflineAudioContextConstructor);
var addAudioWorkletModule = isSecureContext ? createAddAudioWorkletModule(
  cacheTestResult,
  createNotSupportedError,
  createEvaluateSource(window2),
  exposeCurrentFrameAndCurrentTime,
  createFetchSource(createAbortError),
  getNativeContext,
  getOrCreateBackupOfflineAudioContext,
  isNativeOfflineAudioContext,
  nativeAudioWorkletNodeConstructor,
  /* @__PURE__ */ new WeakMap(),
  /* @__PURE__ */ new WeakMap(),
  createTestAudioWorkletProcessorPostMessageSupport(nativeAudioWorkletNodeConstructor, nativeOfflineAudioContextConstructor),
  // @todo window is guaranteed to be defined because isSecureContext checks that as well.
  window2
) : void 0;
var isNativeContext = createIsNativeContext(isNativeAudioContext, isNativeOfflineAudioContext);
var decodeAudioData = createDecodeAudioData(audioBufferStore, cacheTestResult, createDataCloneError, createEncodingError, /* @__PURE__ */ new WeakSet(), getNativeContext, isNativeContext, testAudioBufferCopyChannelMethodsOutOfBoundsSupport, testPromiseSupport, wrapAudioBufferCopyChannelMethods, wrapAudioBufferCopyChannelMethodsOutOfBounds);
var baseAudioContextConstructor = createBaseAudioContextConstructor(addAudioWorkletModule, analyserNodeConstructor, audioBufferConstructor, audioBufferSourceNodeConstructor, biquadFilterNodeConstructor, channelMergerNodeConstructor, channelSplitterNodeConstructor, constantSourceNodeConstructor, convolverNodeConstructor, decodeAudioData, delayNodeConstructor, dynamicsCompressorNodeConstructor, gainNodeConstructor, iIRFilterNodeConstructor, minimalBaseAudioContextConstructor, oscillatorNodeConstructor, pannerNodeConstructor, periodicWaveConstructor, stereoPannerNodeConstructor, waveShaperNodeConstructor);
var mediaElementAudioSourceNodeConstructor = createMediaElementAudioSourceNodeConstructor(audioNodeConstructor, createNativeMediaElementAudioSourceNode, getNativeContext, isNativeOfflineAudioContext);
var mediaStreamAudioDestinationNodeConstructor = createMediaStreamAudioDestinationNodeConstructor(audioNodeConstructor, createNativeMediaStreamAudioDestinationNode, getNativeContext, isNativeOfflineAudioContext);
var mediaStreamAudioSourceNodeConstructor = createMediaStreamAudioSourceNodeConstructor(audioNodeConstructor, createNativeMediaStreamAudioSourceNode, getNativeContext, isNativeOfflineAudioContext);
var createNativeMediaStreamTrackAudioSourceNode = createNativeMediaStreamTrackAudioSourceNodeFactory(createInvalidStateError, isNativeOfflineAudioContext);
var mediaStreamTrackAudioSourceNodeConstructor = createMediaStreamTrackAudioSourceNodeConstructor(audioNodeConstructor, createNativeMediaStreamTrackAudioSourceNode, getNativeContext);
var audioContextConstructor = createAudioContextConstructor(baseAudioContextConstructor, createInvalidStateError, createNotSupportedError, createUnknownError, mediaElementAudioSourceNodeConstructor, mediaStreamAudioDestinationNodeConstructor, mediaStreamAudioSourceNodeConstructor, mediaStreamTrackAudioSourceNodeConstructor, nativeAudioContextConstructor);
var getUnrenderedAudioWorkletNodes = createGetUnrenderedAudioWorkletNodes(unrenderedAudioWorkletNodeStore);
var addUnrenderedAudioWorkletNode = createAddUnrenderedAudioWorkletNode(getUnrenderedAudioWorkletNodes);
var connectMultipleOutputs = createConnectMultipleOutputs(createIndexSizeError);
var deleteUnrenderedAudioWorkletNode = createDeleteUnrenderedAudioWorkletNode(getUnrenderedAudioWorkletNodes);
var disconnectMultipleOutputs = createDisconnectMultipleOutputs(createIndexSizeError);
var activeAudioWorkletNodeInputsStore = /* @__PURE__ */ new WeakMap();
var getActiveAudioWorkletNodeInputs = createGetActiveAudioWorkletNodeInputs(activeAudioWorkletNodeInputsStore, getValueForKey);
var createNativeAudioWorkletNodeFaker = createNativeAudioWorkletNodeFakerFactory(connectMultipleOutputs, createIndexSizeError, createInvalidStateError, createNativeChannelMergerNode, createNativeChannelSplitterNode, createNativeConstantSourceNode, createNativeGainNode, createNativeScriptProcessorNode, createNotSupportedError, disconnectMultipleOutputs, exposeCurrentFrameAndCurrentTime, getActiveAudioWorkletNodeInputs, monitorConnections);
var createNativeAudioWorkletNode = createNativeAudioWorkletNodeFactory(createInvalidStateError, createNativeAudioWorkletNodeFaker, createNativeGainNode, createNotSupportedError, monitorConnections);
var createAudioWorkletNodeRenderer = createAudioWorkletNodeRendererFactory(connectAudioParam, connectMultipleOutputs, createNativeAudioBufferSourceNode, createNativeChannelMergerNode, createNativeChannelSplitterNode, createNativeConstantSourceNode, createNativeGainNode, deleteUnrenderedAudioWorkletNode, disconnectMultipleOutputs, exposeCurrentFrameAndCurrentTime, getNativeAudioNode, nativeAudioWorkletNodeConstructor, nativeOfflineAudioContextConstructor, renderAutomation, renderInputsOfAudioNode, renderNativeOfflineAudioContext);
var getBackupOfflineAudioContext = createGetBackupOfflineAudioContext(backupOfflineAudioContextStore);
var setActiveAudioWorkletNodeInputs = createSetActiveAudioWorkletNodeInputs(activeAudioWorkletNodeInputsStore);
var audioWorkletNodeConstructor = isSecureContext ? createAudioWorkletNodeConstructor(addUnrenderedAudioWorkletNode, audioNodeConstructor, createAudioParam, createAudioWorkletNodeRenderer, createNativeAudioWorkletNode, getAudioNodeConnections, getBackupOfflineAudioContext, getNativeContext, isNativeOfflineAudioContext, nativeAudioWorkletNodeConstructor, sanitizeAudioWorkletNodeOptions, setActiveAudioWorkletNodeInputs, testAudioWorkletNodeOptionsClonability, wrapEventListener) : void 0;
var minimalAudioContextConstructor = createMinimalAudioContextConstructor(createInvalidStateError, createNotSupportedError, createUnknownError, minimalBaseAudioContextConstructor, nativeAudioContextConstructor);
var createNativeOfflineAudioContext = createCreateNativeOfflineAudioContext(createNotSupportedError, nativeOfflineAudioContextConstructor);
var startRendering = createStartRendering(audioBufferStore, cacheTestResult, getAudioNodeRenderer, getUnrenderedAudioWorkletNodes, renderNativeOfflineAudioContext, testAudioBufferCopyChannelMethodsOutOfBoundsSupport, wrapAudioBufferCopyChannelMethods, wrapAudioBufferCopyChannelMethodsOutOfBounds);
var minimalOfflineAudioContextConstructor = createMinimalOfflineAudioContextConstructor(cacheTestResult, createInvalidStateError, createNativeOfflineAudioContext, minimalBaseAudioContextConstructor, startRendering);
var offlineAudioContextConstructor = createOfflineAudioContextConstructor(baseAudioContextConstructor, cacheTestResult, createInvalidStateError, createNativeOfflineAudioContext, startRendering);
var isAnyAudioContext = createIsAnyAudioContext(CONTEXT_STORE, isNativeAudioContext);
var isAnyAudioNode = createIsAnyAudioNode(AUDIO_NODE_STORE, isNativeAudioNode2);
var isAnyAudioParam = createIsAnyAudioParam(AUDIO_PARAM_STORE, isNativeAudioParam);
var isAnyOfflineAudioContext = createIsAnyOfflineAudioContext(CONTEXT_STORE, isNativeOfflineAudioContext);

// node_modules/tone/build/esm/core/util/TypeCheck.js
function isUndef(arg) {
  return arg === void 0;
}
function isDefined(arg) {
  return arg !== void 0;
}
function isFunction(arg) {
  return typeof arg === "function";
}
function isNumber(arg) {
  return typeof arg === "number";
}
function isObject(arg) {
  return Object.prototype.toString.call(arg) === "[object Object]" && arg.constructor === Object;
}
function isBoolean(arg) {
  return typeof arg === "boolean";
}
function isArray(arg) {
  return Array.isArray(arg);
}
function isString(arg) {
  return typeof arg === "string";
}
function isNote(arg) {
  return isString(arg) && /^([a-g]{1}(?:b|#|x|bb)?)(-?[0-9]+)/i.test(arg);
}

// node_modules/tone/build/esm/core/util/Debug.js
function assert(statement, error) {
  if (!statement) {
    throw new Error(error);
  }
}
function assertRange(value, gte, lte = Infinity) {
  if (!(gte <= value && value <= lte)) {
    throw new RangeError(`Value must be within [${gte}, ${lte}], got: ${value}`);
  }
}
function assertContextRunning(context2) {
  if (!context2.isOffline && context2.state !== "running") {
    warn('The AudioContext is "suspended". Invoke Tone.start() from a user action to start the audio.');
  }
}
var isInsideScheduledCallback = false;
var printedScheduledWarning = false;
function enterScheduledCallback(insideCallback) {
  isInsideScheduledCallback = insideCallback;
}
function assertUsedScheduleTime(time) {
  if (isUndef(time) && isInsideScheduledCallback && !printedScheduledWarning) {
    printedScheduledWarning = true;
    warn("Events scheduled inside of scheduled callbacks should use the passed in scheduling time. See https://github.com/Tonejs/Tone.js/wiki/Accurate-Timing");
  }
}
var defaultLogger = console;
function log(...args) {
  defaultLogger.log(...args);
}
function warn(...args) {
  defaultLogger.warn(...args);
}

// node_modules/tone/build/esm/core/context/AudioContext.js
function createAudioContext(options) {
  return new audioContextConstructor(options);
}
function createOfflineAudioContext(channels, length, sampleRate) {
  return new offlineAudioContextConstructor(channels, length, sampleRate);
}
var theWindow = typeof self === "object" ? self : null;
var hasAudioContext = theWindow && (theWindow.hasOwnProperty("AudioContext") || theWindow.hasOwnProperty("webkitAudioContext"));
function createAudioWorkletNode(context2, name, options) {
  assert(isDefined(audioWorkletNodeConstructor), "AudioWorkletNode only works in a secure context (https or localhost)");
  return new (context2 instanceof (theWindow === null || theWindow === void 0 ? void 0 : theWindow.BaseAudioContext) ? theWindow === null || theWindow === void 0 ? void 0 : theWindow.AudioWorkletNode : audioWorkletNodeConstructor)(context2, name, options);
}

// node_modules/tslib/tslib.es6.mjs
function __decorate(decorators, target, key, desc) {
  var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;
  if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);
  else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;
  return c > 3 && r && Object.defineProperty(target, key, r), r;
}
function __awaiter(thisArg, _arguments, P, generator) {
  function adopt(value) {
    return value instanceof P ? value : new P(function(resolve) {
      resolve(value);
    });
  }
  return new (P || (P = Promise))(function(resolve, reject) {
    function fulfilled(value) {
      try {
        step(generator.next(value));
      } catch (e) {
        reject(e);
      }
    }
    function rejected(value) {
      try {
        step(generator["throw"](value));
      } catch (e) {
        reject(e);
      }
    }
    function step(result) {
      result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected);
    }
    step((generator = generator.apply(thisArg, _arguments || [])).next());
  });
}

// node_modules/tone/build/esm/core/clock/Ticker.js
var Ticker = class {
  constructor(callback, type, updateInterval, contextSampleRate) {
    this._callback = callback;
    this._type = type;
    this._minimumUpdateInterval = Math.max(128 / (contextSampleRate || 44100), 1e-3);
    this.updateInterval = updateInterval;
    this._createClock();
  }
  /**
   * Generate a web worker
   */
  _createWorker() {
    const blob = new Blob([
      /* javascript */
      `
			// the initial timeout time
			let timeoutTime =  ${(this._updateInterval * 1e3).toFixed(1)};
			// onmessage callback
			self.onmessage = function(msg){
				timeoutTime = parseInt(msg.data);
			};
			// the tick function which posts a message
			// and schedules a new tick
			function tick(){
				setTimeout(tick, timeoutTime);
				self.postMessage('tick');
			}
			// call tick initially
			tick();
			`
    ], { type: "text/javascript" });
    const blobUrl = URL.createObjectURL(blob);
    const worker = new Worker(blobUrl);
    worker.onmessage = this._callback.bind(this);
    this._worker = worker;
  }
  /**
   * Create a timeout loop
   */
  _createTimeout() {
    this._timeout = setTimeout(() => {
      this._createTimeout();
      this._callback();
    }, this._updateInterval * 1e3);
  }
  /**
   * Create the clock source.
   */
  _createClock() {
    if (this._type === "worker") {
      try {
        this._createWorker();
      } catch (e) {
        this._type = "timeout";
        this._createClock();
      }
    } else if (this._type === "timeout") {
      this._createTimeout();
    }
  }
  /**
   * Clean up the current clock source
   */
  _disposeClock() {
    if (this._timeout) {
      clearTimeout(this._timeout);
    }
    if (this._worker) {
      this._worker.terminate();
      this._worker.onmessage = null;
    }
  }
  /**
   * The rate in seconds the ticker will update
   */
  get updateInterval() {
    return this._updateInterval;
  }
  set updateInterval(interval) {
    var _a;
    this._updateInterval = Math.max(interval, this._minimumUpdateInterval);
    if (this._type === "worker") {
      (_a = this._worker) === null || _a === void 0 ? void 0 : _a.postMessage(this._updateInterval * 1e3);
    }
  }
  /**
   * The type of the ticker, either a worker or a timeout
   */
  get type() {
    return this._type;
  }
  set type(type) {
    this._disposeClock();
    this._type = type;
    this._createClock();
  }
  /**
   * Clean up
   */
  dispose() {
    this._disposeClock();
  }
};

// node_modules/tone/build/esm/core/util/AdvancedTypeCheck.js
function isAudioParam(arg) {
  return isAnyAudioParam(arg);
}
function isAudioNode2(arg) {
  return isAnyAudioNode(arg);
}
function isOfflineAudioContext(arg) {
  return isAnyOfflineAudioContext(arg);
}
function isAudioContext(arg) {
  return isAnyAudioContext(arg);
}
function isAudioBuffer(arg) {
  return arg instanceof audioBufferConstructor;
}

// node_modules/tone/build/esm/core/util/Defaults.js
function noCopy(key, arg) {
  return key === "value" || isAudioParam(arg) || isAudioNode2(arg) || isAudioBuffer(arg);
}
function deepMerge(target, ...sources) {
  if (!sources.length) {
    return target;
  }
  const source = sources.shift();
  if (isObject(target) && isObject(source)) {
    for (const key in source) {
      if (noCopy(key, source[key])) {
        target[key] = source[key];
      } else if (isObject(source[key])) {
        if (!target[key]) {
          Object.assign(target, { [key]: {} });
        }
        deepMerge(target[key], source[key]);
      } else {
        Object.assign(target, { [key]: source[key] });
      }
    }
  }
  return deepMerge(target, ...sources);
}
function deepEquals(arrayA, arrayB) {
  return arrayA.length === arrayB.length && arrayA.every((element, index) => arrayB[index] === element);
}
function optionsFromArguments(defaults, argsArray, keys = [], objKey) {
  const opts = {};
  const args = Array.from(argsArray);
  if (isObject(args[0]) && objKey && !Reflect.has(args[0], objKey)) {
    const partOfDefaults = Object.keys(args[0]).some((key) => Reflect.has(defaults, key));
    if (!partOfDefaults) {
      deepMerge(opts, { [objKey]: args[0] });
      keys.splice(keys.indexOf(objKey), 1);
      args.shift();
    }
  }
  if (args.length === 1 && isObject(args[0])) {
    deepMerge(opts, args[0]);
  } else {
    for (let i = 0; i < keys.length; i++) {
      if (isDefined(args[i])) {
        opts[keys[i]] = args[i];
      }
    }
  }
  return deepMerge(defaults, opts);
}
function getDefaultsFromInstance(instance) {
  return instance.constructor.getDefaults();
}
function defaultArg(given, fallback) {
  if (isUndef(given)) {
    return fallback;
  } else {
    return given;
  }
}
function omitFromObject(obj, omit) {
  omit.forEach((prop) => {
    if (Reflect.has(obj, prop)) {
      delete obj[prop];
    }
  });
  return obj;
}

// node_modules/tone/build/esm/core/Tone.js
var Tone = class {
  constructor() {
    this.debug = false;
    this._wasDisposed = false;
  }
  /**
   * Returns all of the default options belonging to the class.
   */
  static getDefaults() {
    return {};
  }
  /**
   * Prints the outputs to the console log for debugging purposes.
   * Prints the contents only if either the object has a property
   * called `debug` set to true, or a variable called TONE_DEBUG_CLASS
   * is set to the name of the class.
   * @example
   * const osc = new Tone.Oscillator();
   * // prints all logs originating from this oscillator
   * osc.debug = true;
   * // calls to start/stop will print in the console
   * osc.start();
   */
  log(...args) {
    if (this.debug || theWindow && this.toString() === theWindow.TONE_DEBUG_CLASS) {
      log(this, ...args);
    }
  }
  /**
   * disconnect and dispose.
   */
  dispose() {
    this._wasDisposed = true;
    return this;
  }
  /**
   * Indicates if the instance was disposed. 'Disposing' an
   * instance means that all of the Web Audio nodes that were
   * created for the instance are disconnected and freed for garbage collection.
   */
  get disposed() {
    return this._wasDisposed;
  }
  /**
   * Convert the class to a string
   * @example
   * const osc = new Tone.Oscillator();
   * console.log(osc.toString());
   */
  toString() {
    return this.name;
  }
};
Tone.version = version;

// node_modules/tone/build/esm/core/util/Math.js
var EPSILON = 1e-6;
function GT(a, b) {
  return a > b + EPSILON;
}
function GTE(a, b) {
  return GT(a, b) || EQ(a, b);
}
function LT(a, b) {
  return a + EPSILON < b;
}
function EQ(a, b) {
  return Math.abs(a - b) < EPSILON;
}
function clamp(value, min, max) {
  return Math.max(Math.min(value, max), min);
}

// node_modules/tone/build/esm/core/util/Timeline.js
var Timeline = class _Timeline extends Tone {
  constructor() {
    super();
    this.name = "Timeline";
    this._timeline = [];
    const options = optionsFromArguments(_Timeline.getDefaults(), arguments, ["memory"]);
    this.memory = options.memory;
    this.increasing = options.increasing;
  }
  static getDefaults() {
    return {
      memory: Infinity,
      increasing: false
    };
  }
  /**
   * The number of items in the timeline.
   */
  get length() {
    return this._timeline.length;
  }
  /**
   * Insert an event object onto the timeline. Events must have a "time" attribute.
   * @param event  The event object to insert into the timeline.
   */
  add(event) {
    assert(Reflect.has(event, "time"), "Timeline: events must have a time attribute");
    event.time = event.time.valueOf();
    if (this.increasing && this.length) {
      const lastValue = this._timeline[this.length - 1];
      assert(GTE(event.time, lastValue.time), "The time must be greater than or equal to the last scheduled time");
      this._timeline.push(event);
    } else {
      const index = this._search(event.time);
      this._timeline.splice(index + 1, 0, event);
    }
    if (this.length > this.memory) {
      const diff = this.length - this.memory;
      this._timeline.splice(0, diff);
    }
    return this;
  }
  /**
   * Remove an event from the timeline.
   * @param  {Object}  event  The event object to remove from the list.
   * @returns {Timeline} this
   */
  remove(event) {
    const index = this._timeline.indexOf(event);
    if (index !== -1) {
      this._timeline.splice(index, 1);
    }
    return this;
  }
  /**
   * Get the nearest event whose time is less than or equal to the given time.
   * @param  time  The time to query.
   */
  get(time, param = "time") {
    const index = this._search(time, param);
    if (index !== -1) {
      return this._timeline[index];
    } else {
      return null;
    }
  }
  /**
   * Return the first event in the timeline without removing it
   * @returns {Object} The first event object
   * @deprecated
   */
  peek() {
    return this._timeline[0];
  }
  /**
   * Return the first event in the timeline and remove it
   * @deprecated
   */
  shift() {
    return this._timeline.shift();
  }
  /**
   * Get the event which is scheduled after the given time.
   * @param  time  The time to query.
   */
  getAfter(time, param = "time") {
    const index = this._search(time, param);
    if (index + 1 < this._timeline.length) {
      return this._timeline[index + 1];
    } else {
      return null;
    }
  }
  /**
   * Get the event before the event at the given time.
   * @param  time  The time to query.
   */
  getBefore(time) {
    const len = this._timeline.length;
    if (len > 0 && this._timeline[len - 1].time < time) {
      return this._timeline[len - 1];
    }
    const index = this._search(time);
    if (index - 1 >= 0) {
      return this._timeline[index - 1];
    } else {
      return null;
    }
  }
  /**
   * Cancel events at and after the given time
   * @param  after  The time to query.
   */
  cancel(after) {
    if (this._timeline.length > 1) {
      let index = this._search(after);
      if (index >= 0) {
        if (EQ(this._timeline[index].time, after)) {
          for (let i = index; i >= 0; i--) {
            if (EQ(this._timeline[i].time, after)) {
              index = i;
            } else {
              break;
            }
          }
          this._timeline = this._timeline.slice(0, index);
        } else {
          this._timeline = this._timeline.slice(0, index + 1);
        }
      } else {
        this._timeline = [];
      }
    } else if (this._timeline.length === 1) {
      if (GTE(this._timeline[0].time, after)) {
        this._timeline = [];
      }
    }
    return this;
  }
  /**
   * Cancel events before or equal to the given time.
   * @param  time  The time to cancel before.
   */
  cancelBefore(time) {
    const index = this._search(time);
    if (index >= 0) {
      this._timeline = this._timeline.slice(index + 1);
    }
    return this;
  }
  /**
   * Returns the previous event if there is one. null otherwise
   * @param  event The event to find the previous one of
   * @return The event right before the given event
   */
  previousEvent(event) {
    const index = this._timeline.indexOf(event);
    if (index > 0) {
      return this._timeline[index - 1];
    } else {
      return null;
    }
  }
  /**
   * Does a binary search on the timeline array and returns the
   * nearest event index whose time is after or equal to the given time.
   * If a time is searched before the first index in the timeline, -1 is returned.
   * If the time is after the end, the index of the last item is returned.
   */
  _search(time, param = "time") {
    if (this._timeline.length === 0) {
      return -1;
    }
    let beginning = 0;
    const len = this._timeline.length;
    let end = len;
    if (len > 0 && this._timeline[len - 1][param] <= time) {
      return len - 1;
    }
    while (beginning < end) {
      let midPoint = Math.floor(beginning + (end - beginning) / 2);
      const event = this._timeline[midPoint];
      const nextEvent = this._timeline[midPoint + 1];
      if (EQ(event[param], time)) {
        for (let i = midPoint; i < this._timeline.length; i++) {
          const testEvent = this._timeline[i];
          if (EQ(testEvent[param], time)) {
            midPoint = i;
          } else {
            break;
          }
        }
        return midPoint;
      } else if (LT(event[param], time) && GT(nextEvent[param], time)) {
        return midPoint;
      } else if (GT(event[param], time)) {
        end = midPoint;
      } else {
        beginning = midPoint + 1;
      }
    }
    return -1;
  }
  /**
   * Internal iterator. Applies extra safety checks for
   * removing items from the array.
   */
  _iterate(callback, lowerBound = 0, upperBound = this._timeline.length - 1) {
    this._timeline.slice(lowerBound, upperBound + 1).forEach(callback);
  }
  /**
   * Iterate over everything in the array
   * @param  callback The callback to invoke with every item
   */
  forEach(callback) {
    this._iterate(callback);
    return this;
  }
  /**
   * Iterate over everything in the array at or before the given time.
   * @param  time The time to check if items are before
   * @param  callback The callback to invoke with every item
   */
  forEachBefore(time, callback) {
    const upperBound = this._search(time);
    if (upperBound !== -1) {
      this._iterate(callback, 0, upperBound);
    }
    return this;
  }
  /**
   * Iterate over everything in the array after the given time.
   * @param  time The time to check if items are before
   * @param  callback The callback to invoke with every item
   */
  forEachAfter(time, callback) {
    const lowerBound = this._search(time);
    this._iterate(callback, lowerBound + 1);
    return this;
  }
  /**
   * Iterate over everything in the array between the startTime and endTime.
   * The timerange is inclusive of the startTime, but exclusive of the endTime.
   * range = [startTime, endTime).
   * @param  startTime The time to check if items are before
   * @param  endTime The end of the test interval.
   * @param  callback The callback to invoke with every item
   */
  forEachBetween(startTime, endTime, callback) {
    let lowerBound = this._search(startTime);
    let upperBound = this._search(endTime);
    if (lowerBound !== -1 && upperBound !== -1) {
      if (this._timeline[lowerBound].time !== startTime) {
        lowerBound += 1;
      }
      if (this._timeline[upperBound].time === endTime) {
        upperBound -= 1;
      }
      this._iterate(callback, lowerBound, upperBound);
    } else if (lowerBound === -1) {
      this._iterate(callback, 0, upperBound);
    }
    return this;
  }
  /**
   * Iterate over everything in the array at or after the given time. Similar to
   * forEachAfter, but includes the item(s) at the given time.
   * @param  time The time to check if items are before
   * @param  callback The callback to invoke with every item
   */
  forEachFrom(time, callback) {
    let lowerBound = this._search(time);
    while (lowerBound >= 0 && this._timeline[lowerBound].time >= time) {
      lowerBound--;
    }
    this._iterate(callback, lowerBound + 1);
    return this;
  }
  /**
   * Iterate over everything in the array at the given time
   * @param  time The time to check if items are before
   * @param  callback The callback to invoke with every item
   */
  forEachAtTime(time, callback) {
    const upperBound = this._search(time);
    if (upperBound !== -1 && EQ(this._timeline[upperBound].time, time)) {
      let lowerBound = upperBound;
      for (let i = upperBound; i >= 0; i--) {
        if (EQ(this._timeline[i].time, time)) {
          lowerBound = i;
        } else {
          break;
        }
      }
      this._iterate((event) => {
        callback(event);
      }, lowerBound, upperBound);
    }
    return this;
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._timeline = [];
    return this;
  }
};

// node_modules/tone/build/esm/core/context/ContextInitialization.js
var notifyNewContext = [];
function onContextInit(cb) {
  notifyNewContext.push(cb);
}
function initializeContext(ctx) {
  notifyNewContext.forEach((cb) => cb(ctx));
}
var notifyCloseContext = [];
function onContextClose(cb) {
  notifyCloseContext.push(cb);
}
function closeContext(ctx) {
  notifyCloseContext.forEach((cb) => cb(ctx));
}

// node_modules/tone/build/esm/core/util/Emitter.js
var Emitter = class _Emitter extends Tone {
  constructor() {
    super(...arguments);
    this.name = "Emitter";
  }
  /**
   * Bind a callback to a specific event.
   * @param  event     The name of the event to listen for.
   * @param  callback  The callback to invoke when the event is emitted
   */
  on(event, callback) {
    const events = event.split(/\W+/);
    events.forEach((eventName) => {
      if (isUndef(this._events)) {
        this._events = {};
      }
      if (!this._events.hasOwnProperty(eventName)) {
        this._events[eventName] = [];
      }
      this._events[eventName].push(callback);
    });
    return this;
  }
  /**
   * Bind a callback which is only invoked once
   * @param  event     The name of the event to listen for.
   * @param  callback  The callback to invoke when the event is emitted
   */
  once(event, callback) {
    const boundCallback = (...args) => {
      callback(...args);
      this.off(event, boundCallback);
    };
    this.on(event, boundCallback);
    return this;
  }
  /**
   * Remove the event listener.
   * @param  event     The event to stop listening to.
   * @param  callback  The callback which was bound to the event with Emitter.on.
   *                   If no callback is given, all callbacks events are removed.
   */
  off(event, callback) {
    const events = event.split(/\W+/);
    events.forEach((eventName) => {
      if (isUndef(this._events)) {
        this._events = {};
      }
      if (this._events.hasOwnProperty(eventName)) {
        if (isUndef(callback)) {
          this._events[eventName] = [];
        } else {
          const eventList = this._events[eventName];
          for (let i = eventList.length - 1; i >= 0; i--) {
            if (eventList[i] === callback) {
              eventList.splice(i, 1);
            }
          }
        }
      }
    });
    return this;
  }
  /**
   * Invoke all of the callbacks bound to the event
   * with any arguments passed in.
   * @param  event  The name of the event.
   * @param args The arguments to pass to the functions listening.
   */
  emit(event, ...args) {
    if (this._events) {
      if (this._events.hasOwnProperty(event)) {
        const eventList = this._events[event].slice(0);
        for (let i = 0, len = eventList.length; i < len; i++) {
          eventList[i].apply(this, args);
        }
      }
    }
    return this;
  }
  /**
   * Add Emitter functions (on/off/emit) to the object
   */
  static mixin(constr) {
    ["on", "once", "off", "emit"].forEach((name) => {
      const property = Object.getOwnPropertyDescriptor(_Emitter.prototype, name);
      Object.defineProperty(constr.prototype, name, property);
    });
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    this._events = void 0;
    return this;
  }
};

// node_modules/tone/build/esm/core/context/BaseContext.js
var BaseContext = class extends Emitter {
  constructor() {
    super(...arguments);
    this.isOffline = false;
  }
  /*
   * This is a placeholder so that JSON.stringify does not throw an error
   * This matches what JSON.stringify(audioContext) returns on a native
   * audioContext instance.
   */
  toJSON() {
    return {};
  }
};

// node_modules/tone/build/esm/core/context/Context.js
var Context = class _Context extends BaseContext {
  constructor() {
    var _a, _b;
    super();
    this.name = "Context";
    this._constants = /* @__PURE__ */ new Map();
    this._timeouts = new Timeline();
    this._timeoutIds = 0;
    this._initialized = false;
    this._closeStarted = false;
    this.isOffline = false;
    this._workletPromise = null;
    const options = optionsFromArguments(_Context.getDefaults(), arguments, [
      "context"
    ]);
    if (options.context) {
      this._context = options.context;
      this._latencyHint = ((_a = arguments[0]) === null || _a === void 0 ? void 0 : _a.latencyHint) || "";
    } else {
      this._context = createAudioContext({
        latencyHint: options.latencyHint
      });
      this._latencyHint = options.latencyHint;
    }
    this._ticker = new Ticker(this.emit.bind(this, "tick"), options.clockSource, options.updateInterval, this._context.sampleRate);
    this.on("tick", this._timeoutLoop.bind(this));
    this._context.onstatechange = () => {
      this.emit("statechange", this.state);
    };
    this[((_b = arguments[0]) === null || _b === void 0 ? void 0 : _b.hasOwnProperty("updateInterval")) ? "_lookAhead" : "lookAhead"] = options.lookAhead;
  }
  static getDefaults() {
    return {
      clockSource: "worker",
      latencyHint: "interactive",
      lookAhead: 0.1,
      updateInterval: 0.05
    };
  }
  /**
   * Finish setting up the context. **You usually do not need to do this manually.**
   */
  initialize() {
    if (!this._initialized) {
      initializeContext(this);
      this._initialized = true;
    }
    return this;
  }
  //---------------------------
  // BASE AUDIO CONTEXT METHODS
  //---------------------------
  createAnalyser() {
    return this._context.createAnalyser();
  }
  createOscillator() {
    return this._context.createOscillator();
  }
  createBufferSource() {
    return this._context.createBufferSource();
  }
  createBiquadFilter() {
    return this._context.createBiquadFilter();
  }
  createBuffer(numberOfChannels, length, sampleRate) {
    return this._context.createBuffer(numberOfChannels, length, sampleRate);
  }
  createChannelMerger(numberOfInputs) {
    return this._context.createChannelMerger(numberOfInputs);
  }
  createChannelSplitter(numberOfOutputs) {
    return this._context.createChannelSplitter(numberOfOutputs);
  }
  createConstantSource() {
    return this._context.createConstantSource();
  }
  createConvolver() {
    return this._context.createConvolver();
  }
  createDelay(maxDelayTime) {
    return this._context.createDelay(maxDelayTime);
  }
  createDynamicsCompressor() {
    return this._context.createDynamicsCompressor();
  }
  createGain() {
    return this._context.createGain();
  }
  createIIRFilter(feedForward, feedback) {
    return this._context.createIIRFilter(feedForward, feedback);
  }
  createPanner() {
    return this._context.createPanner();
  }
  createPeriodicWave(real, imag, constraints) {
    return this._context.createPeriodicWave(real, imag, constraints);
  }
  createStereoPanner() {
    return this._context.createStereoPanner();
  }
  createWaveShaper() {
    return this._context.createWaveShaper();
  }
  createMediaStreamSource(stream) {
    assert(isAudioContext(this._context), "Not available if OfflineAudioContext");
    const context2 = this._context;
    return context2.createMediaStreamSource(stream);
  }
  createMediaElementSource(element) {
    assert(isAudioContext(this._context), "Not available if OfflineAudioContext");
    const context2 = this._context;
    return context2.createMediaElementSource(element);
  }
  createMediaStreamDestination() {
    assert(isAudioContext(this._context), "Not available if OfflineAudioContext");
    const context2 = this._context;
    return context2.createMediaStreamDestination();
  }
  decodeAudioData(audioData) {
    return this._context.decodeAudioData(audioData);
  }
  /**
   * The current time in seconds of the AudioContext.
   */
  get currentTime() {
    return this._context.currentTime;
  }
  /**
   * The current time in seconds of the AudioContext.
   */
  get state() {
    return this._context.state;
  }
  /**
   * The current time in seconds of the AudioContext.
   */
  get sampleRate() {
    return this._context.sampleRate;
  }
  /**
   * The listener
   */
  get listener() {
    this.initialize();
    return this._listener;
  }
  set listener(l) {
    assert(!this._initialized, "The listener cannot be set after initialization.");
    this._listener = l;
  }
  /**
   * There is only one Transport per Context. It is created on initialization.
   */
  get transport() {
    this.initialize();
    return this._transport;
  }
  set transport(t) {
    assert(!this._initialized, "The transport cannot be set after initialization.");
    this._transport = t;
  }
  /**
   * This is the Draw object for the context which is useful for synchronizing the draw frame with the Tone.js clock.
   */
  get draw() {
    this.initialize();
    return this._draw;
  }
  set draw(d) {
    assert(!this._initialized, "Draw cannot be set after initialization.");
    this._draw = d;
  }
  /**
   * A reference to the Context's destination node.
   */
  get destination() {
    this.initialize();
    return this._destination;
  }
  set destination(d) {
    assert(!this._initialized, "The destination cannot be set after initialization.");
    this._destination = d;
  }
  /**
   * Create an audio worklet node from a name and options. The module
   * must first be loaded using {@link addAudioWorkletModule}.
   */
  createAudioWorkletNode(name, options) {
    return createAudioWorkletNode(this.rawContext, name, options);
  }
  /**
   * Add an AudioWorkletProcessor module
   * @param url The url of the module
   */
  addAudioWorkletModule(url) {
    return __awaiter(this, void 0, void 0, function* () {
      assert(isDefined(this.rawContext.audioWorklet), "AudioWorkletNode is only available in a secure context (https or localhost)");
      if (!this._workletPromise) {
        this._workletPromise = this.rawContext.audioWorklet.addModule(url);
      }
      yield this._workletPromise;
    });
  }
  /**
   * Returns a promise which resolves when all of the worklets have been loaded on this context
   */
  workletsAreReady() {
    return __awaiter(this, void 0, void 0, function* () {
      (yield this._workletPromise) ? this._workletPromise : Promise.resolve();
    });
  }
  //---------------------------
  // TICKER
  //---------------------------
  /**
   * How often the interval callback is invoked.
   * This number corresponds to how responsive the scheduling
   * can be. Setting to 0 will result in the lowest practial interval
   * based on context properties. context.updateInterval + context.lookAhead
   * gives you the total latency between scheduling an event and hearing it.
   */
  get updateInterval() {
    return this._ticker.updateInterval;
  }
  set updateInterval(interval) {
    this._ticker.updateInterval = interval;
  }
  /**
   * What the source of the clock is, either "worker" (default),
   * "timeout", or "offline" (none).
   */
  get clockSource() {
    return this._ticker.type;
  }
  set clockSource(type) {
    this._ticker.type = type;
  }
  /**
   * The amount of time into the future events are scheduled. Giving Web Audio
   * a short amount of time into the future to schedule events can reduce clicks and
   * improve performance. This value can be set to 0 to get the lowest latency.
   * Adjusting this value also affects the {@link updateInterval}.
   */
  get lookAhead() {
    return this._lookAhead;
  }
  set lookAhead(time) {
    this._lookAhead = time;
    this.updateInterval = time ? time / 2 : 0.01;
  }
  /**
   * The type of playback, which affects tradeoffs between audio
   * output latency and responsiveness.
   * In addition to setting the value in seconds, the latencyHint also
   * accepts the strings "interactive" (prioritizes low latency),
   * "playback" (prioritizes sustained playback), "balanced" (balances
   * latency and performance).
   * @example
   * // prioritize sustained playback
   * const context = new Tone.Context({ latencyHint: "playback" });
   * // set this context as the global Context
   * Tone.setContext(context);
   * // the global context is gettable with Tone.getContext()
   * console.log(Tone.getContext().latencyHint);
   */
  get latencyHint() {
    return this._latencyHint;
  }
  /**
   * The unwrapped AudioContext or OfflineAudioContext
   */
  get rawContext() {
    return this._context;
  }
  /**
   * The current audio context time plus a short {@link lookAhead}.
   * @example
   * setInterval(() => {
   * 	console.log("now", Tone.now());
   * }, 100);
   */
  now() {
    return this._context.currentTime + this._lookAhead;
  }
  /**
   * The current audio context time without the {@link lookAhead}.
   * In most cases it is better to use {@link now} instead of {@link immediate} since
   * with {@link now} the {@link lookAhead} is applied equally to _all_ components including internal components,
   * to making sure that everything is scheduled in sync. Mixing {@link now} and {@link immediate}
   * can cause some timing issues. If no lookAhead is desired, you can set the {@link lookAhead} to `0`.
   */
  immediate() {
    return this._context.currentTime;
  }
  /**
   * Starts the audio context from a suspended state. This is required
   * to initially start the AudioContext.
   * @see {@link start}
   */
  resume() {
    if (isAudioContext(this._context)) {
      return this._context.resume();
    } else {
      return Promise.resolve();
    }
  }
  /**
   * Close the context. Once closed, the context can no longer be used and
   * any AudioNodes created from the context will be silent.
   */
  close() {
    return __awaiter(this, void 0, void 0, function* () {
      if (isAudioContext(this._context) && this.state !== "closed" && !this._closeStarted) {
        this._closeStarted = true;
        yield this._context.close();
      }
      if (this._initialized) {
        closeContext(this);
      }
    });
  }
  /**
   * **Internal** Generate a looped buffer at some constant value.
   */
  getConstant(val) {
    if (this._constants.has(val)) {
      return this._constants.get(val);
    } else {
      const buffer = this._context.createBuffer(1, 128, this._context.sampleRate);
      const arr = buffer.getChannelData(0);
      for (let i = 0; i < arr.length; i++) {
        arr[i] = val;
      }
      const constant = this._context.createBufferSource();
      constant.channelCount = 1;
      constant.channelCountMode = "explicit";
      constant.buffer = buffer;
      constant.loop = true;
      constant.start(0);
      this._constants.set(val, constant);
      return constant;
    }
  }
  /**
   * Clean up. Also closes the audio context.
   */
  dispose() {
    super.dispose();
    this._ticker.dispose();
    this._timeouts.dispose();
    Object.keys(this._constants).map((val) => this._constants[val].disconnect());
    this.close();
    return this;
  }
  //---------------------------
  // TIMEOUTS
  //---------------------------
  /**
   * The private loop which keeps track of the context scheduled timeouts
   * Is invoked from the clock source
   */
  _timeoutLoop() {
    const now = this.now();
    this._timeouts.forEachBefore(now, (event) => {
      event.callback();
      this._timeouts.remove(event);
    });
  }
  /**
   * A setTimeout which is guaranteed by the clock source.
   * Also runs in the offline context.
   * @param  fn       The callback to invoke
   * @param  timeout  The timeout in seconds
   * @returns ID to use when invoking Context.clearTimeout
   */
  setTimeout(fn, timeout) {
    this._timeoutIds++;
    const now = this.now();
    this._timeouts.add({
      callback: fn,
      id: this._timeoutIds,
      time: now + timeout
    });
    return this._timeoutIds;
  }
  /**
   * Clears a previously scheduled timeout with Tone.context.setTimeout
   * @param  id  The ID returned from setTimeout
   */
  clearTimeout(id) {
    this._timeouts.forEach((event) => {
      if (event.id === id) {
        this._timeouts.remove(event);
      }
    });
    return this;
  }
  /**
   * Clear the function scheduled by {@link setInterval}
   */
  clearInterval(id) {
    return this.clearTimeout(id);
  }
  /**
   * Adds a repeating event to the context's callback clock
   */
  setInterval(fn, interval) {
    const id = ++this._timeoutIds;
    const intervalFn = () => {
      const now = this.now();
      this._timeouts.add({
        callback: () => {
          fn();
          intervalFn();
        },
        id,
        time: now + interval
      });
    };
    intervalFn();
    return id;
  }
};

// node_modules/tone/build/esm/core/context/DummyContext.js
var DummyContext = class extends BaseContext {
  constructor() {
    super(...arguments);
    this.lookAhead = 0;
    this.latencyHint = 0;
    this.isOffline = false;
  }
  //---------------------------
  // BASE AUDIO CONTEXT METHODS
  //---------------------------
  createAnalyser() {
    return {};
  }
  createOscillator() {
    return {};
  }
  createBufferSource() {
    return {};
  }
  createBiquadFilter() {
    return {};
  }
  createBuffer(_numberOfChannels, _length, _sampleRate) {
    return {};
  }
  createChannelMerger(_numberOfInputs) {
    return {};
  }
  createChannelSplitter(_numberOfOutputs) {
    return {};
  }
  createConstantSource() {
    return {};
  }
  createConvolver() {
    return {};
  }
  createDelay(_maxDelayTime) {
    return {};
  }
  createDynamicsCompressor() {
    return {};
  }
  createGain() {
    return {};
  }
  createIIRFilter(_feedForward, _feedback) {
    return {};
  }
  createPanner() {
    return {};
  }
  createPeriodicWave(_real, _imag, _constraints) {
    return {};
  }
  createStereoPanner() {
    return {};
  }
  createWaveShaper() {
    return {};
  }
  createMediaStreamSource(_stream) {
    return {};
  }
  createMediaElementSource(_element) {
    return {};
  }
  createMediaStreamDestination() {
    return {};
  }
  decodeAudioData(_audioData) {
    return Promise.resolve({});
  }
  //---------------------------
  // TONE AUDIO CONTEXT METHODS
  //---------------------------
  createAudioWorkletNode(_name, _options) {
    return {};
  }
  get rawContext() {
    return {};
  }
  addAudioWorkletModule(_url) {
    return __awaiter(this, void 0, void 0, function* () {
      return Promise.resolve();
    });
  }
  resume() {
    return Promise.resolve();
  }
  setTimeout(_fn, _timeout) {
    return 0;
  }
  clearTimeout(_id) {
    return this;
  }
  setInterval(_fn, _interval) {
    return 0;
  }
  clearInterval(_id) {
    return this;
  }
  getConstant(_val) {
    return {};
  }
  get currentTime() {
    return 0;
  }
  get state() {
    return {};
  }
  get sampleRate() {
    return 0;
  }
  get listener() {
    return {};
  }
  get transport() {
    return {};
  }
  get draw() {
    return {};
  }
  set draw(_d) {
  }
  get destination() {
    return {};
  }
  set destination(_d) {
  }
  now() {
    return 0;
  }
  immediate() {
    return 0;
  }
};

// node_modules/tone/build/esm/core/util/Interface.js
function readOnly(target, property) {
  if (isArray(property)) {
    property.forEach((str) => readOnly(target, str));
  } else {
    Object.defineProperty(target, property, {
      enumerable: true,
      writable: false
    });
  }
}
function writable(target, property) {
  if (isArray(property)) {
    property.forEach((str) => writable(target, str));
  } else {
    Object.defineProperty(target, property, {
      writable: true
    });
  }
}
var noOp = () => {
};

// node_modules/tone/build/esm/core/context/ToneAudioBuffer.js
var ToneAudioBuffer = class _ToneAudioBuffer extends Tone {
  constructor() {
    super();
    this.name = "ToneAudioBuffer";
    this.onload = noOp;
    const options = optionsFromArguments(_ToneAudioBuffer.getDefaults(), arguments, ["url", "onload", "onerror"]);
    this.reverse = options.reverse;
    this.onload = options.onload;
    if (isString(options.url)) {
      this.load(options.url).catch(options.onerror);
    } else if (options.url) {
      this.set(options.url);
    }
  }
  static getDefaults() {
    return {
      onerror: noOp,
      onload: noOp,
      reverse: false
    };
  }
  /**
   * The sample rate of the AudioBuffer
   */
  get sampleRate() {
    if (this._buffer) {
      return this._buffer.sampleRate;
    } else {
      return getContext().sampleRate;
    }
  }
  /**
   * Pass in an AudioBuffer or ToneAudioBuffer to set the value of this buffer.
   */
  set(buffer) {
    if (buffer instanceof _ToneAudioBuffer) {
      if (buffer.loaded) {
        this._buffer = buffer.get();
      } else {
        buffer.onload = () => {
          this.set(buffer);
          this.onload(this);
        };
      }
    } else {
      this._buffer = buffer;
    }
    if (this._reversed) {
      this._reverse();
    }
    return this;
  }
  /**
   * The audio buffer stored in the object.
   */
  get() {
    return this._buffer;
  }
  /**
   * Makes an fetch request for the selected url then decodes the file as an audio buffer.
   * Invokes the callback once the audio buffer loads.
   * @param url The url of the buffer to load. filetype support depends on the browser.
   * @returns A Promise which resolves with this ToneAudioBuffer
   */
  load(url) {
    return __awaiter(this, void 0, void 0, function* () {
      const doneLoading = _ToneAudioBuffer.load(url).then((audioBuffer) => {
        this.set(audioBuffer);
        this.onload(this);
      });
      _ToneAudioBuffer.downloads.push(doneLoading);
      try {
        yield doneLoading;
      } finally {
        const index = _ToneAudioBuffer.downloads.indexOf(doneLoading);
        _ToneAudioBuffer.downloads.splice(index, 1);
      }
      return this;
    });
  }
  /**
   * clean up
   */
  dispose() {
    super.dispose();
    this._buffer = void 0;
    return this;
  }
  /**
   * Set the audio buffer from the array.
   * To create a multichannel AudioBuffer, pass in a multidimensional array.
   * @param array The array to fill the audio buffer
   */
  fromArray(array) {
    const isMultidimensional = isArray(array) && array[0].length > 0;
    const channels = isMultidimensional ? array.length : 1;
    const len = isMultidimensional ? array[0].length : array.length;
    const context2 = getContext();
    const buffer = context2.createBuffer(channels, len, context2.sampleRate);
    const multiChannelArray = !isMultidimensional && channels === 1 ? [array] : array;
    for (let c = 0; c < channels; c++) {
      buffer.copyToChannel(multiChannelArray[c], c);
    }
    this._buffer = buffer;
    return this;
  }
  /**
   * Sums multiple channels into 1 channel
   * @param chanNum Optionally only copy a single channel from the array.
   */
  toMono(chanNum) {
    if (isNumber(chanNum)) {
      this.fromArray(this.toArray(chanNum));
    } else {
      let outputArray = new Float32Array(this.length);
      const numChannels = this.numberOfChannels;
      for (let channel = 0; channel < numChannels; channel++) {
        const channelArray = this.toArray(channel);
        for (let i = 0; i < channelArray.length; i++) {
          outputArray[i] += channelArray[i];
        }
      }
      outputArray = outputArray.map((sample) => sample / numChannels);
      this.fromArray(outputArray);
    }
    return this;
  }
  /**
   * Get the buffer as an array. Single channel buffers will return a 1-dimensional
   * Float32Array, and multichannel buffers will return multidimensional arrays.
   * @param channel Optionally only copy a single channel from the array.
   */
  toArray(channel) {
    if (isNumber(channel)) {
      return this.getChannelData(channel);
    } else if (this.numberOfChannels === 1) {
      return this.toArray(0);
    } else {
      const ret = [];
      for (let c = 0; c < this.numberOfChannels; c++) {
        ret[c] = this.getChannelData(c);
      }
      return ret;
    }
  }
  /**
   * Returns the Float32Array representing the PCM audio data for the specific channel.
   * @param  channel  The channel number to return
   * @return The audio as a TypedArray
   */
  getChannelData(channel) {
    if (this._buffer) {
      return this._buffer.getChannelData(channel);
    } else {
      return new Float32Array(0);
    }
  }
  /**
   * Cut a subsection of the array and return a buffer of the
   * subsection. Does not modify the original buffer
   * @param start The time to start the slice
   * @param end The end time to slice. If none is given will default to the end of the buffer
   */
  slice(start2, end = this.duration) {
    assert(this.loaded, "Buffer is not loaded");
    const startSamples = Math.floor(start2 * this.sampleRate);
    const endSamples = Math.floor(end * this.sampleRate);
    assert(startSamples < endSamples, "The start time must be less than the end time");
    const length = endSamples - startSamples;
    const retBuffer = getContext().createBuffer(this.numberOfChannels, length, this.sampleRate);
    for (let channel = 0; channel < this.numberOfChannels; channel++) {
      retBuffer.copyToChannel(this.getChannelData(channel).subarray(startSamples, endSamples), channel);
    }
    return new _ToneAudioBuffer(retBuffer);
  }
  /**
   * Reverse the buffer.
   */
  _reverse() {
    if (this.loaded) {
      for (let i = 0; i < this.numberOfChannels; i++) {
        this.getChannelData(i).reverse();
      }
    }
    return this;
  }
  /**
   * If the buffer is loaded or not
   */
  get loaded() {
    return this.length > 0;
  }
  /**
   * The duration of the buffer in seconds.
   */
  get duration() {
    if (this._buffer) {
      return this._buffer.duration;
    } else {
      return 0;
    }
  }
  /**
   * The length of the buffer in samples
   */
  get length() {
    if (this._buffer) {
      return this._buffer.length;
    } else {
      return 0;
    }
  }
  /**
   * The number of discrete audio channels. Returns 0 if no buffer is loaded.
   */
  get numberOfChannels() {
    if (this._buffer) {
      return this._buffer.numberOfChannels;
    } else {
      return 0;
    }
  }
  /**
   * Reverse the buffer.
   */
  get reverse() {
    return this._reversed;
  }
  set reverse(rev) {
    if (this._reversed !== rev) {
      this._reversed = rev;
      this._reverse();
    }
  }
  /**
   * Create a ToneAudioBuffer from the array. To create a multichannel AudioBuffer,
   * pass in a multidimensional array.
   * @param array The array to fill the audio buffer
   * @return A ToneAudioBuffer created from the array
   */
  static fromArray(array) {
    return new _ToneAudioBuffer().fromArray(array);
  }
  /**
   * Creates a ToneAudioBuffer from a URL, returns a promise which resolves to a ToneAudioBuffer
   * @param  url The url to load.
   * @return A promise which resolves to a ToneAudioBuffer
   */
  static fromUrl(url) {
    return __awaiter(this, void 0, void 0, function* () {
      const buffer = new _ToneAudioBuffer();
      return yield buffer.load(url);
    });
  }
  /**
   * Loads a url using fetch and returns the AudioBuffer.
   */
  static load(url) {
    return __awaiter(this, void 0, void 0, function* () {
      const baseUrl = _ToneAudioBuffer.baseUrl === "" || _ToneAudioBuffer.baseUrl.endsWith("/") ? _ToneAudioBuffer.baseUrl : _ToneAudioBuffer.baseUrl + "/";
      const response = yield fetch(baseUrl + url);
      if (!response.ok) {
        throw new Error(`could not load url: ${url}`);
      }
      const arrayBuffer = yield response.arrayBuffer();
      const audioBuffer = yield getContext().decodeAudioData(arrayBuffer);
      return audioBuffer;
    });
  }
  /**
   * Checks a url's extension to see if the current browser can play that file type.
   * @param url The url/extension to test
   * @return If the file extension can be played
   * @static
   * @example
   * Tone.ToneAudioBuffer.supportsType("wav"); // returns true
   * Tone.ToneAudioBuffer.supportsType("path/to/file.wav"); // returns true
   */
  static supportsType(url) {
    const extensions = url.split(".");
    const extension = extensions[extensions.length - 1];
    const response = document.createElement("audio").canPlayType("audio/" + extension);
    return response !== "";
  }
  /**
   * Returns a Promise which resolves when all of the buffers have loaded
   */
  static loaded() {
    return __awaiter(this, void 0, void 0, function* () {
      yield Promise.resolve();
      while (_ToneAudioBuffer.downloads.length) {
        yield _ToneAudioBuffer.downloads[0];
      }
    });
  }
};
ToneAudioBuffer.baseUrl = "";
ToneAudioBuffer.downloads = [];

// node_modules/tone/build/esm/core/context/OfflineContext.js
var OfflineContext = class extends Context {
  constructor() {
    super({
      clockSource: "offline",
      context: isOfflineAudioContext(arguments[0]) ? arguments[0] : createOfflineAudioContext(arguments[0], arguments[1] * arguments[2], arguments[2]),
      lookAhead: 0,
      updateInterval: isOfflineAudioContext(arguments[0]) ? 128 / arguments[0].sampleRate : 128 / arguments[2]
    });
    this.name = "OfflineContext";
    this._currentTime = 0;
    this.isOffline = true;
    this._duration = isOfflineAudioContext(arguments[0]) ? arguments[0].length / arguments[0].sampleRate : arguments[1];
  }
  /**
   * Override the now method to point to the internal clock time
   */
  now() {
    return this._currentTime;
  }
  /**
   * Same as this.now()
   */
  get currentTime() {
    return this._currentTime;
  }
  /**
   * Render just the clock portion of the audio context.
   */
  _renderClock(asynchronous) {
    return __awaiter(this, void 0, void 0, function* () {
      let index = 0;
      while (this._duration - this._currentTime >= 0) {
        this.emit("tick");
        this._currentTime += 128 / this.sampleRate;
        index++;
        const yieldEvery = Math.floor(this.sampleRate / 128);
        if (asynchronous && index % yieldEvery === 0) {
          yield new Promise((done) => setTimeout(done, 1));
        }
      }
    });
  }
  /**
   * Render the output of the OfflineContext
   * @param asynchronous If the clock should be rendered asynchronously, which will not block the main thread, but be slightly slower.
   */
  render() {
    return __awaiter(this, arguments, void 0, function* (asynchronous = true) {
      yield this.workletsAreReady();
      yield this._renderClock(asynchronous);
      const buffer = yield this._context.startRendering();
      return new ToneAudioBuffer(buffer);
    });
  }
  /**
   * Close the context
   */
  close() {
    return Promise.resolve();
  }
};

// node_modules/tone/build/esm/core/Global.js
var dummyContext = new DummyContext();
var globalContext = dummyContext;
function getContext() {
  if (globalContext === dummyContext && hasAudioContext) {
    setContext(new Context());
  }
  return globalContext;
}
function setContext(context2, disposeOld = false) {
  if (disposeOld) {
    globalContext.dispose();
  }
  if (isAudioContext(context2)) {
    globalContext = new Context(context2);
  } else if (isOfflineAudioContext(context2)) {
    globalContext = new OfflineContext(context2);
  } else {
    globalContext = context2;
  }
}
function start() {
  return globalContext.resume();
}
if (theWindow && !theWindow.TONE_SILENCE_LOGGING) {
  let prefix = "v";
  if (version === "dev") {
    prefix = "";
  }
  const printString = ` * Tone.js ${prefix}${version} * `;
  console.log(`%c${printString}`, "background: #000; color: #fff");
}

// node_modules/tone/build/esm/core/type/Conversions.js
function dbToGain(db) {
  return Math.pow(10, db / 20);
}
function gainToDb(gain) {
  return 20 * (Math.log(gain) / Math.LN10);
}
function intervalToFrequencyRatio(interval) {
  return Math.pow(2, interval / 12);
}
var A4 = 440;
function getA4() {
  return A4;
}
function setA4(freq) {
  A4 = freq;
}
function ftom(frequency) {
  return Math.round(ftomf(frequency));
}
function ftomf(frequency) {
  return 69 + 12 * Math.log2(frequency / A4);
}
function mtof(midi) {
  return A4 * Math.pow(2, (midi - 69) / 12);
}

// node_modules/tone/build/esm/core/type/TimeBase.js
var TimeBaseClass = class _TimeBaseClass extends Tone {
  /**
   * @param context The context associated with the time value. Used to compute
   * Transport and context-relative timing.
   * @param  value  The time value as a number, string or object
   * @param  units  Unit values
   */
  constructor(context2, value, units) {
    super();
    this.defaultUnits = "s";
    this._val = value;
    this._units = units;
    this.context = context2;
    this._expressions = this._getExpressions();
  }
  /**
   * All of the time encoding expressions
   */
  _getExpressions() {
    return {
      hz: {
        method: (value) => {
          return this._frequencyToUnits(parseFloat(value));
        },
        regexp: /^(\d+(?:\.\d+)?)hz$/i
      },
      i: {
        method: (value) => {
          return this._ticksToUnits(parseInt(value, 10));
        },
        regexp: /^(\d+)i$/i
      },
      m: {
        method: (value) => {
          return this._beatsToUnits(parseInt(value, 10) * this._getTimeSignature());
        },
        regexp: /^(\d+)m$/i
      },
      n: {
        method: (value, dot) => {
          const numericValue = parseInt(value, 10);
          const scalar = dot === "." ? 1.5 : 1;
          if (numericValue === 1) {
            return this._beatsToUnits(this._getTimeSignature()) * scalar;
          } else {
            return this._beatsToUnits(4 / numericValue) * scalar;
          }
        },
        regexp: /^(\d+)n(\.?)$/i
      },
      number: {
        method: (value) => {
          return this._expressions[this.defaultUnits].method.call(this, value);
        },
        regexp: /^(\d+(?:\.\d+)?)$/
      },
      s: {
        method: (value) => {
          return this._secondsToUnits(parseFloat(value));
        },
        regexp: /^(\d+(?:\.\d+)?)s$/
      },
      samples: {
        method: (value) => {
          return parseInt(value, 10) / this.context.sampleRate;
        },
        regexp: /^(\d+)samples$/
      },
      t: {
        method: (value) => {
          const numericValue = parseInt(value, 10);
          return this._beatsToUnits(8 / (Math.floor(numericValue) * 3));
        },
        regexp: /^(\d+)t$/i
      },
      tr: {
        method: (m, q, s) => {
          let total = 0;
          if (m && m !== "0") {
            total += this._beatsToUnits(this._getTimeSignature() * parseFloat(m));
          }
          if (q && q !== "0") {
            total += this._beatsToUnits(parseFloat(q));
          }
          if (s && s !== "0") {
            total += this._beatsToUnits(parseFloat(s) / 4);
          }
          return total;
        },
        regexp: /^(\d+(?:\.\d+)?):(\d+(?:\.\d+)?):?(\d+(?:\.\d+)?)?$/
      }
    };
  }
  //-------------------------------------
  // 	VALUE OF
  //-------------------------------------
  /**
   * Evaluate the time value. Returns the time in seconds.
   */
  valueOf() {
    if (this._val instanceof _TimeBaseClass) {
      this.fromType(this._val);
    }
    if (isUndef(this._val)) {
      return this._noArg();
    } else if (isString(this._val) && isUndef(this._units)) {
      for (const units in this._expressions) {
        if (this._expressions[units].regexp.test(this._val.trim())) {
          this._units = units;
          break;
        }
      }
    } else if (isObject(this._val)) {
      let total = 0;
      for (const typeName in this._val) {
        if (isDefined(this._val[typeName])) {
          const quantity = this._val[typeName];
          const time = (
            // @ts-ignore
            new this.constructor(this.context, typeName).valueOf() * quantity
          );
          total += time;
        }
      }
      return total;
    }
    if (isDefined(this._units)) {
      const expr = this._expressions[this._units];
      const matching = this._val.toString().trim().match(expr.regexp);
      if (matching) {
        return expr.method.apply(this, matching.slice(1));
      } else {
        return expr.method.call(this, this._val);
      }
    } else if (isString(this._val)) {
      return parseFloat(this._val);
    } else {
      return this._val;
    }
  }
  //-------------------------------------
  // 	UNIT CONVERSIONS
  //-------------------------------------
  /**
   * Returns the value of a frequency in the current units
   */
  _frequencyToUnits(freq) {
    return 1 / freq;
  }
  /**
   * Return the value of the beats in the current units
   */
  _beatsToUnits(beats) {
    return 60 / this._getBpm() * beats;
  }
  /**
   * Returns the value of a second in the current units
   */
  _secondsToUnits(seconds) {
    return seconds;
  }
  /**
   * Returns the value of a tick in the current time units
   */
  _ticksToUnits(ticks) {
    return ticks * this._beatsToUnits(1) / this._getPPQ();
  }
  /**
   * With no arguments, return 'now'
   */
  _noArg() {
    return this._now();
  }
  //-------------------------------------
  // 	TEMPO CONVERSIONS
  //-------------------------------------
  /**
   * Return the bpm
   */
  _getBpm() {
    return this.context.transport.bpm.value;
  }
  /**
   * Return the timeSignature
   */
  _getTimeSignature() {
    return this.context.transport.timeSignature;
  }
  /**
   * Return the PPQ or 192 if Transport is not available
   */
  _getPPQ() {
    return this.context.transport.PPQ;
  }
  //-------------------------------------
  // 	CONVERSION INTERFACE
  //-------------------------------------
  /**
   * Coerce a time type into this units type.
   * @param type Any time type units
   */
  fromType(type) {
    this._units = void 0;
    switch (this.defaultUnits) {
      case "s":
        this._val = type.toSeconds();
        break;
      case "i":
        this._val = type.toTicks();
        break;
      case "hz":
        this._val = type.toFrequency();
        break;
      case "midi":
        this._val = type.toMidi();
        break;
    }
    return this;
  }
  /**
   * Return the value in hertz
   */
  toFrequency() {
    return 1 / this.toSeconds();
  }
  /**
   * Return the time in samples
   */
  toSamples() {
    return this.toSeconds() * this.context.sampleRate;
  }
  /**
   * Return the time in milliseconds.
   */
  toMilliseconds() {
    return this.toSeconds() * 1e3;
  }
};

// node_modules/tone/build/esm/core/type/Time.js
var TimeClass = class _TimeClass extends TimeBaseClass {
  constructor() {
    super(...arguments);
    this.name = "TimeClass";
  }
  _getExpressions() {
    return Object.assign(super._getExpressions(), {
      now: {
        method: (capture) => {
          return this._now() + new this.constructor(this.context, capture).valueOf();
        },
        regexp: /^\+(.+)/
      },
      quantize: {
        method: (capture) => {
          const quantTo = new _TimeClass(this.context, capture).valueOf();
          return this._secondsToUnits(this.context.transport.nextSubdivision(quantTo));
        },
        regexp: /^@(.+)/
      }
    });
  }
  /**
   * Quantize the time by the given subdivision. Optionally add a
   * percentage which will move the time value towards the ideal
   * quantized value by that percentage.
   * @param  subdiv    The subdivision to quantize to
   * @param  percent  Move the time value towards the quantized value by a percentage.
   * @example
   * Tone.Time(21).quantize(2); // returns 22
   * Tone.Time(0.6).quantize("4n", 0.5); // returns 0.55
   */
  quantize(subdiv, percent = 1) {
    const subdivision = new this.constructor(this.context, subdiv).valueOf();
    const value = this.valueOf();
    const multiple = Math.round(value / subdivision);
    const ideal = multiple * subdivision;
    const diff = ideal - value;
    return value + diff * percent;
  }
  //-------------------------------------
  // CONVERSIONS
  //-------------------------------------
  /**
   * Convert a Time to Notation. The notation values are will be the
   * closest representation between 1m to 128th note.
   * @return {Notation}
   * @example
   * // if the Transport is at 120bpm:
   * Tone.Time(2).toNotation(); // returns "1m"
   */
  toNotation() {
    const time = this.toSeconds();
    const testNotations = ["1m"];
    for (let power = 1; power < 9; power++) {
      const subdiv = Math.pow(2, power);
      testNotations.push(subdiv + "n.");
      testNotations.push(subdiv + "n");
      testNotations.push(subdiv + "t");
    }
    testNotations.push("0");
    let closest = testNotations[0];
    let closestSeconds = new _TimeClass(this.context, testNotations[0]).toSeconds();
    testNotations.forEach((notation) => {
      const notationSeconds = new _TimeClass(this.context, notation).toSeconds();
      if (Math.abs(notationSeconds - time) < Math.abs(closestSeconds - time)) {
        closest = notation;
        closestSeconds = notationSeconds;
      }
    });
    return closest;
  }
  /**
   * Return the time encoded as Bars:Beats:Sixteenths.
   */
  toBarsBeatsSixteenths() {
    const quarterTime = this._beatsToUnits(1);
    let quarters = this.valueOf() / quarterTime;
    quarters = parseFloat(quarters.toFixed(4));
    const measures = Math.floor(quarters / this._getTimeSignature());
    let sixteenths = quarters % 1 * 4;
    quarters = Math.floor(quarters) % this._getTimeSignature();
    const sixteenthString = sixteenths.toString();
    if (sixteenthString.length > 3) {
      sixteenths = parseFloat(parseFloat(sixteenthString).toFixed(3));
    }
    const progress = [measures, quarters, sixteenths];
    return progress.join(":");
  }
  /**
   * Return the time in ticks.
   */
  toTicks() {
    const quarterTime = this._beatsToUnits(1);
    const quarters = this.valueOf() / quarterTime;
    return quarters * this._getPPQ();
  }
  /**
   * Return the time in seconds.
   */
  toSeconds() {
    return this.valueOf();
  }
  /**
   * Return the value as a midi note.
   */
  toMidi() {
    return ftom(this.toFrequency());
  }
  _now() {
    return this.context.now();
  }
};

// node_modules/tone/build/esm/core/type/Frequency.js
var FrequencyClass = class _FrequencyClass extends TimeClass {
  constructor() {
    super(...arguments);
    this.name = "Frequency";
    this.defaultUnits = "hz";
  }
  /**
   * The [concert tuning pitch](https://en.wikipedia.org/wiki/Concert_pitch) which is used
   * to generate all the other pitch values from notes. A4's values in Hertz.
   */
  static get A4() {
    return getA4();
  }
  static set A4(freq) {
    setA4(freq);
  }
  //-------------------------------------
  // 	AUGMENT BASE EXPRESSIONS
  //-------------------------------------
  _getExpressions() {
    return Object.assign({}, super._getExpressions(), {
      midi: {
        regexp: /^(\d+(?:\.\d+)?midi)/,
        method(value) {
          if (this.defaultUnits === "midi") {
            return value;
          } else {
            return _FrequencyClass.mtof(value);
          }
        }
      },
      note: {
        regexp: /^([a-g]{1}(?:b|#|##|x|bb|###|#x|x#|bbb)?)(-?[0-9]+)/i,
        method(pitch, octave) {
          const index = noteToScaleIndex[pitch.toLowerCase()];
          const noteNumber = index + (parseInt(octave, 10) + 1) * 12;
          if (this.defaultUnits === "midi") {
            return noteNumber;
          } else {
            return _FrequencyClass.mtof(noteNumber);
          }
        }
      },
      tr: {
        regexp: /^(\d+(?:\.\d+)?):(\d+(?:\.\d+)?):?(\d+(?:\.\d+)?)?/,
        method(m, q, s) {
          let total = 1;
          if (m && m !== "0") {
            total *= this._beatsToUnits(this._getTimeSignature() * parseFloat(m));
          }
          if (q && q !== "0") {
            total *= this._beatsToUnits(parseFloat(q));
          }
          if (s && s !== "0") {
            total *= this._beatsToUnits(parseFloat(s) / 4);
          }
          return total;
        }
      }
    });
  }
  //-------------------------------------
  // 	EXPRESSIONS
  //-------------------------------------
  /**
   * Transposes the frequency by the given number of semitones.
   * @return  A new transposed frequency
   * @example
   * Tone.Frequency("A4").transpose(3); // "C5"
   */
  transpose(interval) {
    return new _FrequencyClass(this.context, this.valueOf() * intervalToFrequencyRatio(interval));
  }
  /**
   * Takes an array of semitone intervals and returns
   * an array of frequencies transposed by those intervals.
   * @return  Returns an array of Frequencies
   * @example
   * Tone.Frequency("A4").harmonize([0, 3, 7]); // ["A4", "C5", "E5"]
   */
  harmonize(intervals) {
    return intervals.map((interval) => {
      return this.transpose(interval);
    });
  }
  //-------------------------------------
  // 	UNIT CONVERSIONS
  //-------------------------------------
  /**
   * Return the value of the frequency as a MIDI note
   * @example
   * Tone.Frequency("C4").toMidi(); // 60
   */
  toMidi() {
    return ftom(this.valueOf());
  }
  /**
   * Return the value of the frequency in Scientific Pitch Notation
   * @example
   * Tone.Frequency(69, "midi").toNote(); // "A4"
   */
  toNote() {
    const freq = this.toFrequency();
    const log2 = Math.log2(freq / _FrequencyClass.A4);
    let noteNumber = Math.round(12 * log2) + 57;
    const octave = Math.floor(noteNumber / 12);
    if (octave < 0) {
      noteNumber += -12 * octave;
    }
    const noteName = scaleIndexToNote[noteNumber % 12];
    return noteName + octave.toString();
  }
  /**
   * Return the duration of one cycle in seconds.
   */
  toSeconds() {
    return 1 / super.toSeconds();
  }
  /**
   * Return the duration of one cycle in ticks
   */
  toTicks() {
    const quarterTime = this._beatsToUnits(1);
    const quarters = this.valueOf() / quarterTime;
    return Math.floor(quarters * this._getPPQ());
  }
  //-------------------------------------
  // 	UNIT CONVERSIONS HELPERS
  //-------------------------------------
  /**
   * With no arguments, return 0
   */
  _noArg() {
    return 0;
  }
  /**
   * Returns the value of a frequency in the current units
   */
  _frequencyToUnits(freq) {
    return freq;
  }
  /**
   * Returns the value of a tick in the current time units
   */
  _ticksToUnits(ticks) {
    return 1 / (ticks * 60 / (this._getBpm() * this._getPPQ()));
  }
  /**
   * Return the value of the beats in the current units
   */
  _beatsToUnits(beats) {
    return 1 / super._beatsToUnits(beats);
  }
  /**
   * Returns the value of a second in the current units
   */
  _secondsToUnits(seconds) {
    return 1 / seconds;
  }
  /**
   * Convert a MIDI note to frequency value.
   * @param  midi The midi number to convert.
   * @return The corresponding frequency value
   */
  static mtof(midi) {
    return mtof(midi);
  }
  /**
   * Convert a frequency value to a MIDI note.
   * @param frequency The value to frequency value to convert.
   */
  static ftom(frequency) {
    return ftom(frequency);
  }
};
var noteToScaleIndex = {
  cbbb: -3,
  cbb: -2,
  cb: -1,
  c: 0,
  "c#": 1,
  cx: 2,
  "c##": 2,
  "c###": 3,
  "cx#": 3,
  "c#x": 3,
  dbbb: -1,
  dbb: 0,
  db: 1,
  d: 2,
  "d#": 3,
  dx: 4,
  "d##": 4,
  "d###": 5,
  "dx#": 5,
  "d#x": 5,
  ebbb: 1,
  ebb: 2,
  eb: 3,
  e: 4,
  "e#": 5,
  ex: 6,
  "e##": 6,
  "e###": 7,
  "ex#": 7,
  "e#x": 7,
  fbbb: 2,
  fbb: 3,
  fb: 4,
  f: 5,
  "f#": 6,
  fx: 7,
  "f##": 7,
  "f###": 8,
  "fx#": 8,
  "f#x": 8,
  gbbb: 4,
  gbb: 5,
  gb: 6,
  g: 7,
  "g#": 8,
  gx: 9,
  "g##": 9,
  "g###": 10,
  "gx#": 10,
  "g#x": 10,
  abbb: 6,
  abb: 7,
  ab: 8,
  a: 9,
  "a#": 10,
  ax: 11,
  "a##": 11,
  "a###": 12,
  "ax#": 12,
  "a#x": 12,
  bbbb: 8,
  bbb: 9,
  bb: 10,
  b: 11,
  "b#": 12,
  bx: 13,
  "b##": 13,
  "b###": 14,
  "bx#": 14,
  "b#x": 14
};
var scaleIndexToNote = [
  "C",
  "C#",
  "D",
  "D#",
  "E",
  "F",
  "F#",
  "G",
  "G#",
  "A",
  "A#",
  "B"
];

// node_modules/tone/build/esm/core/type/TransportTime.js
var TransportTimeClass = class extends TimeClass {
  constructor() {
    super(...arguments);
    this.name = "TransportTime";
  }
  /**
   * Return the current time in whichever context is relevant
   */
  _now() {
    return this.context.transport.seconds;
  }
};

// node_modules/tone/build/esm/core/context/ToneWithContext.js
var ToneWithContext = class _ToneWithContext extends Tone {
  constructor() {
    super();
    const options = optionsFromArguments(_ToneWithContext.getDefaults(), arguments, ["context"]);
    if (this.defaultContext) {
      this.context = this.defaultContext;
    } else {
      this.context = options.context;
    }
  }
  static getDefaults() {
    return {
      context: getContext()
    };
  }
  /**
   * Return the current time of the Context clock plus the lookAhead.
   * @example
   * setInterval(() => {
   * 	console.log(Tone.now());
   * }, 100);
   */
  now() {
    return this.context.currentTime + this.context.lookAhead;
  }
  /**
   * Return the current time of the Context clock without any lookAhead.
   * @example
   * setInterval(() => {
   * 	console.log(Tone.immediate());
   * }, 100);
   */
  immediate() {
    return this.context.currentTime;
  }
  /**
   * The duration in seconds of one sample.
   */
  get sampleTime() {
    return 1 / this.context.sampleRate;
  }
  /**
   * The number of seconds of 1 processing block (128 samples)
   * @example
   * console.log(Tone.Destination.blockTime);
   */
  get blockTime() {
    return 128 / this.context.sampleRate;
  }
  /**
   * Convert the incoming time to seconds.
   * This is calculated against the current {@link TransportClass} bpm
   * @example
   * const gain = new Tone.Gain();
   * setInterval(() => console.log(gain.toSeconds("4n")), 100);
   * // ramp the tempo to 60 bpm over 30 seconds
   * Tone.getTransport().bpm.rampTo(60, 30);
   */
  toSeconds(time) {
    assertUsedScheduleTime(time);
    return new TimeClass(this.context, time).toSeconds();
  }
  /**
   * Convert the input to a frequency number
   * @example
   * const gain = new Tone.Gain();
   * console.log(gain.toFrequency("4n"));
   */
  toFrequency(freq) {
    return new FrequencyClass(this.context, freq).toFrequency();
  }
  /**
   * Convert the input time into ticks
   * @example
   * const gain = new Tone.Gain();
   * console.log(gain.toTicks("4n"));
   */
  toTicks(time) {
    return new TransportTimeClass(this.context, time).toTicks();
  }
  //-------------------------------------
  // 	GET/SET
  //-------------------------------------
  /**
   * Get a subset of the properties which are in the partial props
   */
  _getPartialProperties(props) {
    const options = this.get();
    Object.keys(options).forEach((name) => {
      if (isUndef(props[name])) {
        delete options[name];
      }
    });
    return options;
  }
  /**
   * Get the object's attributes.
   * @example
   * const osc = new Tone.Oscillator();
   * console.log(osc.get());
   */
  get() {
    const defaults = getDefaultsFromInstance(this);
    Object.keys(defaults).forEach((attribute) => {
      if (Reflect.has(this, attribute)) {
        const member = this[attribute];
        if (isDefined(member) && isDefined(member.value) && isDefined(member.setValueAtTime)) {
          defaults[attribute] = member.value;
        } else if (member instanceof _ToneWithContext) {
          defaults[attribute] = member._getPartialProperties(defaults[attribute]);
        } else if (isArray(member) || isNumber(member) || isString(member) || isBoolean(member)) {
          defaults[attribute] = member;
        } else {
          delete defaults[attribute];
        }
      }
    });
    return defaults;
  }
  /**
   * Set multiple properties at once with an object.
   * @example
   * const filter = new Tone.Filter().toDestination();
   * // set values using an object
   * filter.set({
   * 	frequency: "C6",
   * 	type: "highpass"
   * });
   * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/Analogsynth_octaves_highmid.mp3").connect(filter);
   * player.autostart = true;
   */
  set(props) {
    Object.keys(props).forEach((attribute) => {
      if (Reflect.has(this, attribute) && isDefined(this[attribute])) {
        if (this[attribute] && isDefined(this[attribute].value) && isDefined(this[attribute].setValueAtTime)) {
          if (this[attribute].value !== props[attribute]) {
            this[attribute].value = props[attribute];
          }
        } else if (this[attribute] instanceof _ToneWithContext) {
          this[attribute].set(props[attribute]);
        } else {
          this[attribute] = props[attribute];
        }
      }
    });
    return this;
  }
};

// node_modules/tone/build/esm/core/util/StateTimeline.js
var StateTimeline = class extends Timeline {
  constructor(initial = "stopped") {
    super();
    this.name = "StateTimeline";
    this._initial = initial;
    this.setStateAtTime(this._initial, 0);
  }
  /**
   * Returns the scheduled state scheduled before or at
   * the given time.
   * @param  time  The time to query.
   * @return  The name of the state input in setStateAtTime.
   */
  getValueAtTime(time) {
    const event = this.get(time);
    if (event !== null) {
      return event.state;
    } else {
      return this._initial;
    }
  }
  /**
   * Add a state to the timeline.
   * @param  state The name of the state to set.
   * @param  time  The time to query.
   * @param options Any additional options that are needed in the timeline.
   */
  setStateAtTime(state, time, options) {
    assertRange(time, 0);
    this.add(Object.assign({}, options, {
      state,
      time
    }));
    return this;
  }
  /**
   * Return the event before the time with the given state
   * @param  state The state to look for
   * @param  time  When to check before
   * @return  The event with the given state before the time
   */
  getLastState(state, time) {
    const index = this._search(time);
    for (let i = index; i >= 0; i--) {
      const event = this._timeline[i];
      if (event.state === state) {
        return event;
      }
    }
  }
  /**
   * Return the event after the time with the given state
   * @param  state The state to look for
   * @param  time  When to check from
   * @return  The event with the given state after the time
   */
  getNextState(state, time) {
    const index = this._search(time);
    if (index !== -1) {
      for (let i = index; i < this._timeline.length; i++) {
        const event = this._timeline[i];
        if (event.state === state) {
          return event;
        }
      }
    }
  }
};

// node_modules/tone/build/esm/core/context/Param.js
var Param = class _Param extends ToneWithContext {
  constructor() {
    const options = optionsFromArguments(_Param.getDefaults(), arguments, [
      "param",
      "units",
      "convert"
    ]);
    super(options);
    this.name = "Param";
    this.overridden = false;
    this._minOutput = 1e-7;
    assert(isDefined(options.param) && (isAudioParam(options.param) || options.param instanceof _Param), "param must be an AudioParam");
    while (!isAudioParam(options.param)) {
      options.param = options.param._param;
    }
    this._swappable = isDefined(options.swappable) ? options.swappable : false;
    if (this._swappable) {
      this.input = this.context.createGain();
      this._param = options.param;
      this.input.connect(this._param);
    } else {
      this._param = this.input = options.param;
    }
    this._events = new Timeline(1e3);
    this._initialValue = this._param.defaultValue;
    this.units = options.units;
    this.convert = options.convert;
    this._minValue = options.minValue;
    this._maxValue = options.maxValue;
    if (isDefined(options.value) && options.value !== this._toType(this._initialValue)) {
      this.setValueAtTime(options.value, 0);
    }
  }
  static getDefaults() {
    return Object.assign(ToneWithContext.getDefaults(), {
      convert: true,
      units: "number"
    });
  }
  get value() {
    const now = this.now();
    return this.getValueAtTime(now);
  }
  set value(value) {
    this.cancelScheduledValues(this.now());
    this.setValueAtTime(value, this.now());
  }
  get minValue() {
    if (isDefined(this._minValue)) {
      return this._minValue;
    } else if (this.units === "time" || this.units === "frequency" || this.units === "normalRange" || this.units === "positive" || this.units === "transportTime" || this.units === "ticks" || this.units === "bpm" || this.units === "hertz" || this.units === "samples") {
      return 0;
    } else if (this.units === "audioRange") {
      return -1;
    } else if (this.units === "decibels") {
      return -Infinity;
    } else {
      return this._param.minValue;
    }
  }
  get maxValue() {
    if (isDefined(this._maxValue)) {
      return this._maxValue;
    } else if (this.units === "normalRange" || this.units === "audioRange") {
      return 1;
    } else {
      return this._param.maxValue;
    }
  }
  /**
   * Type guard based on the unit name
   */
  _is(arg, type) {
    return this.units === type;
  }
  /**
   * Make sure the value is always in the defined range
   */
  _assertRange(value) {
    if (isDefined(this.maxValue) && isDefined(this.minValue)) {
      assertRange(value, this._fromType(this.minValue), this._fromType(this.maxValue));
    }
    return value;
  }
  /**
   * Convert the given value from the type specified by Param.units
   * into the destination value (such as Gain or Frequency).
   */
  _fromType(val) {
    if (this.convert && !this.overridden) {
      if (this._is(val, "time")) {
        return this.toSeconds(val);
      } else if (this._is(val, "decibels")) {
        return dbToGain(val);
      } else if (this._is(val, "frequency")) {
        return this.toFrequency(val);
      } else {
        return val;
      }
    } else if (this.overridden) {
      return 0;
    } else {
      return val;
    }
  }
  /**
   * Convert the parameters value into the units specified by Param.units.
   */
  _toType(val) {
    if (this.convert && this.units === "decibels") {
      return gainToDb(val);
    } else {
      return val;
    }
  }
  //-------------------------------------
  // ABSTRACT PARAM INTERFACE
  // all docs are generated from ParamInterface.ts
  //-------------------------------------
  setValueAtTime(value, time) {
    const computedTime = this.toSeconds(time);
    const numericValue = this._fromType(value);
    assert(isFinite(numericValue) && isFinite(computedTime), `Invalid argument(s) to setValueAtTime: ${JSON.stringify(value)}, ${JSON.stringify(time)}`);
    this._assertRange(numericValue);
    this.log(this.units, "setValueAtTime", value, computedTime);
    this._events.add({
      time: computedTime,
      type: "setValueAtTime",
      value: numericValue
    });
    this._param.setValueAtTime(numericValue, computedTime);
    return this;
  }
  getValueAtTime(time) {
    const computedTime = Math.max(this.toSeconds(time), 0);
    const after = this._events.getAfter(computedTime);
    const before = this._events.get(computedTime);
    let value = this._initialValue;
    if (before === null) {
      value = this._initialValue;
    } else if (before.type === "setTargetAtTime" && (after === null || after.type === "setValueAtTime")) {
      const previous = this._events.getBefore(before.time);
      let previousVal;
      if (previous === null) {
        previousVal = this._initialValue;
      } else {
        previousVal = previous.value;
      }
      if (before.type === "setTargetAtTime") {
        value = this._exponentialApproach(before.time, previousVal, before.value, before.constant, computedTime);
      }
    } else if (after === null) {
      value = before.value;
    } else if (after.type === "linearRampToValueAtTime" || after.type === "exponentialRampToValueAtTime") {
      let beforeValue = before.value;
      if (before.type === "setTargetAtTime") {
        const previous = this._events.getBefore(before.time);
        if (previous === null) {
          beforeValue = this._initialValue;
        } else {
          beforeValue = previous.value;
        }
      }
      if (after.type === "linearRampToValueAtTime") {
        value = this._linearInterpolate(before.time, beforeValue, after.time, after.value, computedTime);
      } else {
        value = this._exponentialInterpolate(before.time, beforeValue, after.time, after.value, computedTime);
      }
    } else {
      value = before.value;
    }
    return this._toType(value);
  }
  setRampPoint(time) {
    time = this.toSeconds(time);
    let currentVal = this.getValueAtTime(time);
    this.cancelAndHoldAtTime(time);
    if (this._fromType(currentVal) === 0) {
      currentVal = this._toType(this._minOutput);
    }
    this.setValueAtTime(currentVal, time);
    return this;
  }
  linearRampToValueAtTime(value, endTime) {
    const numericValue = this._fromType(value);
    const computedTime = this.toSeconds(endTime);
    assert(isFinite(numericValue) && isFinite(computedTime), `Invalid argument(s) to linearRampToValueAtTime: ${JSON.stringify(value)}, ${JSON.stringify(endTime)}`);
    this._assertRange(numericValue);
    this._events.add({
      time: computedTime,
      type: "linearRampToValueAtTime",
      value: numericValue
    });
    this.log(this.units, "linearRampToValueAtTime", value, computedTime);
    this._param.linearRampToValueAtTime(numericValue, computedTime);
    return this;
  }
  exponentialRampToValueAtTime(value, endTime) {
    let numericValue = this._fromType(value);
    numericValue = EQ(numericValue, 0) ? this._minOutput : numericValue;
    this._assertRange(numericValue);
    const computedTime = this.toSeconds(endTime);
    assert(isFinite(numericValue) && isFinite(computedTime), `Invalid argument(s) to exponentialRampToValueAtTime: ${JSON.stringify(value)}, ${JSON.stringify(endTime)}`);
    this._events.add({
      time: computedTime,
      type: "exponentialRampToValueAtTime",
      value: numericValue
    });
    this.log(this.units, "exponentialRampToValueAtTime", value, computedTime);
    this._param.exponentialRampToValueAtTime(numericValue, computedTime);
    return this;
  }
  exponentialRampTo(value, rampTime, startTime) {
    startTime = this.toSeconds(startTime);
    this.setRampPoint(startTime);
    this.exponentialRampToValueAtTime(value, startTime + this.toSeconds(rampTime));
    return this;
  }
  linearRampTo(value, rampTime, startTime) {
    startTime = this.toSeconds(startTime);
    this.setRampPoint(startTime);
    this.linearRampToValueAtTime(value, startTime + this.toSeconds(rampTime));
    return this;
  }
  targetRampTo(value, rampTime, startTime) {
    startTime = this.toSeconds(startTime);
    this.setRampPoint(startTime);
    this.exponentialApproachValueAtTime(value, startTime, rampTime);
    return this;
  }
  exponentialApproachValueAtTime(value, time, rampTime) {
    time = this.toSeconds(time);
    rampTime = this.toSeconds(rampTime);
    const timeConstant = Math.log(rampTime + 1) / Math.log(200);
    this.setTargetAtTime(value, time, timeConstant);
    this.cancelAndHoldAtTime(time + rampTime * 0.9);
    this.linearRampToValueAtTime(value, time + rampTime);
    return this;
  }
  setTargetAtTime(value, startTime, timeConstant) {
    const numericValue = this._fromType(value);
    assert(isFinite(timeConstant) && timeConstant > 0, "timeConstant must be a number greater than 0");
    const computedTime = this.toSeconds(startTime);
    this._assertRange(numericValue);
    assert(isFinite(numericValue) && isFinite(computedTime), `Invalid argument(s) to setTargetAtTime: ${JSON.stringify(value)}, ${JSON.stringify(startTime)}`);
    this._events.add({
      constant: timeConstant,
      time: computedTime,
      type: "setTargetAtTime",
      value: numericValue
    });
    this.log(this.units, "setTargetAtTime", value, computedTime, timeConstant);
    this._param.setTargetAtTime(numericValue, computedTime, timeConstant);
    return this;
  }
  setValueCurveAtTime(values, startTime, duration, scaling = 1) {
    duration = this.toSeconds(duration);
    startTime = this.toSeconds(startTime);
    const startingValue = this._fromType(values[0]) * scaling;
    this.setValueAtTime(this._toType(startingValue), startTime);
    const segTime = duration / (values.length - 1);
    for (let i = 1; i < values.length; i++) {
      const numericValue = this._fromType(values[i]) * scaling;
      this.linearRampToValueAtTime(this._toType(numericValue), startTime + i * segTime);
    }
    return this;
  }
  cancelScheduledValues(time) {
    const computedTime = this.toSeconds(time);
    assert(isFinite(computedTime), `Invalid argument to cancelScheduledValues: ${JSON.stringify(time)}`);
    this._events.cancel(computedTime);
    this._param.cancelScheduledValues(computedTime);
    this.log(this.units, "cancelScheduledValues", computedTime);
    return this;
  }
  cancelAndHoldAtTime(time) {
    const computedTime = this.toSeconds(time);
    const valueAtTime = this._fromType(this.getValueAtTime(computedTime));
    assert(isFinite(computedTime), `Invalid argument to cancelAndHoldAtTime: ${JSON.stringify(time)}`);
    this.log(this.units, "cancelAndHoldAtTime", computedTime, "value=" + valueAtTime);
    const before = this._events.get(computedTime);
    const after = this._events.getAfter(computedTime);
    if (before && EQ(before.time, computedTime)) {
      if (after) {
        this._param.cancelScheduledValues(after.time);
        this._events.cancel(after.time);
      } else {
        this._param.cancelAndHoldAtTime(computedTime);
        this._events.cancel(computedTime + this.sampleTime);
      }
    } else if (after) {
      this._param.cancelScheduledValues(after.time);
      this._events.cancel(after.time);
      if (after.type === "linearRampToValueAtTime") {
        this.linearRampToValueAtTime(this._toType(valueAtTime), computedTime);
      } else if (after.type === "exponentialRampToValueAtTime") {
        this.exponentialRampToValueAtTime(this._toType(valueAtTime), computedTime);
      }
    }
    this._events.add({
      time: computedTime,
      type: "setValueAtTime",
      value: valueAtTime
    });
    this._param.setValueAtTime(valueAtTime, computedTime);
    return this;
  }
  rampTo(value, rampTime = 0.1, startTime) {
    if (this.units === "frequency" || this.units === "bpm" || this.units === "decibels") {
      this.exponentialRampTo(value, rampTime, startTime);
    } else {
      this.linearRampTo(value, rampTime, startTime);
    }
    return this;
  }
  /**
   * Apply all of the previously scheduled events to the passed in Param or AudioParam.
   * The applied values will start at the context's current time and schedule
   * all of the events which are scheduled on this Param onto the passed in param.
   */
  apply(param) {
    const now = this.context.currentTime;
    param.setValueAtTime(this.getValueAtTime(now), now);
    const previousEvent = this._events.get(now);
    if (previousEvent && previousEvent.type === "setTargetAtTime") {
      const nextEvent = this._events.getAfter(previousEvent.time);
      const endTime = nextEvent ? nextEvent.time : now + 2;
      const subdivisions = (endTime - now) / 10;
      for (let i = now; i < endTime; i += subdivisions) {
        param.linearRampToValueAtTime(this.getValueAtTime(i), i);
      }
    }
    this._events.forEachAfter(this.context.currentTime, (event) => {
      if (event.type === "cancelScheduledValues") {
        param.cancelScheduledValues(event.time);
      } else if (event.type === "setTargetAtTime") {
        param.setTargetAtTime(event.value, event.time, event.constant);
      } else {
        param[event.type](event.value, event.time);
      }
    });
    return this;
  }
  /**
   * Replace the Param's internal AudioParam. Will apply scheduled curves
   * onto the parameter and replace the connections.
   */
  setParam(param) {
    assert(this._swappable, "The Param must be assigned as 'swappable' in the constructor");
    const input = this.input;
    input.disconnect(this._param);
    this.apply(param);
    this._param = param;
    input.connect(this._param);
    return this;
  }
  dispose() {
    super.dispose();
    this._events.dispose();
    return this;
  }
  get defaultValue() {
    return this._toType(this._param.defaultValue);
  }
  //-------------------------------------
  // 	AUTOMATION CURVE CALCULATIONS
  // 	MIT License, copyright (c) 2014 Jordan Santell
  //-------------------------------------
  // Calculates the the value along the curve produced by setTargetAtTime
  _exponentialApproach(t0, v0, v1, timeConstant, t) {
    return v1 + (v0 - v1) * Math.exp(-(t - t0) / timeConstant);
  }
  // Calculates the the value along the curve produced by linearRampToValueAtTime
  _linearInterpolate(t0, v0, t1, v1, t) {
    return v0 + (v1 - v0) * ((t - t0) / (t1 - t0));
  }
  // Calculates the the value along the curve produced by exponentialRampToValueAtTime
  _exponentialInterpolate(t0, v0, t1, v1, t) {
    return v0 * Math.pow(v1 / v0, (t - t0) / (t1 - t0));
  }
};

// node_modules/tone/build/esm/core/context/ToneAudioNode.js
var ToneAudioNode = class _ToneAudioNode extends ToneWithContext {
  constructor() {
    super(...arguments);
    this._internalChannels = [];
  }
  /**
   * The number of inputs feeding into the AudioNode.
   * For source nodes, this will be 0.
   * @example
   * const node = new Tone.Gain();
   * console.log(node.numberOfInputs);
   */
  get numberOfInputs() {
    if (isDefined(this.input)) {
      if (isAudioParam(this.input) || this.input instanceof Param) {
        return 1;
      } else {
        return this.input.numberOfInputs;
      }
    } else {
      return 0;
    }
  }
  /**
   * The number of outputs of the AudioNode.
   * @example
   * const node = new Tone.Gain();
   * console.log(node.numberOfOutputs);
   */
  get numberOfOutputs() {
    if (isDefined(this.output)) {
      return this.output.numberOfOutputs;
    } else {
      return 0;
    }
  }
  //-------------------------------------
  // AUDIO PROPERTIES
  //-------------------------------------
  /**
   * Used to decide which nodes to get/set properties on
   */
  _isAudioNode(node) {
    return isDefined(node) && (node instanceof _ToneAudioNode || isAudioNode2(node));
  }
  /**
   * Get all of the audio nodes (either internal or input/output) which together
   * make up how the class node responds to channel input/output
   */
  _getInternalNodes() {
    const nodeList = this._internalChannels.slice(0);
    if (this._isAudioNode(this.input)) {
      nodeList.push(this.input);
    }
    if (this._isAudioNode(this.output)) {
      if (this.input !== this.output) {
        nodeList.push(this.output);
      }
    }
    return nodeList;
  }
  /**
   * Set the audio options for this node such as channelInterpretation
   * channelCount, etc.
   * @param options
   */
  _setChannelProperties(options) {
    const nodeList = this._getInternalNodes();
    nodeList.forEach((node) => {
      node.channelCount = options.channelCount;
      node.channelCountMode = options.channelCountMode;
      node.channelInterpretation = options.channelInterpretation;
    });
  }
  /**
   * Get the current audio options for this node such as channelInterpretation
   * channelCount, etc.
   */
  _getChannelProperties() {
    const nodeList = this._getInternalNodes();
    assert(nodeList.length > 0, "ToneAudioNode does not have any internal nodes");
    const node = nodeList[0];
    return {
      channelCount: node.channelCount,
      channelCountMode: node.channelCountMode,
      channelInterpretation: node.channelInterpretation
    };
  }
  /**
   * channelCount is the number of channels used when up-mixing and down-mixing
   * connections to any inputs to the node. The default value is 2 except for
   * specific nodes where its value is specially determined.
   */
  get channelCount() {
    return this._getChannelProperties().channelCount;
  }
  set channelCount(channelCount) {
    const props = this._getChannelProperties();
    this._setChannelProperties(Object.assign(props, { channelCount }));
  }
  /**
   * channelCountMode determines how channels will be counted when up-mixing and
   * down-mixing connections to any inputs to the node.
   * The default value is "max". This attribute has no effect for nodes with no inputs.
   * * "max" - computedNumberOfChannels is the maximum of the number of channels of all connections to an input. In this mode channelCount is ignored.
   * * "clamped-max" - computedNumberOfChannels is determined as for "max" and then clamped to a maximum value of the given channelCount.
   * * "explicit" - computedNumberOfChannels is the exact value as specified by the channelCount.
   */
  get channelCountMode() {
    return this._getChannelProperties().channelCountMode;
  }
  set channelCountMode(channelCountMode) {
    const props = this._getChannelProperties();
    this._setChannelProperties(Object.assign(props, { channelCountMode }));
  }
  /**
   * channelInterpretation determines how individual channels will be treated
   * when up-mixing and down-mixing connections to any inputs to the node.
   * The default value is "speakers".
   */
  get channelInterpretation() {
    return this._getChannelProperties().channelInterpretation;
  }
  set channelInterpretation(channelInterpretation) {
    const props = this._getChannelProperties();
    this._setChannelProperties(Object.assign(props, { channelInterpretation }));
  }
  //-------------------------------------
  // CONNECTIONS
  //-------------------------------------
  /**
   * connect the output of a ToneAudioNode to an AudioParam, AudioNode, or ToneAudioNode
   * @param destination The output to connect to
   * @param outputNum The output to connect from
   * @param inputNum The input to connect to
   */
  connect(destination, outputNum = 0, inputNum = 0) {
    connect(this, destination, outputNum, inputNum);
    return this;
  }
  /**
   * Connect the output to the context's destination node.
   * @example
   * const osc = new Tone.Oscillator("C2").start();
   * osc.toDestination();
   */
  toDestination() {
    this.connect(this.context.destination);
    return this;
  }
  /**
   * Connect the output to the context's destination node.
   * @see {@link toDestination}
   * @deprecated
   */
  toMaster() {
    warn("toMaster() has been renamed toDestination()");
    return this.toDestination();
  }
  /**
   * disconnect the output
   */
  disconnect(destination, outputNum = 0, inputNum = 0) {
    disconnect(this, destination, outputNum, inputNum);
    return this;
  }
  /**
   * Connect the output of this node to the rest of the nodes in series.
   * @example
   * const player = new Tone.Player("https://tonejs.github.io/audio/drum-samples/handdrum-loop.mp3");
   * player.autostart = true;
   * const filter = new Tone.AutoFilter(4).start();
   * const distortion = new Tone.Distortion(0.5);
   * // connect the player to the filter, distortion and then to the master output
   * player.chain(filter, distortion, Tone.Destination);
   */
  chain(...nodes) {
    connectSeries(this, ...nodes);
    return this;
  }
  /**
   * connect the output of this node to the rest of the nodes in parallel.
   * @example
   * const player = new Tone.Player("https://tonejs.github.io/audio/drum-samples/conga-rhythm.mp3");
   * player.autostart = true;
   * const pitchShift = new Tone.PitchShift(4).toDestination();
   * const filter = new Tone.Filter("G5").toDestination();
   * // connect a node to the pitch shift and filter in parallel
   * player.fan(pitchShift, filter);
   */
  fan(...nodes) {
    nodes.forEach((node) => this.connect(node));
    return this;
  }
  /**
   * Dispose and disconnect
   */
  dispose() {
    super.dispose();
    if (isDefined(this.input)) {
      if (this.input instanceof _ToneAudioNode) {
        this.input.dispose();
      } else if (isAudioNode2(this.input)) {
        this.input.disconnect();
      }
    }
    if (isDefined(this.output)) {
      if (this.output instanceof _ToneAudioNode) {
        this.output.dispose();
      } else if (isAudioNode2(this.output)) {
        this.output.disconnect();
      }
    }
    this._internalChannels = [];
    return this;
  }
};
function connectSeries(...nodes) {
  const first = nodes.shift();
  nodes.reduce((prev, current) => {
    if (prev instanceof ToneAudioNode) {
      prev.connect(current);
    } else if (isAudioNode2(prev)) {
      connect(prev, current);
    }
    return current;
  }, first);
}
function connect(srcNode, dstNode, outputNumber = 0, inputNumber = 0) {
  assert(isDefined(srcNode), "Cannot connect from undefined node");
  assert(isDefined(dstNode), "Cannot connect to undefined node");
  if (dstNode instanceof ToneAudioNode || isAudioNode2(dstNode)) {
    assert(dstNode.numberOfInputs > 0, "Cannot connect to node with no inputs");
  }
  assert(srcNode.numberOfOutputs > 0, "Cannot connect from node with no outputs");
  while (dstNode instanceof ToneAudioNode || dstNode instanceof Param) {
    if (isDefined(dstNode.input)) {
      dstNode = dstNode.input;
    }
  }
  while (srcNode instanceof ToneAudioNode) {
    if (isDefined(srcNode.output)) {
      srcNode = srcNode.output;
    }
  }
  if (isAudioParam(dstNode)) {
    srcNode.connect(dstNode, outputNumber);
  } else {
    srcNode.connect(dstNode, outputNumber, inputNumber);
  }
}
function disconnect(srcNode, dstNode, outputNumber = 0, inputNumber = 0) {
  if (isDefined(dstNode)) {
    while (dstNode instanceof ToneAudioNode) {
      dstNode = dstNode.input;
    }
  }
  while (!isAudioNode2(srcNode)) {
    if (isDefined(srcNode.output)) {
      srcNode = srcNode.output;
    }
  }
  if (isAudioParam(dstNode)) {
    srcNode.disconnect(dstNode, outputNumber);
  } else if (isAudioNode2(dstNode)) {
    srcNode.disconnect(dstNode, outputNumber, inputNumber);
  } else {
    srcNode.disconnect();
  }
}

// node_modules/tone/build/esm/core/context/Gain.js
var Gain = class _Gain extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Gain.getDefaults(), arguments, [
      "gain",
      "units"
    ]);
    super(options);
    this.name = "Gain";
    this._gainNode = this.context.createGain();
    this.input = this._gainNode;
    this.output = this._gainNode;
    this.gain = new Param({
      context: this.context,
      convert: options.convert,
      param: this._gainNode.gain,
      units: options.units,
      value: options.gain,
      minValue: options.minValue,
      maxValue: options.maxValue
    });
    readOnly(this, "gain");
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      convert: true,
      gain: 1,
      units: "gain"
    });
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._gainNode.disconnect();
    this.gain.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/OneShotSource.js
var OneShotSource = class extends ToneAudioNode {
  constructor(options) {
    super(options);
    this.onended = noOp;
    this._startTime = -1;
    this._stopTime = -1;
    this._timeout = -1;
    this.output = new Gain({
      context: this.context,
      gain: 0
    });
    this._gainNode = this.output;
    this.getStateAtTime = function(time) {
      const computedTime = this.toSeconds(time);
      if (this._startTime !== -1 && computedTime >= this._startTime && (this._stopTime === -1 || computedTime <= this._stopTime)) {
        return "started";
      } else {
        return "stopped";
      }
    };
    this._fadeIn = options.fadeIn;
    this._fadeOut = options.fadeOut;
    this._curve = options.curve;
    this.onended = options.onended;
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      curve: "linear",
      fadeIn: 0,
      fadeOut: 0,
      onended: noOp
    });
  }
  /**
   * Start the source at the given time
   * @param  time When to start the source
   */
  _startGain(time, gain = 1) {
    assert(this._startTime === -1, "Source cannot be started more than once");
    const fadeInTime = this.toSeconds(this._fadeIn);
    this._startTime = time + fadeInTime;
    this._startTime = Math.max(this._startTime, this.context.currentTime);
    if (fadeInTime > 0) {
      this._gainNode.gain.setValueAtTime(0, time);
      if (this._curve === "linear") {
        this._gainNode.gain.linearRampToValueAtTime(gain, time + fadeInTime);
      } else {
        this._gainNode.gain.exponentialApproachValueAtTime(gain, time, fadeInTime);
      }
    } else {
      this._gainNode.gain.setValueAtTime(gain, time);
    }
    return this;
  }
  /**
   * Stop the source node at the given time.
   * @param time When to stop the source
   */
  stop(time) {
    this.log("stop", time);
    this._stopGain(this.toSeconds(time));
    return this;
  }
  /**
   * Stop the source at the given time
   * @param  time When to stop the source
   */
  _stopGain(time) {
    assert(this._startTime !== -1, "'start' must be called before 'stop'");
    this.cancelStop();
    const fadeOutTime = this.toSeconds(this._fadeOut);
    this._stopTime = this.toSeconds(time) + fadeOutTime;
    this._stopTime = Math.max(this._stopTime, this.now());
    if (fadeOutTime > 0) {
      if (this._curve === "linear") {
        this._gainNode.gain.linearRampTo(0, fadeOutTime, time);
      } else {
        this._gainNode.gain.targetRampTo(0, fadeOutTime, time);
      }
    } else {
      this._gainNode.gain.cancelAndHoldAtTime(time);
      this._gainNode.gain.setValueAtTime(0, time);
    }
    this.context.clearTimeout(this._timeout);
    this._timeout = this.context.setTimeout(() => {
      const additionalTail = this._curve === "exponential" ? fadeOutTime * 2 : 0;
      this._stopSource(this.now() + additionalTail);
      this._onended();
    }, this._stopTime - this.context.currentTime);
    return this;
  }
  /**
   * Invoke the onended callback
   */
  _onended() {
    if (this.onended === noOp) {
      return;
    }
    this.onended(this);
    this.onended = noOp;
    if (!this.context.isOffline) {
      const disposeCallback = () => this.dispose();
      if (typeof requestIdleCallback !== "undefined") {
        requestIdleCallback(disposeCallback);
      } else {
        setTimeout(disposeCallback, 10);
      }
    }
  }
  /**
   * Get the playback state at the current time
   */
  get state() {
    return this.getStateAtTime(this.now());
  }
  /**
   * Cancel a scheduled stop event
   */
  cancelStop() {
    this.log("cancelStop");
    assert(this._startTime !== -1, "Source is not started");
    this._gainNode.gain.cancelScheduledValues(this._startTime + this.sampleTime);
    this.context.clearTimeout(this._timeout);
    this._stopTime = -1;
    return this;
  }
  dispose() {
    super.dispose();
    this._gainNode.dispose();
    this.onended = noOp;
    return this;
  }
};

// node_modules/tone/build/esm/signal/ToneConstantSource.js
var ToneConstantSource = class _ToneConstantSource extends OneShotSource {
  constructor() {
    const options = optionsFromArguments(_ToneConstantSource.getDefaults(), arguments, ["offset"]);
    super(options);
    this.name = "ToneConstantSource";
    this._source = this.context.createConstantSource();
    connect(this._source, this._gainNode);
    this.offset = new Param({
      context: this.context,
      convert: options.convert,
      param: this._source.offset,
      units: options.units,
      value: options.offset,
      minValue: options.minValue,
      maxValue: options.maxValue
    });
  }
  static getDefaults() {
    return Object.assign(OneShotSource.getDefaults(), {
      convert: true,
      offset: 1,
      units: "number"
    });
  }
  /**
   * Start the source node at the given time
   * @param  time When to start the source
   */
  start(time) {
    const computedTime = this.toSeconds(time);
    this.log("start", computedTime);
    this._startGain(computedTime);
    this._source.start(computedTime);
    return this;
  }
  _stopSource(time) {
    this._source.stop(time);
  }
  dispose() {
    super.dispose();
    if (this.state === "started") {
      this.stop();
    }
    this._source.disconnect();
    this.offset.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/signal/Signal.js
var Signal = class _Signal extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Signal.getDefaults(), arguments, [
      "value",
      "units"
    ]);
    super(options);
    this.name = "Signal";
    this.override = true;
    this.output = this._constantSource = new ToneConstantSource({
      context: this.context,
      convert: options.convert,
      offset: options.value,
      units: options.units,
      minValue: options.minValue,
      maxValue: options.maxValue
    });
    this._constantSource.start(0);
    this.input = this._param = this._constantSource.offset;
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      convert: true,
      units: "number",
      value: 0
    });
  }
  connect(destination, outputNum = 0, inputNum = 0) {
    connectSignal(this, destination, outputNum, inputNum);
    return this;
  }
  dispose() {
    super.dispose();
    this._param.dispose();
    this._constantSource.dispose();
    return this;
  }
  //-------------------------------------
  // ABSTRACT PARAM INTERFACE
  // just a proxy for the ConstantSourceNode's offset AudioParam
  // all docs are generated from AbstractParam.ts
  //-------------------------------------
  setValueAtTime(value, time) {
    this._param.setValueAtTime(value, time);
    return this;
  }
  getValueAtTime(time) {
    return this._param.getValueAtTime(time);
  }
  setRampPoint(time) {
    this._param.setRampPoint(time);
    return this;
  }
  linearRampToValueAtTime(value, time) {
    this._param.linearRampToValueAtTime(value, time);
    return this;
  }
  exponentialRampToValueAtTime(value, time) {
    this._param.exponentialRampToValueAtTime(value, time);
    return this;
  }
  exponentialRampTo(value, rampTime, startTime) {
    this._param.exponentialRampTo(value, rampTime, startTime);
    return this;
  }
  linearRampTo(value, rampTime, startTime) {
    this._param.linearRampTo(value, rampTime, startTime);
    return this;
  }
  targetRampTo(value, rampTime, startTime) {
    this._param.targetRampTo(value, rampTime, startTime);
    return this;
  }
  exponentialApproachValueAtTime(value, time, rampTime) {
    this._param.exponentialApproachValueAtTime(value, time, rampTime);
    return this;
  }
  setTargetAtTime(value, startTime, timeConstant) {
    this._param.setTargetAtTime(value, startTime, timeConstant);
    return this;
  }
  setValueCurveAtTime(values, startTime, duration, scaling) {
    this._param.setValueCurveAtTime(values, startTime, duration, scaling);
    return this;
  }
  cancelScheduledValues(time) {
    this._param.cancelScheduledValues(time);
    return this;
  }
  cancelAndHoldAtTime(time) {
    this._param.cancelAndHoldAtTime(time);
    return this;
  }
  rampTo(value, rampTime, startTime) {
    this._param.rampTo(value, rampTime, startTime);
    return this;
  }
  get value() {
    return this._param.value;
  }
  set value(value) {
    this._param.value = value;
  }
  get convert() {
    return this._param.convert;
  }
  set convert(convert) {
    this._param.convert = convert;
  }
  get units() {
    return this._param.units;
  }
  get overridden() {
    return this._param.overridden;
  }
  set overridden(overridden) {
    this._param.overridden = overridden;
  }
  get maxValue() {
    return this._param.maxValue;
  }
  get minValue() {
    return this._param.minValue;
  }
  /**
   * @see {@link Param.apply}.
   */
  apply(param) {
    this._param.apply(param);
    return this;
  }
};
function connectSignal(signal, destination, outputNum, inputNum) {
  if (destination instanceof Param || isAudioParam(destination) || destination instanceof Signal && destination.override) {
    destination.cancelScheduledValues(0);
    destination.setValueAtTime(0, 0);
    if (destination instanceof Signal) {
      destination.overridden = true;
    }
  }
  connect(signal, destination, outputNum, inputNum);
}

// node_modules/tone/build/esm/core/clock/TickParam.js
var TickParam = class _TickParam extends Param {
  constructor() {
    const options = optionsFromArguments(_TickParam.getDefaults(), arguments, ["value"]);
    super(options);
    this.name = "TickParam";
    this._events = new Timeline(Infinity);
    this._multiplier = 1;
    this._multiplier = options.multiplier;
    this._events.cancel(0);
    this._events.add({
      ticks: 0,
      time: 0,
      type: "setValueAtTime",
      value: this._fromType(options.value)
    });
    this.setValueAtTime(options.value, 0);
  }
  static getDefaults() {
    return Object.assign(Param.getDefaults(), {
      multiplier: 1,
      units: "hertz",
      value: 1
    });
  }
  setTargetAtTime(value, time, constant) {
    time = this.toSeconds(time);
    this.setRampPoint(time);
    const computedValue = this._fromType(value);
    const prevEvent = this._events.get(time);
    const segments = Math.round(Math.max(1 / constant, 1));
    for (let i = 0; i <= segments; i++) {
      const segTime = constant * i + time;
      const rampVal = this._exponentialApproach(prevEvent.time, prevEvent.value, computedValue, constant, segTime);
      this.linearRampToValueAtTime(this._toType(rampVal), segTime);
    }
    return this;
  }
  setValueAtTime(value, time) {
    const computedTime = this.toSeconds(time);
    super.setValueAtTime(value, time);
    const event = this._events.get(computedTime);
    const previousEvent = this._events.previousEvent(event);
    const ticksUntilTime = this._getTicksUntilEvent(previousEvent, computedTime);
    event.ticks = Math.max(ticksUntilTime, 0);
    return this;
  }
  linearRampToValueAtTime(value, time) {
    const computedTime = this.toSeconds(time);
    super.linearRampToValueAtTime(value, time);
    const event = this._events.get(computedTime);
    const previousEvent = this._events.previousEvent(event);
    const ticksUntilTime = this._getTicksUntilEvent(previousEvent, computedTime);
    event.ticks = Math.max(ticksUntilTime, 0);
    return this;
  }
  exponentialRampToValueAtTime(value, time) {
    time = this.toSeconds(time);
    const computedVal = this._fromType(value);
    const prevEvent = this._events.get(time);
    const segments = Math.round(Math.max((time - prevEvent.time) * 10, 1));
    const segmentDur = (time - prevEvent.time) / segments;
    for (let i = 0; i <= segments; i++) {
      const segTime = segmentDur * i + prevEvent.time;
      const rampVal = this._exponentialInterpolate(prevEvent.time, prevEvent.value, time, computedVal, segTime);
      this.linearRampToValueAtTime(this._toType(rampVal), segTime);
    }
    return this;
  }
  /**
   * Returns the tick value at the time. Takes into account
   * any automation curves scheduled on the signal.
   * @param  event The time to get the tick count at
   * @return The number of ticks which have elapsed at the time given any automations.
   */
  _getTicksUntilEvent(event, time) {
    if (event === null) {
      event = {
        ticks: 0,
        time: 0,
        type: "setValueAtTime",
        value: 0
      };
    } else if (isUndef(event.ticks)) {
      const previousEvent = this._events.previousEvent(event);
      event.ticks = this._getTicksUntilEvent(previousEvent, event.time);
    }
    const val0 = this._fromType(this.getValueAtTime(event.time));
    let val1 = this._fromType(this.getValueAtTime(time));
    const onTheLineEvent = this._events.get(time);
    if (onTheLineEvent && onTheLineEvent.time === time && onTheLineEvent.type === "setValueAtTime") {
      val1 = this._fromType(this.getValueAtTime(time - this.sampleTime));
    }
    return 0.5 * (time - event.time) * (val0 + val1) + event.ticks;
  }
  /**
   * Returns the tick value at the time. Takes into account
   * any automation curves scheduled on the signal.
   * @param  time The time to get the tick count at
   * @return The number of ticks which have elapsed at the time given any automations.
   */
  getTicksAtTime(time) {
    const computedTime = this.toSeconds(time);
    const event = this._events.get(computedTime);
    return Math.max(this._getTicksUntilEvent(event, computedTime), 0);
  }
  /**
   * Return the elapsed time of the number of ticks from the given time
   * @param ticks The number of ticks to calculate
   * @param  time The time to get the next tick from
   * @return The duration of the number of ticks from the given time in seconds
   */
  getDurationOfTicks(ticks, time) {
    const computedTime = this.toSeconds(time);
    const currentTick = this.getTicksAtTime(time);
    return this.getTimeOfTick(currentTick + ticks) - computedTime;
  }
  /**
   * Given a tick, returns the time that tick occurs at.
   * @return The time that the tick occurs.
   */
  getTimeOfTick(tick) {
    const before = this._events.get(tick, "ticks");
    const after = this._events.getAfter(tick, "ticks");
    if (before && before.ticks === tick) {
      return before.time;
    } else if (before && after && after.type === "linearRampToValueAtTime" && before.value !== after.value) {
      const val0 = this._fromType(this.getValueAtTime(before.time));
      const val1 = this._fromType(this.getValueAtTime(after.time));
      const delta = (val1 - val0) / (after.time - before.time);
      const k = Math.sqrt(Math.pow(val0, 2) - 2 * delta * (before.ticks - tick));
      const sol1 = (-val0 + k) / delta;
      const sol2 = (-val0 - k) / delta;
      return (sol1 > 0 ? sol1 : sol2) + before.time;
    } else if (before) {
      if (before.value === 0) {
        return Infinity;
      } else {
        return before.time + (tick - before.ticks) / before.value;
      }
    } else {
      return tick / this._initialValue;
    }
  }
  /**
   * Convert some number of ticks their the duration in seconds accounting
   * for any automation curves starting at the given time.
   * @param  ticks The number of ticks to convert to seconds.
   * @param  when  When along the automation timeline to convert the ticks.
   * @return The duration in seconds of the ticks.
   */
  ticksToTime(ticks, when) {
    return this.getDurationOfTicks(ticks, when);
  }
  /**
   * The inverse of {@link ticksToTime}. Convert a duration in
   * seconds to the corresponding number of ticks accounting for any
   * automation curves starting at the given time.
   * @param  duration The time interval to convert to ticks.
   * @param  when When along the automation timeline to convert the ticks.
   * @return The duration in ticks.
   */
  timeToTicks(duration, when) {
    const computedTime = this.toSeconds(when);
    const computedDuration = this.toSeconds(duration);
    const startTicks = this.getTicksAtTime(computedTime);
    const endTicks = this.getTicksAtTime(computedTime + computedDuration);
    return endTicks - startTicks;
  }
  /**
   * Convert from the type when the unit value is BPM
   */
  _fromType(val) {
    if (this.units === "bpm" && this.multiplier) {
      return 1 / (60 / val / this.multiplier);
    } else {
      return super._fromType(val);
    }
  }
  /**
   * Special case of type conversion where the units === "bpm"
   */
  _toType(val) {
    if (this.units === "bpm" && this.multiplier) {
      return val / this.multiplier * 60;
    } else {
      return super._toType(val);
    }
  }
  /**
   * A multiplier on the bpm value. Useful for setting a PPQ relative to the base frequency value.
   */
  get multiplier() {
    return this._multiplier;
  }
  set multiplier(m) {
    const currentVal = this.value;
    this._multiplier = m;
    this.cancelScheduledValues(0);
    this.setValueAtTime(currentVal, 0);
  }
};

// node_modules/tone/build/esm/core/clock/TickSignal.js
var TickSignal = class _TickSignal extends Signal {
  constructor() {
    const options = optionsFromArguments(_TickSignal.getDefaults(), arguments, ["value"]);
    super(options);
    this.name = "TickSignal";
    this.input = this._param = new TickParam({
      context: this.context,
      convert: options.convert,
      multiplier: options.multiplier,
      param: this._constantSource.offset,
      units: options.units,
      value: options.value
    });
  }
  static getDefaults() {
    return Object.assign(Signal.getDefaults(), {
      multiplier: 1,
      units: "hertz",
      value: 1
    });
  }
  ticksToTime(ticks, when) {
    return this._param.ticksToTime(ticks, when);
  }
  timeToTicks(duration, when) {
    return this._param.timeToTicks(duration, when);
  }
  getTimeOfTick(tick) {
    return this._param.getTimeOfTick(tick);
  }
  getDurationOfTicks(ticks, time) {
    return this._param.getDurationOfTicks(ticks, time);
  }
  getTicksAtTime(time) {
    return this._param.getTicksAtTime(time);
  }
  /**
   * A multiplier on the bpm value. Useful for setting a PPQ relative to the base frequency value.
   */
  get multiplier() {
    return this._param.multiplier;
  }
  set multiplier(m) {
    this._param.multiplier = m;
  }
  dispose() {
    super.dispose();
    this._param.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/core/clock/TickSource.js
var TickSource = class _TickSource extends ToneWithContext {
  constructor() {
    const options = optionsFromArguments(_TickSource.getDefaults(), arguments, ["frequency"]);
    super(options);
    this.name = "TickSource";
    this._state = new StateTimeline();
    this._tickOffset = new Timeline();
    this._ticksAtTime = new Timeline();
    this._secondsAtTime = new Timeline();
    this.frequency = new TickSignal({
      context: this.context,
      units: options.units,
      value: options.frequency
    });
    readOnly(this, "frequency");
    this._state.setStateAtTime("stopped", 0);
    this.setTicksAtTime(0, 0);
  }
  static getDefaults() {
    return Object.assign({
      frequency: 1,
      units: "hertz"
    }, ToneWithContext.getDefaults());
  }
  /**
   * Returns the playback state of the source, either "started", "stopped" or "paused".
   */
  get state() {
    return this.getStateAtTime(this.now());
  }
  /**
   * Start the clock at the given time. Optionally pass in an offset
   * of where to start the tick counter from.
   * @param  time    The time the clock should start
   * @param offset The number of ticks to start the source at
   */
  start(time, offset) {
    const computedTime = this.toSeconds(time);
    if (this._state.getValueAtTime(computedTime) !== "started") {
      this._state.setStateAtTime("started", computedTime);
      if (isDefined(offset)) {
        this.setTicksAtTime(offset, computedTime);
      }
      this._ticksAtTime.cancel(computedTime);
      this._secondsAtTime.cancel(computedTime);
    }
    return this;
  }
  /**
   * Stop the clock. Stopping the clock resets the tick counter to 0.
   * @param time The time when the clock should stop.
   */
  stop(time) {
    const computedTime = this.toSeconds(time);
    if (this._state.getValueAtTime(computedTime) === "stopped") {
      const event = this._state.get(computedTime);
      if (event && event.time > 0) {
        this._tickOffset.cancel(event.time);
        this._state.cancel(event.time);
      }
    }
    this._state.cancel(computedTime);
    this._state.setStateAtTime("stopped", computedTime);
    this.setTicksAtTime(0, computedTime);
    this._ticksAtTime.cancel(computedTime);
    this._secondsAtTime.cancel(computedTime);
    return this;
  }
  /**
   * Pause the clock. Pausing does not reset the tick counter.
   * @param time The time when the clock should stop.
   */
  pause(time) {
    const computedTime = this.toSeconds(time);
    if (this._state.getValueAtTime(computedTime) === "started") {
      this._state.setStateAtTime("paused", computedTime);
      this._ticksAtTime.cancel(computedTime);
      this._secondsAtTime.cancel(computedTime);
    }
    return this;
  }
  /**
   * Cancel start/stop/pause and setTickAtTime events scheduled after the given time.
   * @param time When to clear the events after
   */
  cancel(time) {
    time = this.toSeconds(time);
    this._state.cancel(time);
    this._tickOffset.cancel(time);
    this._ticksAtTime.cancel(time);
    this._secondsAtTime.cancel(time);
    return this;
  }
  /**
   * Get the elapsed ticks at the given time
   * @param  time  When to get the tick value
   * @return The number of ticks
   */
  getTicksAtTime(time) {
    const computedTime = this.toSeconds(time);
    const stopEvent = this._state.getLastState("stopped", computedTime);
    const memoizedEvent = this._ticksAtTime.get(computedTime);
    const tmpEvent = {
      state: "paused",
      time: computedTime
    };
    this._state.add(tmpEvent);
    let lastState = memoizedEvent ? memoizedEvent : stopEvent;
    let elapsedTicks = memoizedEvent ? memoizedEvent.ticks : 0;
    let eventToMemoize = null;
    this._state.forEachBetween(lastState.time, computedTime + this.sampleTime, (e) => {
      let periodStartTime = lastState.time;
      const offsetEvent = this._tickOffset.get(e.time);
      if (offsetEvent && offsetEvent.time >= lastState.time) {
        elapsedTicks = offsetEvent.ticks;
        periodStartTime = offsetEvent.time;
      }
      if (lastState.state === "started" && e.state !== "started") {
        elapsedTicks += this.frequency.getTicksAtTime(e.time) - this.frequency.getTicksAtTime(periodStartTime);
        if (e.time !== tmpEvent.time) {
          eventToMemoize = {
            state: e.state,
            time: e.time,
            ticks: elapsedTicks
          };
        }
      }
      lastState = e;
    });
    this._state.remove(tmpEvent);
    if (eventToMemoize) {
      this._ticksAtTime.add(eventToMemoize);
    }
    return elapsedTicks;
  }
  /**
   * The number of times the callback was invoked. Starts counting at 0
   * and increments after the callback was invoked. Returns -1 when stopped.
   */
  get ticks() {
    return this.getTicksAtTime(this.now());
  }
  set ticks(t) {
    this.setTicksAtTime(t, this.now());
  }
  /**
   * The time since ticks=0 that the TickSource has been running. Accounts
   * for tempo curves
   */
  get seconds() {
    return this.getSecondsAtTime(this.now());
  }
  set seconds(s) {
    const now = this.now();
    const ticks = this.frequency.timeToTicks(s, now);
    this.setTicksAtTime(ticks, now);
  }
  /**
   * Return the elapsed seconds at the given time.
   * @param  time  When to get the elapsed seconds
   * @return  The number of elapsed seconds
   */
  getSecondsAtTime(time) {
    time = this.toSeconds(time);
    const stopEvent = this._state.getLastState("stopped", time);
    const tmpEvent = { state: "paused", time };
    this._state.add(tmpEvent);
    const memoizedEvent = this._secondsAtTime.get(time);
    let lastState = memoizedEvent ? memoizedEvent : stopEvent;
    let elapsedSeconds = memoizedEvent ? memoizedEvent.seconds : 0;
    let eventToMemoize = null;
    this._state.forEachBetween(lastState.time, time + this.sampleTime, (e) => {
      let periodStartTime = lastState.time;
      const offsetEvent = this._tickOffset.get(e.time);
      if (offsetEvent && offsetEvent.time >= lastState.time) {
        elapsedSeconds = offsetEvent.seconds;
        periodStartTime = offsetEvent.time;
      }
      if (lastState.state === "started" && e.state !== "started") {
        elapsedSeconds += e.time - periodStartTime;
        if (e.time !== tmpEvent.time) {
          eventToMemoize = {
            state: e.state,
            time: e.time,
            seconds: elapsedSeconds
          };
        }
      }
      lastState = e;
    });
    this._state.remove(tmpEvent);
    if (eventToMemoize) {
      this._secondsAtTime.add(eventToMemoize);
    }
    return elapsedSeconds;
  }
  /**
   * Set the clock's ticks at the given time.
   * @param  ticks The tick value to set
   * @param  time  When to set the tick value
   */
  setTicksAtTime(ticks, time) {
    time = this.toSeconds(time);
    this._tickOffset.cancel(time);
    this._tickOffset.add({
      seconds: this.frequency.getDurationOfTicks(ticks, time),
      ticks,
      time
    });
    this._ticksAtTime.cancel(time);
    this._secondsAtTime.cancel(time);
    return this;
  }
  /**
   * Returns the scheduled state at the given time.
   * @param  time  The time to query.
   */
  getStateAtTime(time) {
    time = this.toSeconds(time);
    return this._state.getValueAtTime(time);
  }
  /**
   * Get the time of the given tick. The second argument
   * is when to test before. Since ticks can be set (with setTicksAtTime)
   * there may be multiple times for a given tick value.
   * @param  tick The tick number.
   * @param  before When to measure the tick value from.
   * @return The time of the tick
   */
  getTimeOfTick(tick, before = this.now()) {
    const offset = this._tickOffset.get(before);
    const event = this._state.get(before);
    const startTime = Math.max(offset.time, event.time);
    const absoluteTicks = this.frequency.getTicksAtTime(startTime) + tick - offset.ticks;
    return this.frequency.getTimeOfTick(absoluteTicks);
  }
  /**
   * Invoke the callback event at all scheduled ticks between the
   * start time and the end time
   * @param  startTime  The beginning of the search range
   * @param  endTime    The end of the search range
   * @param  callback   The callback to invoke with each tick
   */
  forEachTickBetween(startTime, endTime, callback) {
    let lastStateEvent = this._state.get(startTime);
    this._state.forEachBetween(startTime, endTime, (event) => {
      if (lastStateEvent && lastStateEvent.state === "started" && event.state !== "started") {
        this.forEachTickBetween(Math.max(lastStateEvent.time, startTime), event.time - this.sampleTime, callback);
      }
      lastStateEvent = event;
    });
    let error = null;
    if (lastStateEvent && lastStateEvent.state === "started") {
      const maxStartTime = Math.max(lastStateEvent.time, startTime);
      const startTicks = this.frequency.getTicksAtTime(maxStartTime);
      const ticksAtStart = this.frequency.getTicksAtTime(lastStateEvent.time);
      const diff = startTicks - ticksAtStart;
      let offset = Math.ceil(diff) - diff;
      offset = EQ(offset, 1) ? 0 : offset;
      let nextTickTime = this.frequency.getTimeOfTick(startTicks + offset);
      while (nextTickTime < endTime) {
        try {
          callback(nextTickTime, Math.round(this.getTicksAtTime(nextTickTime)));
        } catch (e) {
          error = e;
          break;
        }
        nextTickTime += this.frequency.getDurationOfTicks(1, nextTickTime);
      }
    }
    if (error) {
      throw error;
    }
    return this;
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    this._state.dispose();
    this._tickOffset.dispose();
    this._ticksAtTime.dispose();
    this._secondsAtTime.dispose();
    this.frequency.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/core/clock/Clock.js
var Clock = class _Clock extends ToneWithContext {
  constructor() {
    const options = optionsFromArguments(_Clock.getDefaults(), arguments, [
      "callback",
      "frequency"
    ]);
    super(options);
    this.name = "Clock";
    this.callback = noOp;
    this._lastUpdate = 0;
    this._state = new StateTimeline("stopped");
    this._boundLoop = this._loop.bind(this);
    this.callback = options.callback;
    this._tickSource = new TickSource({
      context: this.context,
      frequency: options.frequency,
      units: options.units
    });
    this._lastUpdate = 0;
    this.frequency = this._tickSource.frequency;
    readOnly(this, "frequency");
    this._state.setStateAtTime("stopped", 0);
    this.context.on("tick", this._boundLoop);
  }
  static getDefaults() {
    return Object.assign(ToneWithContext.getDefaults(), {
      callback: noOp,
      frequency: 1,
      units: "hertz"
    });
  }
  /**
   * Returns the playback state of the source, either "started", "stopped" or "paused".
   */
  get state() {
    return this._state.getValueAtTime(this.now());
  }
  /**
   * Start the clock at the given time. Optionally pass in an offset
   * of where to start the tick counter from.
   * @param  time    The time the clock should start
   * @param offset  Where the tick counter starts counting from.
   */
  start(time, offset) {
    assertContextRunning(this.context);
    const computedTime = this.toSeconds(time);
    this.log("start", computedTime);
    if (this._state.getValueAtTime(computedTime) !== "started") {
      this._state.setStateAtTime("started", computedTime);
      this._tickSource.start(computedTime, offset);
      if (computedTime < this._lastUpdate) {
        this.emit("start", computedTime, offset);
      }
    }
    return this;
  }
  /**
   * Stop the clock. Stopping the clock resets the tick counter to 0.
   * @param time The time when the clock should stop.
   * @example
   * const clock = new Tone.Clock(time => {
   * 	console.log(time);
   * }, 1);
   * clock.start();
   * // stop the clock after 10 seconds
   * clock.stop("+10");
   */
  stop(time) {
    const computedTime = this.toSeconds(time);
    this.log("stop", computedTime);
    this._state.cancel(computedTime);
    this._state.setStateAtTime("stopped", computedTime);
    this._tickSource.stop(computedTime);
    if (computedTime < this._lastUpdate) {
      this.emit("stop", computedTime);
    }
    return this;
  }
  /**
   * Pause the clock. Pausing does not reset the tick counter.
   * @param time The time when the clock should stop.
   */
  pause(time) {
    const computedTime = this.toSeconds(time);
    if (this._state.getValueAtTime(computedTime) === "started") {
      this._state.setStateAtTime("paused", computedTime);
      this._tickSource.pause(computedTime);
      if (computedTime < this._lastUpdate) {
        this.emit("pause", computedTime);
      }
    }
    return this;
  }
  /**
   * The number of times the callback was invoked. Starts counting at 0
   * and increments after the callback was invoked.
   */
  get ticks() {
    return Math.ceil(this.getTicksAtTime(this.now()));
  }
  set ticks(t) {
    this._tickSource.ticks = t;
  }
  /**
   * The time since ticks=0 that the Clock has been running. Accounts for tempo curves
   */
  get seconds() {
    return this._tickSource.seconds;
  }
  set seconds(s) {
    this._tickSource.seconds = s;
  }
  /**
   * Return the elapsed seconds at the given time.
   * @param  time  When to get the elapsed seconds
   * @return  The number of elapsed seconds
   */
  getSecondsAtTime(time) {
    return this._tickSource.getSecondsAtTime(time);
  }
  /**
   * Set the clock's ticks at the given time.
   * @param  ticks The tick value to set
   * @param  time  When to set the tick value
   */
  setTicksAtTime(ticks, time) {
    this._tickSource.setTicksAtTime(ticks, time);
    return this;
  }
  /**
   * Get the time of the given tick. The second argument
   * is when to test before. Since ticks can be set (with setTicksAtTime)
   * there may be multiple times for a given tick value.
   * @param  tick The tick number.
   * @param  before When to measure the tick value from.
   * @return The time of the tick
   */
  getTimeOfTick(tick, before = this.now()) {
    return this._tickSource.getTimeOfTick(tick, before);
  }
  /**
   * Get the clock's ticks at the given time.
   * @param  time  When to get the tick value
   * @return The tick value at the given time.
   */
  getTicksAtTime(time) {
    return this._tickSource.getTicksAtTime(time);
  }
  /**
   * Get the time of the next tick
   * @param  offset The tick number.
   */
  nextTickTime(offset, when) {
    const computedTime = this.toSeconds(when);
    const currentTick = this.getTicksAtTime(computedTime);
    return this._tickSource.getTimeOfTick(currentTick + offset, computedTime);
  }
  /**
   * The scheduling loop.
   */
  _loop() {
    const startTime = this._lastUpdate;
    const endTime = this.now();
    this._lastUpdate = endTime;
    this.log("loop", startTime, endTime);
    if (startTime !== endTime) {
      this._state.forEachBetween(startTime, endTime, (e) => {
        switch (e.state) {
          case "started":
            const offset = this._tickSource.getTicksAtTime(e.time);
            this.emit("start", e.time, offset);
            break;
          case "stopped":
            if (e.time !== 0) {
              this.emit("stop", e.time);
            }
            break;
          case "paused":
            this.emit("pause", e.time);
            break;
        }
      });
      this._tickSource.forEachTickBetween(startTime, endTime, (time, ticks) => {
        this.callback(time, ticks);
      });
    }
  }
  /**
   * Returns the scheduled state at the given time.
   * @param  time  The time to query.
   * @return  The name of the state input in setStateAtTime.
   * @example
   * const clock = new Tone.Clock();
   * clock.start("+0.1");
   * clock.getStateAtTime("+0.1"); // returns "started"
   */
  getStateAtTime(time) {
    const computedTime = this.toSeconds(time);
    return this._state.getValueAtTime(computedTime);
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    this.context.off("tick", this._boundLoop);
    this._tickSource.dispose();
    this._state.dispose();
    return this;
  }
};
Emitter.mixin(Clock);

// node_modules/tone/build/esm/core/context/Delay.js
var Delay = class _Delay extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Delay.getDefaults(), arguments, [
      "delayTime",
      "maxDelay"
    ]);
    super(options);
    this.name = "Delay";
    const maxDelayInSeconds = this.toSeconds(options.maxDelay);
    this._maxDelay = Math.max(maxDelayInSeconds, this.toSeconds(options.delayTime));
    this._delayNode = this.input = this.output = this.context.createDelay(maxDelayInSeconds);
    this.delayTime = new Param({
      context: this.context,
      param: this._delayNode.delayTime,
      units: "time",
      value: options.delayTime,
      minValue: 0,
      maxValue: this.maxDelay
    });
    readOnly(this, "delayTime");
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      delayTime: 0,
      maxDelay: 1
    });
  }
  /**
   * The maximum delay time. This cannot be changed after
   * the value is passed into the constructor.
   */
  get maxDelay() {
    return this._maxDelay;
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._delayNode.disconnect();
    this.delayTime.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/channel/Volume.js
var Volume = class _Volume extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Volume.getDefaults(), arguments, [
      "volume"
    ]);
    super(options);
    this.name = "Volume";
    this.input = this.output = new Gain({
      context: this.context,
      gain: options.volume,
      units: "decibels"
    });
    this.volume = this.output.gain;
    readOnly(this, "volume");
    this._unmutedVolume = options.volume;
    this.mute = options.mute;
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      mute: false,
      volume: 0
    });
  }
  /**
   * Mute the output.
   * @example
   * const vol = new Tone.Volume(-12).toDestination();
   * const osc = new Tone.Oscillator().connect(vol).start();
   * // mute the output
   * vol.mute = true;
   */
  get mute() {
    return this.volume.value === -Infinity;
  }
  set mute(mute) {
    if (!this.mute && mute) {
      this._unmutedVolume = this.volume.value;
      this.volume.value = -Infinity;
    } else if (this.mute && !mute) {
      this.volume.value = this._unmutedVolume;
    }
  }
  /**
   * clean up
   */
  dispose() {
    super.dispose();
    this.input.dispose();
    this.volume.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/core/context/Destination.js
var DestinationClass = class _DestinationClass extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_DestinationClass.getDefaults(), arguments);
    super(options);
    this.name = "Destination";
    this.input = new Volume({ context: this.context });
    this.output = new Gain({ context: this.context });
    this.volume = this.input.volume;
    connectSeries(this.input, this.output, this.context.rawContext.destination);
    this.mute = options.mute;
    this._internalChannels = [
      this.input,
      this.context.rawContext.destination,
      this.output
    ];
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      mute: false,
      volume: 0
    });
  }
  /**
   * Mute the output.
   * @example
   * const oscillator = new Tone.Oscillator().start().toDestination();
   * setTimeout(() => {
   * 	// mute the output
   * 	Tone.Destination.mute = true;
   * }, 1000);
   */
  get mute() {
    return this.input.mute;
  }
  set mute(mute) {
    this.input.mute = mute;
  }
  /**
   * Add a master effects chain. NOTE: this will disconnect any nodes which were previously
   * chained in the master effects chain.
   * @param args All arguments will be connected in a row and the Master will be routed through it.
   * @example
   * // route all audio through a filter and compressor
   * const lowpass = new Tone.Filter(800, "lowpass");
   * const compressor = new Tone.Compressor(-18);
   * Tone.Destination.chain(lowpass, compressor);
   */
  chain(...args) {
    this.input.disconnect();
    args.unshift(this.input);
    args.push(this.output);
    connectSeries(...args);
    return this;
  }
  /**
   * The maximum number of channels the system can output
   * @example
   * console.log(Tone.Destination.maxChannelCount);
   */
  get maxChannelCount() {
    return this.context.rawContext.destination.maxChannelCount;
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    this.volume.dispose();
    return this;
  }
};
onContextInit((context2) => {
  context2.destination = new DestinationClass({ context: context2 });
});
onContextClose((context2) => {
  context2.destination.dispose();
});

// node_modules/tone/build/esm/core/context/Listener.js
var ListenerClass = class extends ToneAudioNode {
  constructor() {
    super(...arguments);
    this.name = "Listener";
    this.positionX = new Param({
      context: this.context,
      param: this.context.rawContext.listener.positionX
    });
    this.positionY = new Param({
      context: this.context,
      param: this.context.rawContext.listener.positionY
    });
    this.positionZ = new Param({
      context: this.context,
      param: this.context.rawContext.listener.positionZ
    });
    this.forwardX = new Param({
      context: this.context,
      param: this.context.rawContext.listener.forwardX
    });
    this.forwardY = new Param({
      context: this.context,
      param: this.context.rawContext.listener.forwardY
    });
    this.forwardZ = new Param({
      context: this.context,
      param: this.context.rawContext.listener.forwardZ
    });
    this.upX = new Param({
      context: this.context,
      param: this.context.rawContext.listener.upX
    });
    this.upY = new Param({
      context: this.context,
      param: this.context.rawContext.listener.upY
    });
    this.upZ = new Param({
      context: this.context,
      param: this.context.rawContext.listener.upZ
    });
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      positionX: 0,
      positionY: 0,
      positionZ: 0,
      forwardX: 0,
      forwardY: 0,
      forwardZ: -1,
      upX: 0,
      upY: 1,
      upZ: 0
    });
  }
  dispose() {
    super.dispose();
    this.positionX.dispose();
    this.positionY.dispose();
    this.positionZ.dispose();
    this.forwardX.dispose();
    this.forwardY.dispose();
    this.forwardZ.dispose();
    this.upX.dispose();
    this.upY.dispose();
    this.upZ.dispose();
    return this;
  }
};
onContextInit((context2) => {
  context2.listener = new ListenerClass({ context: context2 });
});
onContextClose((context2) => {
  context2.listener.dispose();
});

// node_modules/tone/build/esm/core/context/Offline.js
function Offline(callback_1, duration_1) {
  return __awaiter(this, arguments, void 0, function* (callback, duration, channels = 2, sampleRate = getContext().sampleRate) {
    const originalContext = getContext();
    const context2 = new OfflineContext(channels, duration, sampleRate);
    setContext(context2);
    yield callback(context2);
    const bufferPromise = context2.render();
    setContext(originalContext);
    const buffer = yield bufferPromise;
    return new ToneAudioBuffer(buffer);
  });
}

// node_modules/tone/build/esm/core/context/ToneAudioBuffers.js
var ToneAudioBuffers = class _ToneAudioBuffers extends Tone {
  constructor() {
    super();
    this.name = "ToneAudioBuffers";
    this._buffers = /* @__PURE__ */ new Map();
    this._loadingCount = 0;
    const options = optionsFromArguments(_ToneAudioBuffers.getDefaults(), arguments, ["urls", "onload", "baseUrl"], "urls");
    this.baseUrl = options.baseUrl;
    Object.keys(options.urls).forEach((name) => {
      this._loadingCount++;
      const url = options.urls[name];
      this.add(name, url, this._bufferLoaded.bind(this, options.onload), options.onerror);
    });
  }
  static getDefaults() {
    return {
      baseUrl: "",
      onerror: noOp,
      onload: noOp,
      urls: {}
    };
  }
  /**
   * True if the buffers object has a buffer by that name.
   * @param  name  The key or index of the buffer.
   */
  has(name) {
    return this._buffers.has(name.toString());
  }
  /**
   * Get a buffer by name. If an array was loaded,
   * then use the array index.
   * @param  name  The key or index of the buffer.
   */
  get(name) {
    assert(this.has(name), `ToneAudioBuffers has no buffer named: ${name}`);
    return this._buffers.get(name.toString());
  }
  /**
   * A buffer was loaded. decrement the counter.
   */
  _bufferLoaded(callback) {
    this._loadingCount--;
    if (this._loadingCount === 0 && callback) {
      callback();
    }
  }
  /**
   * If the buffers are loaded or not
   */
  get loaded() {
    return Array.from(this._buffers).every(([_, buffer]) => buffer.loaded);
  }
  /**
   * Add a buffer by name and url to the Buffers
   * @param  name      A unique name to give the buffer
   * @param  url  Either the url of the bufer, or a buffer which will be added with the given name.
   * @param  callback  The callback to invoke when the url is loaded.
   * @param  onerror  Invoked if the buffer can't be loaded
   */
  add(name, url, callback = noOp, onerror = noOp) {
    if (isString(url)) {
      if (this.baseUrl && url.trim().substring(0, 11).toLowerCase() === "data:audio/") {
        this.baseUrl = "";
      }
      this._buffers.set(name.toString(), new ToneAudioBuffer(this.baseUrl + url, callback, onerror));
    } else {
      this._buffers.set(name.toString(), new ToneAudioBuffer(url, callback, onerror));
    }
    return this;
  }
  dispose() {
    super.dispose();
    this._buffers.forEach((buffer) => buffer.dispose());
    this._buffers.clear();
    return this;
  }
};

// node_modules/tone/build/esm/core/type/Midi.js
var MidiClass = class _MidiClass extends FrequencyClass {
  constructor() {
    super(...arguments);
    this.name = "MidiClass";
    this.defaultUnits = "midi";
  }
  /**
   * Returns the value of a frequency in the current units
   */
  _frequencyToUnits(freq) {
    return ftom(super._frequencyToUnits(freq));
  }
  /**
   * Returns the value of a tick in the current time units
   */
  _ticksToUnits(ticks) {
    return ftom(super._ticksToUnits(ticks));
  }
  /**
   * Return the value of the beats in the current units
   */
  _beatsToUnits(beats) {
    return ftom(super._beatsToUnits(beats));
  }
  /**
   * Returns the value of a second in the current units
   */
  _secondsToUnits(seconds) {
    return ftom(super._secondsToUnits(seconds));
  }
  /**
   * Return the value of the frequency as a MIDI note
   * @example
   * Tone.Midi(60).toMidi(); // 60
   */
  toMidi() {
    return this.valueOf();
  }
  /**
   * Return the value of the frequency as a MIDI note
   * @example
   * Tone.Midi(60).toFrequency(); // 261.6255653005986
   */
  toFrequency() {
    return mtof(this.toMidi());
  }
  /**
   * Transposes the frequency by the given number of semitones.
   * @return A new transposed MidiClass
   * @example
   * Tone.Midi("A4").transpose(3); // "C5"
   */
  transpose(interval) {
    return new _MidiClass(this.context, this.toMidi() + interval);
  }
};

// node_modules/tone/build/esm/core/type/Ticks.js
var TicksClass = class extends TransportTimeClass {
  constructor() {
    super(...arguments);
    this.name = "Ticks";
    this.defaultUnits = "i";
  }
  /**
   * Get the current time in the given units
   */
  _now() {
    return this.context.transport.ticks;
  }
  /**
   * Return the value of the beats in the current units
   */
  _beatsToUnits(beats) {
    return this._getPPQ() * beats;
  }
  /**
   * Returns the value of a second in the current units
   */
  _secondsToUnits(seconds) {
    return Math.floor(seconds / (60 / this._getBpm()) * this._getPPQ());
  }
  /**
   * Returns the value of a tick in the current time units
   */
  _ticksToUnits(ticks) {
    return ticks;
  }
  /**
   * Return the time in ticks
   */
  toTicks() {
    return this.valueOf();
  }
  /**
   * Return the time in seconds
   */
  toSeconds() {
    return this.valueOf() / this._getPPQ() * (60 / this._getBpm());
  }
};

// node_modules/tone/build/esm/core/util/Draw.js
var DrawClass = class extends ToneWithContext {
  constructor() {
    super(...arguments);
    this.name = "Draw";
    this.expiration = 0.25;
    this.anticipation = 8e-3;
    this._events = new Timeline();
    this._boundDrawLoop = this._drawLoop.bind(this);
    this._animationFrame = -1;
  }
  /**
   * Schedule a function at the given time to be invoked
   * on the nearest animation frame.
   * @param  callback  Callback is invoked at the given time.
   * @param  time      The time relative to the AudioContext time to invoke the callback.
   * @example
   * Tone.Transport.scheduleRepeat(time => {
   * 	Tone.Draw.schedule(() => console.log(time), time);
   * }, 1);
   * Tone.Transport.start();
   */
  schedule(callback, time) {
    this._events.add({
      callback,
      time: this.toSeconds(time)
    });
    if (this._events.length === 1) {
      this._animationFrame = requestAnimationFrame(this._boundDrawLoop);
    }
    return this;
  }
  /**
   * Cancel events scheduled after the given time
   * @param  after  Time after which scheduled events will be removed from the scheduling timeline.
   */
  cancel(after) {
    this._events.cancel(this.toSeconds(after));
    return this;
  }
  /**
   * The draw loop
   */
  _drawLoop() {
    const now = this.context.currentTime;
    this._events.forEachBefore(now + this.anticipation, (event) => {
      if (now - event.time <= this.expiration) {
        event.callback();
      }
      this._events.remove(event);
    });
    if (this._events.length > 0) {
      this._animationFrame = requestAnimationFrame(this._boundDrawLoop);
    }
  }
  dispose() {
    super.dispose();
    this._events.dispose();
    cancelAnimationFrame(this._animationFrame);
    return this;
  }
};
onContextInit((context2) => {
  context2.draw = new DrawClass({ context: context2 });
});
onContextClose((context2) => {
  context2.draw.dispose();
});

// node_modules/tone/build/esm/core/util/IntervalTimeline.js
var IntervalTimeline = class extends Tone {
  constructor() {
    super(...arguments);
    this.name = "IntervalTimeline";
    this._root = null;
    this._length = 0;
  }
  /**
   * The event to add to the timeline. All events must
   * have a time and duration value
   * @param  event  The event to add to the timeline
   */
  add(event) {
    assert(isDefined(event.time), "Events must have a time property");
    assert(isDefined(event.duration), "Events must have a duration parameter");
    event.time = event.time.valueOf();
    let node = new IntervalNode(event.time, event.time + event.duration, event);
    if (this._root === null) {
      this._root = node;
    } else {
      this._root.insert(node);
    }
    this._length++;
    while (node !== null) {
      node.updateHeight();
      node.updateMax();
      this._rebalance(node);
      node = node.parent;
    }
    return this;
  }
  /**
   * Remove an event from the timeline.
   * @param  event  The event to remove from the timeline
   */
  remove(event) {
    if (this._root !== null) {
      const results = [];
      this._root.search(event.time, results);
      for (const node of results) {
        if (node.event === event) {
          this._removeNode(node);
          this._length--;
          break;
        }
      }
    }
    return this;
  }
  /**
   * The number of items in the timeline.
   * @readOnly
   */
  get length() {
    return this._length;
  }
  /**
   * Remove events whose time time is after the given time
   * @param  after  The time to query.
   */
  cancel(after) {
    this.forEachFrom(after, (event) => this.remove(event));
    return this;
  }
  /**
   * Set the root node as the given node
   */
  _setRoot(node) {
    this._root = node;
    if (this._root !== null) {
      this._root.parent = null;
    }
  }
  /**
   * Replace the references to the node in the node's parent
   * with the replacement node.
   */
  _replaceNodeInParent(node, replacement) {
    if (node.parent !== null) {
      if (node.isLeftChild()) {
        node.parent.left = replacement;
      } else {
        node.parent.right = replacement;
      }
      this._rebalance(node.parent);
    } else {
      this._setRoot(replacement);
    }
  }
  /**
   * Remove the node from the tree and replace it with
   * a successor which follows the schema.
   */
  _removeNode(node) {
    if (node.left === null && node.right === null) {
      this._replaceNodeInParent(node, null);
    } else if (node.right === null) {
      this._replaceNodeInParent(node, node.left);
    } else if (node.left === null) {
      this._replaceNodeInParent(node, node.right);
    } else {
      const balance = node.getBalance();
      let replacement;
      let temp = null;
      if (balance > 0) {
        if (node.left.right === null) {
          replacement = node.left;
          replacement.right = node.right;
          temp = replacement;
        } else {
          replacement = node.left.right;
          while (replacement.right !== null) {
            replacement = replacement.right;
          }
          if (replacement.parent) {
            replacement.parent.right = replacement.left;
            temp = replacement.parent;
            replacement.left = node.left;
            replacement.right = node.right;
          }
        }
      } else if (node.right.left === null) {
        replacement = node.right;
        replacement.left = node.left;
        temp = replacement;
      } else {
        replacement = node.right.left;
        while (replacement.left !== null) {
          replacement = replacement.left;
        }
        if (replacement.parent) {
          replacement.parent.left = replacement.right;
          temp = replacement.parent;
          replacement.left = node.left;
          replacement.right = node.right;
        }
      }
      if (node.parent !== null) {
        if (node.isLeftChild()) {
          node.parent.left = replacement;
        } else {
          node.parent.right = replacement;
        }
      } else {
        this._setRoot(replacement);
      }
      if (temp) {
        this._rebalance(temp);
      }
    }
    node.dispose();
  }
  /**
   * Rotate the tree to the left
   */
  _rotateLeft(node) {
    const parent = node.parent;
    const isLeftChild = node.isLeftChild();
    const pivotNode = node.right;
    if (pivotNode) {
      node.right = pivotNode.left;
      pivotNode.left = node;
    }
    if (parent !== null) {
      if (isLeftChild) {
        parent.left = pivotNode;
      } else {
        parent.right = pivotNode;
      }
    } else {
      this._setRoot(pivotNode);
    }
  }
  /**
   * Rotate the tree to the right
   */
  _rotateRight(node) {
    const parent = node.parent;
    const isLeftChild = node.isLeftChild();
    const pivotNode = node.left;
    if (pivotNode) {
      node.left = pivotNode.right;
      pivotNode.right = node;
    }
    if (parent !== null) {
      if (isLeftChild) {
        parent.left = pivotNode;
      } else {
        parent.right = pivotNode;
      }
    } else {
      this._setRoot(pivotNode);
    }
  }
  /**
   * Balance the BST
   */
  _rebalance(node) {
    const balance = node.getBalance();
    if (balance > 1 && node.left) {
      if (node.left.getBalance() < 0) {
        this._rotateLeft(node.left);
      } else {
        this._rotateRight(node);
      }
    } else if (balance < -1 && node.right) {
      if (node.right.getBalance() > 0) {
        this._rotateRight(node.right);
      } else {
        this._rotateLeft(node);
      }
    }
  }
  /**
   * Get an event whose time and duration span the give time. Will
   * return the match whose "time" value is closest to the given time.
   * @return  The event which spans the desired time
   */
  get(time) {
    if (this._root !== null) {
      const results = [];
      this._root.search(time, results);
      if (results.length > 0) {
        let max = results[0];
        for (let i = 1; i < results.length; i++) {
          if (results[i].low > max.low) {
            max = results[i];
          }
        }
        return max.event;
      }
    }
    return null;
  }
  /**
   * Iterate over everything in the timeline.
   * @param  callback The callback to invoke with every item
   */
  forEach(callback) {
    if (this._root !== null) {
      const allNodes = [];
      this._root.traverse((node) => allNodes.push(node));
      allNodes.forEach((node) => {
        if (node.event) {
          callback(node.event);
        }
      });
    }
    return this;
  }
  /**
   * Iterate over everything in the array in which the given time
   * overlaps with the time and duration time of the event.
   * @param  time The time to check if items are overlapping
   * @param  callback The callback to invoke with every item
   */
  forEachAtTime(time, callback) {
    if (this._root !== null) {
      const results = [];
      this._root.search(time, results);
      results.forEach((node) => {
        if (node.event) {
          callback(node.event);
        }
      });
    }
    return this;
  }
  /**
   * Iterate over everything in the array in which the time is greater
   * than or equal to the given time.
   * @param  time The time to check if items are before
   * @param  callback The callback to invoke with every item
   */
  forEachFrom(time, callback) {
    if (this._root !== null) {
      const results = [];
      this._root.searchAfter(time, results);
      results.forEach((node) => {
        if (node.event) {
          callback(node.event);
        }
      });
    }
    return this;
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    if (this._root !== null) {
      this._root.traverse((node) => node.dispose());
    }
    this._root = null;
    return this;
  }
};
var IntervalNode = class {
  constructor(low, high, event) {
    this._left = null;
    this._right = null;
    this.parent = null;
    this.height = 0;
    this.event = event;
    this.low = low;
    this.high = high;
    this.max = this.high;
  }
  /**
   * Insert a node into the correct spot in the tree
   */
  insert(node) {
    if (node.low <= this.low) {
      if (this.left === null) {
        this.left = node;
      } else {
        this.left.insert(node);
      }
    } else if (this.right === null) {
      this.right = node;
    } else {
      this.right.insert(node);
    }
  }
  /**
   * Search the tree for nodes which overlap
   * with the given point
   * @param  point  The point to query
   * @param  results  The array to put the results
   */
  search(point, results) {
    if (point > this.max) {
      return;
    }
    if (this.left !== null) {
      this.left.search(point, results);
    }
    if (this.low <= point && this.high > point) {
      results.push(this);
    }
    if (this.low > point) {
      return;
    }
    if (this.right !== null) {
      this.right.search(point, results);
    }
  }
  /**
   * Search the tree for nodes which are less
   * than the given point
   * @param  point  The point to query
   * @param  results  The array to put the results
   */
  searchAfter(point, results) {
    if (this.low >= point) {
      results.push(this);
      if (this.left !== null) {
        this.left.searchAfter(point, results);
      }
    }
    if (this.right !== null) {
      this.right.searchAfter(point, results);
    }
  }
  /**
   * Invoke the callback on this element and both it's branches
   * @param  {Function}  callback
   */
  traverse(callback) {
    callback(this);
    if (this.left !== null) {
      this.left.traverse(callback);
    }
    if (this.right !== null) {
      this.right.traverse(callback);
    }
  }
  /**
   * Update the height of the node
   */
  updateHeight() {
    if (this.left !== null && this.right !== null) {
      this.height = Math.max(this.left.height, this.right.height) + 1;
    } else if (this.right !== null) {
      this.height = this.right.height + 1;
    } else if (this.left !== null) {
      this.height = this.left.height + 1;
    } else {
      this.height = 0;
    }
  }
  /**
   * Update the height of the node
   */
  updateMax() {
    this.max = this.high;
    if (this.left !== null) {
      this.max = Math.max(this.max, this.left.max);
    }
    if (this.right !== null) {
      this.max = Math.max(this.max, this.right.max);
    }
  }
  /**
   * The balance is how the leafs are distributed on the node
   * @return  Negative numbers are balanced to the right
   */
  getBalance() {
    let balance = 0;
    if (this.left !== null && this.right !== null) {
      balance = this.left.height - this.right.height;
    } else if (this.left !== null) {
      balance = this.left.height + 1;
    } else if (this.right !== null) {
      balance = -(this.right.height + 1);
    }
    return balance;
  }
  /**
   * @returns true if this node is the left child of its parent
   */
  isLeftChild() {
    return this.parent !== null && this.parent.left === this;
  }
  /**
   * get/set the left node
   */
  get left() {
    return this._left;
  }
  set left(node) {
    this._left = node;
    if (node !== null) {
      node.parent = this;
    }
    this.updateHeight();
    this.updateMax();
  }
  /**
   * get/set the right node
   */
  get right() {
    return this._right;
  }
  set right(node) {
    this._right = node;
    if (node !== null) {
      node.parent = this;
    }
    this.updateHeight();
    this.updateMax();
  }
  /**
   * null out references.
   */
  dispose() {
    this.parent = null;
    this._left = null;
    this._right = null;
    this.event = null;
  }
};

// node_modules/tone/build/esm/core/util/TimelineValue.js
var TimelineValue = class extends Tone {
  /**
   * @param initialValue The value to return if there is no scheduled values
   */
  constructor(initialValue) {
    super();
    this.name = "TimelineValue";
    this._timeline = new Timeline({
      memory: 10
    });
    this._initialValue = initialValue;
  }
  /**
   * Set the value at the given time
   */
  set(value, time) {
    this._timeline.add({
      value,
      time
    });
    return this;
  }
  /**
   * Get the value at the given time
   */
  get(time) {
    const event = this._timeline.get(time);
    if (event) {
      return event.value;
    } else {
      return this._initialValue;
    }
  }
};

// node_modules/tone/build/esm/signal/SignalOperator.js
var SignalOperator = class _SignalOperator extends ToneAudioNode {
  constructor() {
    super(optionsFromArguments(_SignalOperator.getDefaults(), arguments, [
      "context"
    ]));
  }
  connect(destination, outputNum = 0, inputNum = 0) {
    connectSignal(this, destination, outputNum, inputNum);
    return this;
  }
};

// node_modules/tone/build/esm/signal/WaveShaper.js
var WaveShaper = class _WaveShaper extends SignalOperator {
  constructor() {
    const options = optionsFromArguments(_WaveShaper.getDefaults(), arguments, ["mapping", "length"]);
    super(options);
    this.name = "WaveShaper";
    this._shaper = this.context.createWaveShaper();
    this.input = this._shaper;
    this.output = this._shaper;
    if (isArray(options.mapping) || options.mapping instanceof Float32Array) {
      this.curve = Float32Array.from(options.mapping);
    } else if (isFunction(options.mapping)) {
      this.setMap(options.mapping, options.length);
    }
  }
  static getDefaults() {
    return Object.assign(Signal.getDefaults(), {
      length: 1024
    });
  }
  /**
   * Uses a mapping function to set the value of the curve.
   * @param mapping The function used to define the values.
   *                The mapping function take two arguments:
   *                the first is the value at the current position
   *                which goes from -1 to 1 over the number of elements
   *                in the curve array. The second argument is the array position.
   * @example
   * const shaper = new Tone.WaveShaper();
   * // map the input signal from [-1, 1] to [0, 10]
   * shaper.setMap((val, index) => (val + 1) * 5);
   */
  setMap(mapping, length = 1024) {
    const array = new Float32Array(length);
    for (let i = 0, len = length; i < len; i++) {
      const normalized = i / (len - 1) * 2 - 1;
      array[i] = mapping(normalized, i);
    }
    this.curve = array;
    return this;
  }
  /**
   * The array to set as the waveshaper curve. For linear curves
   * array length does not make much difference, but for complex curves
   * longer arrays will provide smoother interpolation.
   */
  get curve() {
    return this._shaper.curve;
  }
  set curve(mapping) {
    this._shaper.curve = mapping;
  }
  /**
   * Specifies what type of oversampling (if any) should be used when
   * applying the shaping curve. Can either be "none", "2x" or "4x".
   */
  get oversample() {
    return this._shaper.oversample;
  }
  set oversample(oversampling) {
    const isOverSampleType = ["none", "2x", "4x"].some((str) => str.includes(oversampling));
    assert(isOverSampleType, "oversampling must be either 'none', '2x', or '4x'");
    this._shaper.oversample = oversampling;
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._shaper.disconnect();
    return this;
  }
};

// node_modules/tone/build/esm/signal/Pow.js
var Pow = class _Pow extends SignalOperator {
  constructor() {
    const options = optionsFromArguments(_Pow.getDefaults(), arguments, [
      "value"
    ]);
    super(options);
    this.name = "Pow";
    this._exponentScaler = this.input = this.output = new WaveShaper({
      context: this.context,
      mapping: this._expFunc(options.value),
      length: 8192
    });
    this._exponent = options.value;
  }
  static getDefaults() {
    return Object.assign(SignalOperator.getDefaults(), {
      value: 1
    });
  }
  /**
   * the function which maps the waveshaper
   * @param exponent exponent value
   */
  _expFunc(exponent) {
    return (val) => {
      return Math.pow(Math.abs(val), exponent);
    };
  }
  /**
   * The value of the exponent.
   */
  get value() {
    return this._exponent;
  }
  set value(exponent) {
    this._exponent = exponent;
    this._exponentScaler.setMap(this._expFunc(this._exponent));
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._exponentScaler.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/core/clock/TransportEvent.js
var TransportEvent = class _TransportEvent {
  /**
   * @param transport The transport object which the event belongs to
   */
  constructor(transport, opts) {
    this.id = _TransportEvent._eventId++;
    this._remainderTime = 0;
    const options = Object.assign(_TransportEvent.getDefaults(), opts);
    this.transport = transport;
    this.callback = options.callback;
    this._once = options.once;
    this.time = Math.floor(options.time);
    this._remainderTime = options.time - this.time;
  }
  static getDefaults() {
    return {
      callback: noOp,
      once: false,
      time: 0
    };
  }
  /**
   * Get the time and remainder time.
   */
  get floatTime() {
    return this.time + this._remainderTime;
  }
  /**
   * Invoke the event callback.
   * @param  time  The AudioContext time in seconds of the event
   */
  invoke(time) {
    if (this.callback) {
      const tickDuration = this.transport.bpm.getDurationOfTicks(1, time);
      this.callback(time + this._remainderTime * tickDuration);
      if (this._once) {
        this.transport.clear(this.id);
      }
    }
  }
  /**
   * Clean up
   */
  dispose() {
    this.callback = void 0;
    return this;
  }
};
TransportEvent._eventId = 0;

// node_modules/tone/build/esm/core/clock/TransportRepeatEvent.js
var TransportRepeatEvent = class _TransportRepeatEvent extends TransportEvent {
  /**
   * @param transport The transport object which the event belongs to
   */
  constructor(transport, opts) {
    super(transport, opts);
    this._currentId = -1;
    this._nextId = -1;
    this._nextTick = this.time;
    this._boundRestart = this._restart.bind(this);
    const options = Object.assign(_TransportRepeatEvent.getDefaults(), opts);
    this.duration = options.duration;
    this._interval = options.interval;
    this._nextTick = options.time;
    this.transport.on("start", this._boundRestart);
    this.transport.on("loopStart", this._boundRestart);
    this.transport.on("ticks", this._boundRestart);
    this.context = this.transport.context;
    this._restart();
  }
  static getDefaults() {
    return Object.assign({}, TransportEvent.getDefaults(), {
      duration: Infinity,
      interval: 1,
      once: false
    });
  }
  /**
   * Invoke the callback. Returns the tick time which
   * the next event should be scheduled at.
   * @param  time  The AudioContext time in seconds of the event
   */
  invoke(time) {
    this._createEvents(time);
    super.invoke(time);
  }
  /**
   * Create an event on the transport on the nextTick
   */
  _createEvent() {
    if (LT(this._nextTick, this.floatTime + this.duration)) {
      return this.transport.scheduleOnce(this.invoke.bind(this), new TicksClass(this.context, this._nextTick).toSeconds());
    }
    return -1;
  }
  /**
   * Push more events onto the timeline to keep up with the position of the timeline
   */
  _createEvents(time) {
    if (LT(this._nextTick + this._interval, this.floatTime + this.duration)) {
      this._nextTick += this._interval;
      this._currentId = this._nextId;
      this._nextId = this.transport.scheduleOnce(this.invoke.bind(this), new TicksClass(this.context, this._nextTick).toSeconds());
    }
  }
  /**
   * Re-compute the events when the transport time has changed from a start/ticks/loopStart event
   */
  _restart(time) {
    this.transport.clear(this._currentId);
    this.transport.clear(this._nextId);
    this._nextTick = this.floatTime;
    const ticks = this.transport.getTicksAtTime(time);
    if (GT(ticks, this.time)) {
      this._nextTick = this.floatTime + Math.ceil((ticks - this.floatTime) / this._interval) * this._interval;
    }
    this._currentId = this._createEvent();
    this._nextTick += this._interval;
    this._nextId = this._createEvent();
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    this.transport.clear(this._currentId);
    this.transport.clear(this._nextId);
    this.transport.off("start", this._boundRestart);
    this.transport.off("loopStart", this._boundRestart);
    this.transport.off("ticks", this._boundRestart);
    return this;
  }
};

// node_modules/tone/build/esm/core/clock/Transport.js
var TransportClass = class _TransportClass extends ToneWithContext {
  constructor() {
    const options = optionsFromArguments(_TransportClass.getDefaults(), arguments);
    super(options);
    this.name = "Transport";
    this._loop = new TimelineValue(false);
    this._loopStart = 0;
    this._loopEnd = 0;
    this._scheduledEvents = {};
    this._timeline = new Timeline();
    this._repeatedEvents = new IntervalTimeline();
    this._syncedSignals = [];
    this._swingAmount = 0;
    this._ppq = options.ppq;
    this._clock = new Clock({
      callback: this._processTick.bind(this),
      context: this.context,
      frequency: 0,
      units: "bpm"
    });
    this._bindClockEvents();
    this.bpm = this._clock.frequency;
    this._clock.frequency.multiplier = options.ppq;
    this.bpm.setValueAtTime(options.bpm, 0);
    readOnly(this, "bpm");
    this._timeSignature = options.timeSignature;
    this._swingTicks = options.ppq / 2;
  }
  static getDefaults() {
    return Object.assign(ToneWithContext.getDefaults(), {
      bpm: 120,
      loopEnd: "4m",
      loopStart: 0,
      ppq: 192,
      swing: 0,
      swingSubdivision: "8n",
      timeSignature: 4
    });
  }
  //-------------------------------------
  // 	TICKS
  //-------------------------------------
  /**
   * called on every tick
   * @param  tickTime clock relative tick time
   */
  _processTick(tickTime, ticks) {
    if (this._loop.get(tickTime)) {
      if (ticks >= this._loopEnd) {
        this.emit("loopEnd", tickTime);
        this._clock.setTicksAtTime(this._loopStart, tickTime);
        ticks = this._loopStart;
        this.emit("loopStart", tickTime, this._clock.getSecondsAtTime(tickTime));
        this.emit("loop", tickTime);
      }
    }
    if (this._swingAmount > 0 && ticks % this._ppq !== 0 && // not on a downbeat
    ticks % (this._swingTicks * 2) !== 0) {
      const progress = ticks % (this._swingTicks * 2) / (this._swingTicks * 2);
      const amount = Math.sin(progress * Math.PI) * this._swingAmount;
      tickTime += new TicksClass(this.context, this._swingTicks * 2 / 3).toSeconds() * amount;
    }
    enterScheduledCallback(true);
    this._timeline.forEachAtTime(ticks, (event) => event.invoke(tickTime));
    enterScheduledCallback(false);
  }
  //-------------------------------------
  // 	SCHEDULABLE EVENTS
  //-------------------------------------
  /**
   * Schedule an event along the timeline.
   * @param callback The callback to be invoked at the time.
   * @param time The time to invoke the callback at.
   * @return The id of the event which can be used for canceling the event.
   * @example
   * // schedule an event on the 16th measure
   * Tone.getTransport().schedule((time) => {
   * 	// invoked on measure 16
   * 	console.log("measure 16!");
   * }, "16:0:0");
   */
  schedule(callback, time) {
    const event = new TransportEvent(this, {
      callback,
      time: new TransportTimeClass(this.context, time).toTicks()
    });
    return this._addEvent(event, this._timeline);
  }
  /**
   * Schedule a repeated event along the timeline. The event will fire
   * at the `interval` starting at the `startTime` and for the specified
   * `duration`.
   * @param  callback   The callback to invoke.
   * @param  interval   The duration between successive callbacks. Must be a positive number.
   * @param  startTime  When along the timeline the events should start being invoked.
   * @param  duration How long the event should repeat.
   * @return  The ID of the scheduled event. Use this to cancel the event.
   * @example
   * const osc = new Tone.Oscillator().toDestination().start();
   * // a callback invoked every eighth note after the first measure
   * Tone.getTransport().scheduleRepeat((time) => {
   * 	osc.start(time).stop(time + 0.1);
   * }, "8n", "1m");
   */
  scheduleRepeat(callback, interval, startTime, duration = Infinity) {
    const event = new TransportRepeatEvent(this, {
      callback,
      duration: new TimeClass(this.context, duration).toTicks(),
      interval: new TimeClass(this.context, interval).toTicks(),
      time: new TransportTimeClass(this.context, startTime).toTicks()
    });
    return this._addEvent(event, this._repeatedEvents);
  }
  /**
   * Schedule an event that will be removed after it is invoked.
   * @param callback The callback to invoke once.
   * @param time The time the callback should be invoked.
   * @returns The ID of the scheduled event.
   */
  scheduleOnce(callback, time) {
    const event = new TransportEvent(this, {
      callback,
      once: true,
      time: new TransportTimeClass(this.context, time).toTicks()
    });
    return this._addEvent(event, this._timeline);
  }
  /**
   * Clear the passed in event id from the timeline
   * @param eventId The id of the event.
   */
  clear(eventId) {
    if (this._scheduledEvents.hasOwnProperty(eventId)) {
      const item = this._scheduledEvents[eventId.toString()];
      item.timeline.remove(item.event);
      item.event.dispose();
      delete this._scheduledEvents[eventId.toString()];
    }
    return this;
  }
  /**
   * Add an event to the correct timeline. Keep track of the
   * timeline it was added to.
   * @returns the event id which was just added
   */
  _addEvent(event, timeline) {
    this._scheduledEvents[event.id.toString()] = {
      event,
      timeline
    };
    timeline.add(event);
    return event.id;
  }
  /**
   * Remove scheduled events from the timeline after
   * the given time. Repeated events will be removed
   * if their startTime is after the given time
   * @param after Clear all events after this time.
   */
  cancel(after = 0) {
    const computedAfter = this.toTicks(after);
    this._timeline.forEachFrom(computedAfter, (event) => this.clear(event.id));
    this._repeatedEvents.forEachFrom(computedAfter, (event) => this.clear(event.id));
    return this;
  }
  //-------------------------------------
  // 	START/STOP/PAUSE
  //-------------------------------------
  /**
   * Bind start/stop/pause events from the clock and emit them.
   */
  _bindClockEvents() {
    this._clock.on("start", (time, offset) => {
      offset = new TicksClass(this.context, offset).toSeconds();
      this.emit("start", time, offset);
    });
    this._clock.on("stop", (time) => {
      this.emit("stop", time);
    });
    this._clock.on("pause", (time) => {
      this.emit("pause", time);
    });
  }
  /**
   * Returns the playback state of the source, either "started", "stopped", or "paused"
   */
  get state() {
    return this._clock.getStateAtTime(this.now());
  }
  /**
   * Start the transport and all sources synced to the transport.
   * @param  time The time when the transport should start.
   * @param  offset The timeline offset to start the transport.
   * @example
   * // start the transport in one second starting at beginning of the 5th measure.
   * Tone.getTransport().start("+1", "4:0:0");
   */
  start(time, offset) {
    this.context.resume();
    let offsetTicks;
    if (isDefined(offset)) {
      offsetTicks = this.toTicks(offset);
    }
    this._clock.start(time, offsetTicks);
    return this;
  }
  /**
   * Stop the transport and all sources synced to the transport.
   * @param time The time when the transport should stop.
   * @example
   * Tone.getTransport().stop();
   */
  stop(time) {
    this._clock.stop(time);
    return this;
  }
  /**
   * Pause the transport and all sources synced to the transport.
   */
  pause(time) {
    this._clock.pause(time);
    return this;
  }
  /**
   * Toggle the current state of the transport. If it is
   * started, it will stop it, otherwise it will start the Transport.
   * @param  time The time of the event
   */
  toggle(time) {
    time = this.toSeconds(time);
    if (this._clock.getStateAtTime(time) !== "started") {
      this.start(time);
    } else {
      this.stop(time);
    }
    return this;
  }
  //-------------------------------------
  // 	SETTERS/GETTERS
  //-------------------------------------
  /**
   * The time signature as just the numerator over 4.
   * For example 4/4 would be just 4 and 6/8 would be 3.
   * @example
   * // common time
   * Tone.getTransport().timeSignature = 4;
   * // 7/8
   * Tone.getTransport().timeSignature = [7, 8];
   * // this will be reduced to a single number
   * Tone.getTransport().timeSignature; // returns 3.5
   */
  get timeSignature() {
    return this._timeSignature;
  }
  set timeSignature(timeSig) {
    if (isArray(timeSig)) {
      timeSig = timeSig[0] / timeSig[1] * 4;
    }
    this._timeSignature = timeSig;
  }
  /**
   * When the Transport.loop = true, this is the starting position of the loop.
   */
  get loopStart() {
    return new TimeClass(this.context, this._loopStart, "i").toSeconds();
  }
  set loopStart(startPosition) {
    this._loopStart = this.toTicks(startPosition);
  }
  /**
   * When the Transport.loop = true, this is the ending position of the loop.
   */
  get loopEnd() {
    return new TimeClass(this.context, this._loopEnd, "i").toSeconds();
  }
  set loopEnd(endPosition) {
    this._loopEnd = this.toTicks(endPosition);
  }
  /**
   * If the transport loops or not.
   */
  get loop() {
    return this._loop.get(this.now());
  }
  set loop(loop) {
    this._loop.set(loop, this.now());
  }
  /**
   * Set the loop start and stop at the same time.
   * @example
   * // loop over the first measure
   * Tone.getTransport().setLoopPoints(0, "1m");
   * Tone.getTransport().loop = true;
   */
  setLoopPoints(startPosition, endPosition) {
    this.loopStart = startPosition;
    this.loopEnd = endPosition;
    return this;
  }
  /**
   * The swing value. Between 0-1 where 1 equal to the note + half the subdivision.
   */
  get swing() {
    return this._swingAmount;
  }
  set swing(amount) {
    this._swingAmount = amount;
  }
  /**
   * Set the subdivision which the swing will be applied to.
   * The default value is an 8th note. Value must be less
   * than a quarter note.
   */
  get swingSubdivision() {
    return new TicksClass(this.context, this._swingTicks).toNotation();
  }
  set swingSubdivision(subdivision) {
    this._swingTicks = this.toTicks(subdivision);
  }
  /**
   * The Transport's position in Bars:Beats:Sixteenths.
   * Setting the value will jump to that position right away.
   */
  get position() {
    const now = this.now();
    const ticks = this._clock.getTicksAtTime(now);
    return new TicksClass(this.context, ticks).toBarsBeatsSixteenths();
  }
  set position(progress) {
    const ticks = this.toTicks(progress);
    this.ticks = ticks;
  }
  /**
   * The Transport's position in seconds.
   * Setting the value will jump to that position right away.
   */
  get seconds() {
    return this._clock.seconds;
  }
  set seconds(s) {
    const now = this.now();
    const ticks = this._clock.frequency.timeToTicks(s, now);
    this.ticks = ticks;
  }
  /**
   * The Transport's loop position as a normalized value. Always
   * returns 0 if the Transport.loop = false.
   */
  get progress() {
    if (this.loop) {
      const now = this.now();
      const ticks = this._clock.getTicksAtTime(now);
      return (ticks - this._loopStart) / (this._loopEnd - this._loopStart);
    } else {
      return 0;
    }
  }
  /**
   * The Transport's current tick position.
   */
  get ticks() {
    return this._clock.ticks;
  }
  set ticks(t) {
    if (this._clock.ticks !== t) {
      const now = this.now();
      if (this.state === "started") {
        const ticks = this._clock.getTicksAtTime(now);
        const remainingTick = this._clock.frequency.getDurationOfTicks(Math.ceil(ticks) - ticks, now);
        const time = now + remainingTick;
        this.emit("stop", time);
        this._clock.setTicksAtTime(t, time);
        this.emit("start", time, this._clock.getSecondsAtTime(time));
      } else {
        this.emit("ticks", now);
        this._clock.setTicksAtTime(t, now);
      }
    }
  }
  /**
   * Get the clock's ticks at the given time.
   * @param  time  When to get the tick value
   * @return The tick value at the given time.
   */
  getTicksAtTime(time) {
    return this._clock.getTicksAtTime(time);
  }
  /**
   * Return the elapsed seconds at the given time.
   * @param  time  When to get the elapsed seconds
   * @return  The number of elapsed seconds
   */
  getSecondsAtTime(time) {
    return this._clock.getSecondsAtTime(time);
  }
  /**
   * Pulses Per Quarter note. This is the smallest resolution
   * the Transport timing supports. This should be set once
   * on initialization and not set again. Changing this value
   * after other objects have been created can cause problems.
   */
  get PPQ() {
    return this._clock.frequency.multiplier;
  }
  set PPQ(ppq) {
    this._clock.frequency.multiplier = ppq;
  }
  //-------------------------------------
  // 	SYNCING
  //-------------------------------------
  /**
   * Returns the time aligned to the next subdivision
   * of the Transport. If the Transport is not started,
   * it will return 0.
   * Note: this will not work precisely during tempo ramps.
   * @param  subdivision  The subdivision to quantize to
   * @return  The context time of the next subdivision.
   * @example
   * // the transport must be started, otherwise returns 0
   * Tone.getTransport().start();
   * Tone.getTransport().nextSubdivision("4n");
   */
  nextSubdivision(subdivision) {
    subdivision = this.toTicks(subdivision);
    if (this.state !== "started") {
      return 0;
    } else {
      const now = this.now();
      const transportPos = this.getTicksAtTime(now);
      const remainingTicks = subdivision - transportPos % subdivision;
      return this._clock.nextTickTime(remainingTicks, now);
    }
  }
  /**
   * Attaches the signal to the tempo control signal so that
   * any changes in the tempo will change the signal in the same
   * ratio.
   *
   * @param signal
   * @param ratio Optionally pass in the ratio between the two signals.
   * 			Otherwise it will be computed based on their current values.
   */
  syncSignal(signal, ratio) {
    const now = this.now();
    let source = this.bpm;
    let sourceValue = 1 / (60 / source.getValueAtTime(now) / this.PPQ);
    let nodes = [];
    if (signal.units === "time") {
      const scaleFactor = 1 / 64 / sourceValue;
      const scaleBefore = new Gain(scaleFactor);
      const reciprocal = new Pow(-1);
      const scaleAfter = new Gain(scaleFactor);
      source.chain(scaleBefore, reciprocal, scaleAfter);
      source = scaleAfter;
      sourceValue = 1 / sourceValue;
      nodes = [scaleBefore, reciprocal, scaleAfter];
    }
    if (!ratio) {
      if (signal.getValueAtTime(now) !== 0) {
        ratio = signal.getValueAtTime(now) / sourceValue;
      } else {
        ratio = 0;
      }
    }
    const ratioSignal = new Gain(ratio);
    source.connect(ratioSignal);
    ratioSignal.connect(signal._param);
    nodes.push(ratioSignal);
    this._syncedSignals.push({
      initial: signal.value,
      nodes,
      signal
    });
    signal.value = 0;
    return this;
  }
  /**
   * Unsyncs a previously synced signal from the transport's control.
   * @see {@link syncSignal}.
   */
  unsyncSignal(signal) {
    for (let i = this._syncedSignals.length - 1; i >= 0; i--) {
      const syncedSignal = this._syncedSignals[i];
      if (syncedSignal.signal === signal) {
        syncedSignal.nodes.forEach((node) => node.dispose());
        syncedSignal.signal.value = syncedSignal.initial;
        this._syncedSignals.splice(i, 1);
      }
    }
    return this;
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._clock.dispose();
    writable(this, "bpm");
    this._timeline.dispose();
    this._repeatedEvents.dispose();
    return this;
  }
};
Emitter.mixin(TransportClass);
onContextInit((context2) => {
  context2.transport = new TransportClass({ context: context2 });
});
onContextClose((context2) => {
  context2.transport.dispose();
});

// node_modules/tone/build/esm/source/Source.js
var Source = class extends ToneAudioNode {
  constructor(options) {
    super(options);
    this.input = void 0;
    this._state = new StateTimeline("stopped");
    this._synced = false;
    this._scheduled = [];
    this._syncedStart = noOp;
    this._syncedStop = noOp;
    this._state.memory = 100;
    this._state.increasing = true;
    this._volume = this.output = new Volume({
      context: this.context,
      mute: options.mute,
      volume: options.volume
    });
    this.volume = this._volume.volume;
    readOnly(this, "volume");
    this.onstop = options.onstop;
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      mute: false,
      onstop: noOp,
      volume: 0
    });
  }
  /**
   * Returns the playback state of the source, either "started" or "stopped".
   * @example
   * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/ahntone_c3.mp3", () => {
   * 	player.start();
   * 	console.log(player.state);
   * }).toDestination();
   */
  get state() {
    if (this._synced) {
      if (this.context.transport.state === "started") {
        return this._state.getValueAtTime(this.context.transport.seconds);
      } else {
        return "stopped";
      }
    } else {
      return this._state.getValueAtTime(this.now());
    }
  }
  /**
   * Mute the output.
   * @example
   * const osc = new Tone.Oscillator().toDestination().start();
   * // mute the output
   * osc.mute = true;
   */
  get mute() {
    return this._volume.mute;
  }
  set mute(mute) {
    this._volume.mute = mute;
  }
  /**
   * Ensure that the scheduled time is not before the current time.
   * Should only be used when scheduled unsynced.
   */
  _clampToCurrentTime(time) {
    if (this._synced) {
      return time;
    } else {
      return Math.max(time, this.context.currentTime);
    }
  }
  /**
   * Start the source at the specified time. If no time is given,
   * start the source now.
   * @param  time When the source should be started.
   * @example
   * const source = new Tone.Oscillator().toDestination();
   * source.start("+0.5"); // starts the source 0.5 seconds from now
   */
  start(time, offset, duration) {
    let computedTime = isUndef(time) && this._synced ? this.context.transport.seconds : this.toSeconds(time);
    computedTime = this._clampToCurrentTime(computedTime);
    if (!this._synced && this._state.getValueAtTime(computedTime) === "started") {
      assert(GT(computedTime, this._state.get(computedTime).time), "Start time must be strictly greater than previous start time");
      this._state.cancel(computedTime);
      this._state.setStateAtTime("started", computedTime);
      this.log("restart", computedTime);
      this.restart(computedTime, offset, duration);
    } else {
      this.log("start", computedTime);
      this._state.setStateAtTime("started", computedTime);
      if (this._synced) {
        const event = this._state.get(computedTime);
        if (event) {
          event.offset = this.toSeconds(defaultArg(offset, 0));
          event.duration = duration ? this.toSeconds(duration) : void 0;
        }
        const sched = this.context.transport.schedule((t) => {
          this._start(t, offset, duration);
        }, computedTime);
        this._scheduled.push(sched);
        if (this.context.transport.state === "started" && this.context.transport.getSecondsAtTime(this.immediate()) > computedTime) {
          this._syncedStart(this.now(), this.context.transport.seconds);
        }
      } else {
        assertContextRunning(this.context);
        this._start(computedTime, offset, duration);
      }
    }
    return this;
  }
  /**
   * Stop the source at the specified time. If no time is given,
   * stop the source now.
   * @param  time When the source should be stopped.
   * @example
   * const source = new Tone.Oscillator().toDestination();
   * source.start();
   * source.stop("+0.5"); // stops the source 0.5 seconds from now
   */
  stop(time) {
    let computedTime = isUndef(time) && this._synced ? this.context.transport.seconds : this.toSeconds(time);
    computedTime = this._clampToCurrentTime(computedTime);
    if (this._state.getValueAtTime(computedTime) === "started" || isDefined(this._state.getNextState("started", computedTime))) {
      this.log("stop", computedTime);
      if (!this._synced) {
        this._stop(computedTime);
      } else {
        const sched = this.context.transport.schedule(this._stop.bind(this), computedTime);
        this._scheduled.push(sched);
      }
      this._state.cancel(computedTime);
      this._state.setStateAtTime("stopped", computedTime);
    }
    return this;
  }
  /**
   * Restart the source.
   */
  restart(time, offset, duration) {
    time = this.toSeconds(time);
    if (this._state.getValueAtTime(time) === "started") {
      this._state.cancel(time);
      this._restart(time, offset, duration);
    }
    return this;
  }
  /**
   * Sync the source to the Transport so that all subsequent
   * calls to `start` and `stop` are synced to the TransportTime
   * instead of the AudioContext time.
   *
   * @example
   * const osc = new Tone.Oscillator().toDestination();
   * // sync the source so that it plays between 0 and 0.3 on the Transport's timeline
   * osc.sync().start(0).stop(0.3);
   * // start the transport.
   * Tone.Transport.start();
   * // set it to loop once a second
   * Tone.Transport.loop = true;
   * Tone.Transport.loopEnd = 1;
   */
  sync() {
    if (!this._synced) {
      this._synced = true;
      this._syncedStart = (time, offset) => {
        if (GT(offset, 0)) {
          const stateEvent = this._state.get(offset);
          if (stateEvent && stateEvent.state === "started" && stateEvent.time !== offset) {
            const startOffset = offset - this.toSeconds(stateEvent.time);
            let duration;
            if (stateEvent.duration) {
              duration = this.toSeconds(stateEvent.duration) - startOffset;
            }
            this._start(time, this.toSeconds(stateEvent.offset) + startOffset, duration);
          }
        }
      };
      this._syncedStop = (time) => {
        const seconds = this.context.transport.getSecondsAtTime(Math.max(time - this.sampleTime, 0));
        if (this._state.getValueAtTime(seconds) === "started") {
          this._stop(time);
        }
      };
      this.context.transport.on("start", this._syncedStart);
      this.context.transport.on("loopStart", this._syncedStart);
      this.context.transport.on("stop", this._syncedStop);
      this.context.transport.on("pause", this._syncedStop);
      this.context.transport.on("loopEnd", this._syncedStop);
    }
    return this;
  }
  /**
   * Unsync the source to the Transport.
   * @see {@link sync}
   */
  unsync() {
    if (this._synced) {
      this.context.transport.off("stop", this._syncedStop);
      this.context.transport.off("pause", this._syncedStop);
      this.context.transport.off("loopEnd", this._syncedStop);
      this.context.transport.off("start", this._syncedStart);
      this.context.transport.off("loopStart", this._syncedStart);
    }
    this._synced = false;
    this._scheduled.forEach((id) => this.context.transport.clear(id));
    this._scheduled = [];
    this._state.cancel(0);
    this._stop(0);
    return this;
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this.onstop = noOp;
    this.unsync();
    this._volume.dispose();
    this._state.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/buffer/ToneBufferSource.js
var ToneBufferSource = class _ToneBufferSource extends OneShotSource {
  constructor() {
    const options = optionsFromArguments(_ToneBufferSource.getDefaults(), arguments, ["url", "onload"]);
    super(options);
    this.name = "ToneBufferSource";
    this._source = this.context.createBufferSource();
    this._internalChannels = [this._source];
    this._sourceStarted = false;
    this._sourceStopped = false;
    connect(this._source, this._gainNode);
    this._source.onended = () => this._stopSource();
    this.playbackRate = new Param({
      context: this.context,
      param: this._source.playbackRate,
      units: "positive",
      value: options.playbackRate
    });
    this.loop = options.loop;
    this.loopStart = options.loopStart;
    this.loopEnd = options.loopEnd;
    this._buffer = new ToneAudioBuffer(options.url, options.onload, options.onerror);
    this._internalChannels.push(this._source);
  }
  static getDefaults() {
    return Object.assign(OneShotSource.getDefaults(), {
      url: new ToneAudioBuffer(),
      loop: false,
      loopEnd: 0,
      loopStart: 0,
      onload: noOp,
      onerror: noOp,
      playbackRate: 1
    });
  }
  /**
   * The fadeIn time of the amplitude envelope.
   */
  get fadeIn() {
    return this._fadeIn;
  }
  set fadeIn(t) {
    this._fadeIn = t;
  }
  /**
   * The fadeOut time of the amplitude envelope.
   */
  get fadeOut() {
    return this._fadeOut;
  }
  set fadeOut(t) {
    this._fadeOut = t;
  }
  /**
   * The curve applied to the fades, either "linear" or "exponential"
   */
  get curve() {
    return this._curve;
  }
  set curve(t) {
    this._curve = t;
  }
  /**
   * Start the buffer
   * @param  time When the player should start.
   * @param  offset The offset from the beginning of the sample to start at.
   * @param  duration How long the sample should play. If no duration is given, it will default to the full length of the sample (minus any offset)
   * @param  gain  The gain to play the buffer back at.
   */
  start(time, offset, duration, gain = 1) {
    assert(this.buffer.loaded, "buffer is either not set or not loaded");
    const computedTime = this.toSeconds(time);
    this._startGain(computedTime, gain);
    if (this.loop) {
      offset = defaultArg(offset, this.loopStart);
    } else {
      offset = defaultArg(offset, 0);
    }
    let computedOffset = Math.max(this.toSeconds(offset), 0);
    if (this.loop) {
      const loopEnd = this.toSeconds(this.loopEnd) || this.buffer.duration;
      const loopStart = this.toSeconds(this.loopStart);
      const loopDuration = loopEnd - loopStart;
      if (GTE(computedOffset, loopEnd)) {
        computedOffset = (computedOffset - loopStart) % loopDuration + loopStart;
      }
      if (EQ(computedOffset, this.buffer.duration)) {
        computedOffset = 0;
      }
    }
    this._source.buffer = this.buffer.get();
    this._source.loopEnd = this.toSeconds(this.loopEnd) || this.buffer.duration;
    if (LT(computedOffset, this.buffer.duration)) {
      this._sourceStarted = true;
      this._source.start(computedTime, computedOffset);
    }
    if (isDefined(duration)) {
      let computedDur = this.toSeconds(duration);
      computedDur = Math.max(computedDur, 0);
      this.stop(computedTime + computedDur);
    }
    return this;
  }
  _stopSource(time) {
    if (!this._sourceStopped && this._sourceStarted) {
      this._sourceStopped = true;
      this._source.stop(this.toSeconds(time));
      this._onended();
    }
  }
  /**
   * If loop is true, the loop will start at this position.
   */
  get loopStart() {
    return this._source.loopStart;
  }
  set loopStart(loopStart) {
    this._source.loopStart = this.toSeconds(loopStart);
  }
  /**
   * If loop is true, the loop will end at this position.
   */
  get loopEnd() {
    return this._source.loopEnd;
  }
  set loopEnd(loopEnd) {
    this._source.loopEnd = this.toSeconds(loopEnd);
  }
  /**
   * The audio buffer belonging to the player.
   */
  get buffer() {
    return this._buffer;
  }
  set buffer(buffer) {
    this._buffer.set(buffer);
  }
  /**
   * If the buffer should loop once it's over.
   */
  get loop() {
    return this._source.loop;
  }
  set loop(loop) {
    this._source.loop = loop;
    if (this._sourceStarted) {
      this.cancelStop();
    }
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._source.onended = null;
    this._source.disconnect();
    this._buffer.dispose();
    this.playbackRate.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/Noise.js
var Noise = class _Noise extends Source {
  constructor() {
    const options = optionsFromArguments(_Noise.getDefaults(), arguments, [
      "type"
    ]);
    super(options);
    this.name = "Noise";
    this._source = null;
    this._playbackRate = options.playbackRate;
    this.type = options.type;
    this._fadeIn = options.fadeIn;
    this._fadeOut = options.fadeOut;
  }
  static getDefaults() {
    return Object.assign(Source.getDefaults(), {
      fadeIn: 0,
      fadeOut: 0,
      playbackRate: 1,
      type: "white"
    });
  }
  /**
   * The type of the noise. Can be "white", "brown", or "pink".
   * @example
   * const noise = new Tone.Noise().toDestination().start();
   * noise.type = "brown";
   */
  get type() {
    return this._type;
  }
  set type(type) {
    assert(type in _noiseBuffers, "Noise: invalid type: " + type);
    if (this._type !== type) {
      this._type = type;
      if (this.state === "started") {
        const now = this.now();
        this._stop(now);
        this._start(now);
      }
    }
  }
  /**
   * The playback rate of the noise. Affects
   * the "frequency" of the noise.
   */
  get playbackRate() {
    return this._playbackRate;
  }
  set playbackRate(rate) {
    this._playbackRate = rate;
    if (this._source) {
      this._source.playbackRate.value = rate;
    }
  }
  /**
   * internal start method
   */
  _start(time) {
    const buffer = _noiseBuffers[this._type];
    this._source = new ToneBufferSource({
      url: buffer,
      context: this.context,
      fadeIn: this._fadeIn,
      fadeOut: this._fadeOut,
      loop: true,
      onended: () => this.onstop(this),
      playbackRate: this._playbackRate
    }).connect(this.output);
    this._source.start(this.toSeconds(time), Math.random() * (buffer.duration - 1e-3));
  }
  /**
   * internal stop method
   */
  _stop(time) {
    if (this._source) {
      this._source.stop(this.toSeconds(time));
      this._source = null;
    }
  }
  /**
   * The fadeIn time of the amplitude envelope.
   */
  get fadeIn() {
    return this._fadeIn;
  }
  set fadeIn(time) {
    this._fadeIn = time;
    if (this._source) {
      this._source.fadeIn = this._fadeIn;
    }
  }
  /**
   * The fadeOut time of the amplitude envelope.
   */
  get fadeOut() {
    return this._fadeOut;
  }
  set fadeOut(time) {
    this._fadeOut = time;
    if (this._source) {
      this._source.fadeOut = this._fadeOut;
    }
  }
  _restart(time) {
    this._stop(time);
    this._start(time);
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    if (this._source) {
      this._source.disconnect();
    }
    return this;
  }
};
var BUFFER_LENGTH = 44100 * 5;
var NUM_CHANNELS = 2;
var _noiseCache = {
  brown: null,
  pink: null,
  white: null
};
var _noiseBuffers = {
  get brown() {
    if (!_noiseCache.brown) {
      const buffer = [];
      for (let channelNum = 0; channelNum < NUM_CHANNELS; channelNum++) {
        const channel = new Float32Array(BUFFER_LENGTH);
        buffer[channelNum] = channel;
        let lastOut = 0;
        for (let i = 0; i < BUFFER_LENGTH; i++) {
          const white = Math.random() * 2 - 1;
          channel[i] = (lastOut + 0.02 * white) / 1.02;
          lastOut = channel[i];
          channel[i] *= 3.5;
        }
      }
      _noiseCache.brown = new ToneAudioBuffer().fromArray(buffer);
    }
    return _noiseCache.brown;
  },
  get pink() {
    if (!_noiseCache.pink) {
      const buffer = [];
      for (let channelNum = 0; channelNum < NUM_CHANNELS; channelNum++) {
        const channel = new Float32Array(BUFFER_LENGTH);
        buffer[channelNum] = channel;
        let b0, b1, b2, b3, b4, b5, b6;
        b0 = b1 = b2 = b3 = b4 = b5 = b6 = 0;
        for (let i = 0; i < BUFFER_LENGTH; i++) {
          const white = Math.random() * 2 - 1;
          b0 = 0.99886 * b0 + white * 0.0555179;
          b1 = 0.99332 * b1 + white * 0.0750759;
          b2 = 0.969 * b2 + white * 0.153852;
          b3 = 0.8665 * b3 + white * 0.3104856;
          b4 = 0.55 * b4 + white * 0.5329522;
          b5 = -0.7616 * b5 - white * 0.016898;
          channel[i] = b0 + b1 + b2 + b3 + b4 + b5 + b6 + white * 0.5362;
          channel[i] *= 0.11;
          b6 = white * 0.115926;
        }
      }
      _noiseCache.pink = new ToneAudioBuffer().fromArray(buffer);
    }
    return _noiseCache.pink;
  },
  get white() {
    if (!_noiseCache.white) {
      const buffer = [];
      for (let channelNum = 0; channelNum < NUM_CHANNELS; channelNum++) {
        const channel = new Float32Array(BUFFER_LENGTH);
        buffer[channelNum] = channel;
        for (let i = 0; i < BUFFER_LENGTH; i++) {
          channel[i] = Math.random() * 2 - 1;
        }
      }
      _noiseCache.white = new ToneAudioBuffer().fromArray(buffer);
    }
    return _noiseCache.white;
  }
};

// node_modules/tone/build/esm/source/oscillator/OscillatorInterface.js
function generateWaveform(instance, length) {
  return __awaiter(this, void 0, void 0, function* () {
    const duration = length / instance.context.sampleRate;
    const context2 = new OfflineContext(1, duration, instance.context.sampleRate);
    const clone = new instance.constructor(Object.assign(instance.get(), {
      // should do 2 iterations
      frequency: 2 / duration,
      // zero out the detune
      detune: 0,
      context: context2
    })).toDestination();
    clone.start(0);
    const buffer = yield context2.render();
    return buffer.getChannelData(0);
  });
}

// node_modules/tone/build/esm/source/oscillator/ToneOscillatorNode.js
var ToneOscillatorNode = class _ToneOscillatorNode extends OneShotSource {
  constructor() {
    const options = optionsFromArguments(_ToneOscillatorNode.getDefaults(), arguments, ["frequency", "type"]);
    super(options);
    this.name = "ToneOscillatorNode";
    this._oscillator = this.context.createOscillator();
    this._internalChannels = [this._oscillator];
    connect(this._oscillator, this._gainNode);
    this.type = options.type;
    this.frequency = new Param({
      context: this.context,
      param: this._oscillator.frequency,
      units: "frequency",
      value: options.frequency
    });
    this.detune = new Param({
      context: this.context,
      param: this._oscillator.detune,
      units: "cents",
      value: options.detune
    });
    readOnly(this, ["frequency", "detune"]);
  }
  static getDefaults() {
    return Object.assign(OneShotSource.getDefaults(), {
      detune: 0,
      frequency: 440,
      type: "sine"
    });
  }
  /**
   * Start the oscillator node at the given time
   * @param  time When to start the oscillator
   */
  start(time) {
    const computedTime = this.toSeconds(time);
    this.log("start", computedTime);
    this._startGain(computedTime);
    this._oscillator.start(computedTime);
    return this;
  }
  _stopSource(time) {
    this._oscillator.stop(time);
  }
  /**
   * Sets an arbitrary custom periodic waveform given a PeriodicWave.
   * @param  periodicWave PeriodicWave should be created with context.createPeriodicWave
   */
  setPeriodicWave(periodicWave) {
    this._oscillator.setPeriodicWave(periodicWave);
    return this;
  }
  /**
   * The oscillator type. Either 'sine', 'sawtooth', 'square', or 'triangle'
   */
  get type() {
    return this._oscillator.type;
  }
  set type(type) {
    this._oscillator.type = type;
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    if (this.state === "started") {
      this.stop();
    }
    this._oscillator.disconnect();
    this.frequency.dispose();
    this.detune.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/oscillator/Oscillator.js
var Oscillator = class _Oscillator extends Source {
  constructor() {
    const options = optionsFromArguments(_Oscillator.getDefaults(), arguments, ["frequency", "type"]);
    super(options);
    this.name = "Oscillator";
    this._oscillator = null;
    this.frequency = new Signal({
      context: this.context,
      units: "frequency",
      value: options.frequency
    });
    readOnly(this, "frequency");
    this.detune = new Signal({
      context: this.context,
      units: "cents",
      value: options.detune
    });
    readOnly(this, "detune");
    this._partials = options.partials;
    this._partialCount = options.partialCount;
    this._type = options.type;
    if (options.partialCount && options.type !== "custom") {
      this._type = this.baseType + options.partialCount.toString();
    }
    this.phase = options.phase;
  }
  static getDefaults() {
    return Object.assign(Source.getDefaults(), {
      detune: 0,
      frequency: 440,
      partialCount: 0,
      partials: [],
      phase: 0,
      type: "sine"
    });
  }
  /**
   * start the oscillator
   */
  _start(time) {
    const computedTime = this.toSeconds(time);
    const oscillator = new ToneOscillatorNode({
      context: this.context,
      onended: () => this.onstop(this)
    });
    this._oscillator = oscillator;
    if (this._wave) {
      this._oscillator.setPeriodicWave(this._wave);
    } else {
      this._oscillator.type = this._type;
    }
    this._oscillator.connect(this.output);
    this.frequency.connect(this._oscillator.frequency);
    this.detune.connect(this._oscillator.detune);
    this._oscillator.start(computedTime);
  }
  /**
   * stop the oscillator
   */
  _stop(time) {
    const computedTime = this.toSeconds(time);
    if (this._oscillator) {
      this._oscillator.stop(computedTime);
    }
  }
  /**
   * Restart the oscillator. Does not stop the oscillator, but instead
   * just cancels any scheduled 'stop' from being invoked.
   */
  _restart(time) {
    const computedTime = this.toSeconds(time);
    this.log("restart", computedTime);
    if (this._oscillator) {
      this._oscillator.cancelStop();
    }
    this._state.cancel(computedTime);
    return this;
  }
  /**
   * Sync the signal to the Transport's bpm. Any changes to the transports bpm,
   * will also affect the oscillators frequency.
   * @example
   * const osc = new Tone.Oscillator().toDestination().start();
   * osc.frequency.value = 440;
   * // the ratio between the bpm and the frequency will be maintained
   * osc.syncFrequency();
   * // double the tempo
   * Tone.Transport.bpm.value *= 2;
   * // the frequency of the oscillator is doubled to 880
   */
  syncFrequency() {
    this.context.transport.syncSignal(this.frequency);
    return this;
  }
  /**
   * Unsync the oscillator's frequency from the Transport.
   * @see {@link syncFrequency}
   */
  unsyncFrequency() {
    this.context.transport.unsyncSignal(this.frequency);
    return this;
  }
  /**
   * Get a cached periodic wave. Avoids having to recompute
   * the oscillator values when they have already been computed
   * with the same values.
   */
  _getCachedPeriodicWave() {
    if (this._type === "custom") {
      const oscProps = _Oscillator._periodicWaveCache.find((description) => {
        return description.phase === this._phase && deepEquals(description.partials, this._partials);
      });
      return oscProps;
    } else {
      const oscProps = _Oscillator._periodicWaveCache.find((description) => {
        return description.type === this._type && description.phase === this._phase;
      });
      this._partialCount = oscProps ? oscProps.partialCount : this._partialCount;
      return oscProps;
    }
  }
  get type() {
    return this._type;
  }
  set type(type) {
    this._type = type;
    const isBasicType = ["sine", "square", "sawtooth", "triangle"].indexOf(type) !== -1;
    if (this._phase === 0 && isBasicType) {
      this._wave = void 0;
      this._partialCount = 0;
      if (this._oscillator !== null) {
        this._oscillator.type = type;
      }
    } else {
      const cache = this._getCachedPeriodicWave();
      if (isDefined(cache)) {
        const { partials, wave } = cache;
        this._wave = wave;
        this._partials = partials;
        if (this._oscillator !== null) {
          this._oscillator.setPeriodicWave(this._wave);
        }
      } else {
        const [real, imag] = this._getRealImaginary(type, this._phase);
        const periodicWave = this.context.createPeriodicWave(real, imag);
        this._wave = periodicWave;
        if (this._oscillator !== null) {
          this._oscillator.setPeriodicWave(this._wave);
        }
        _Oscillator._periodicWaveCache.push({
          imag,
          partialCount: this._partialCount,
          partials: this._partials,
          phase: this._phase,
          real,
          type: this._type,
          wave: this._wave
        });
        if (_Oscillator._periodicWaveCache.length > 100) {
          _Oscillator._periodicWaveCache.shift();
        }
      }
    }
  }
  get baseType() {
    return this._type.replace(this.partialCount.toString(), "");
  }
  set baseType(baseType) {
    if (this.partialCount && this._type !== "custom" && baseType !== "custom") {
      this.type = baseType + this.partialCount;
    } else {
      this.type = baseType;
    }
  }
  get partialCount() {
    return this._partialCount;
  }
  set partialCount(p) {
    assertRange(p, 0);
    let type = this._type;
    const partial = /^(sine|triangle|square|sawtooth)(\d+)$/.exec(this._type);
    if (partial) {
      type = partial[1];
    }
    if (this._type !== "custom") {
      if (p === 0) {
        this.type = type;
      } else {
        this.type = type + p.toString();
      }
    } else {
      const fullPartials = new Float32Array(p);
      this._partials.forEach((v, i) => fullPartials[i] = v);
      this._partials = Array.from(fullPartials);
      this.type = this._type;
    }
  }
  /**
   * Returns the real and imaginary components based
   * on the oscillator type.
   * @returns [real: Float32Array, imaginary: Float32Array]
   */
  _getRealImaginary(type, phase) {
    const fftSize = 4096;
    let periodicWaveSize = fftSize / 2;
    const real = new Float32Array(periodicWaveSize);
    const imag = new Float32Array(periodicWaveSize);
    let partialCount = 1;
    if (type === "custom") {
      partialCount = this._partials.length + 1;
      this._partialCount = this._partials.length;
      periodicWaveSize = partialCount;
      if (this._partials.length === 0) {
        return [real, imag];
      }
    } else {
      const partial = /^(sine|triangle|square|sawtooth)(\d+)$/.exec(type);
      if (partial) {
        partialCount = parseInt(partial[2], 10) + 1;
        this._partialCount = parseInt(partial[2], 10);
        type = partial[1];
        partialCount = Math.max(partialCount, 2);
        periodicWaveSize = partialCount;
      } else {
        this._partialCount = 0;
      }
      this._partials = [];
    }
    for (let n = 1; n < periodicWaveSize; ++n) {
      const piFactor = 2 / (n * Math.PI);
      let b;
      switch (type) {
        case "sine":
          b = n <= partialCount ? 1 : 0;
          this._partials[n - 1] = b;
          break;
        case "square":
          b = n & 1 ? 2 * piFactor : 0;
          this._partials[n - 1] = b;
          break;
        case "sawtooth":
          b = piFactor * (n & 1 ? 1 : -1);
          this._partials[n - 1] = b;
          break;
        case "triangle":
          if (n & 1) {
            b = 2 * (piFactor * piFactor) * (n - 1 >> 1 & 1 ? -1 : 1);
          } else {
            b = 0;
          }
          this._partials[n - 1] = b;
          break;
        case "custom":
          b = this._partials[n - 1];
          break;
        default:
          throw new TypeError("Oscillator: invalid type: " + type);
      }
      if (b !== 0) {
        real[n] = -b * Math.sin(phase * n);
        imag[n] = b * Math.cos(phase * n);
      } else {
        real[n] = 0;
        imag[n] = 0;
      }
    }
    return [real, imag];
  }
  /**
   * Compute the inverse FFT for a given phase.
   */
  _inverseFFT(real, imag, phase) {
    let sum = 0;
    const len = real.length;
    for (let i = 0; i < len; i++) {
      sum += real[i] * Math.cos(i * phase) + imag[i] * Math.sin(i * phase);
    }
    return sum;
  }
  /**
   * Returns the initial value of the oscillator when stopped.
   * E.g. a "sine" oscillator with phase = 90 would return an initial value of -1.
   */
  getInitialValue() {
    const [real, imag] = this._getRealImaginary(this._type, 0);
    let maxValue = 0;
    const twoPi = Math.PI * 2;
    const testPositions = 32;
    for (let i = 0; i < testPositions; i++) {
      maxValue = Math.max(this._inverseFFT(real, imag, i / testPositions * twoPi), maxValue);
    }
    return clamp(-this._inverseFFT(real, imag, this._phase) / maxValue, -1, 1);
  }
  get partials() {
    return this._partials.slice(0, this.partialCount);
  }
  set partials(partials) {
    this._partials = partials;
    this._partialCount = this._partials.length;
    if (partials.length) {
      this.type = "custom";
    }
  }
  get phase() {
    return this._phase * (180 / Math.PI);
  }
  set phase(phase) {
    this._phase = phase * Math.PI / 180;
    this.type = this._type;
  }
  asArray() {
    return __awaiter(this, arguments, void 0, function* (length = 1024) {
      return generateWaveform(this, length);
    });
  }
  dispose() {
    super.dispose();
    if (this._oscillator !== null) {
      this._oscillator.dispose();
    }
    this._wave = void 0;
    this.frequency.dispose();
    this.detune.dispose();
    return this;
  }
};
Oscillator._periodicWaveCache = [];

// node_modules/tone/build/esm/signal/AudioToGain.js
var AudioToGain = class extends SignalOperator {
  constructor() {
    super(...arguments);
    this.name = "AudioToGain";
    this._norm = new WaveShaper({
      context: this.context,
      mapping: (x) => (x + 1) / 2
    });
    this.input = this._norm;
    this.output = this._norm;
  }
  /**
   * clean up
   */
  dispose() {
    super.dispose();
    this._norm.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/signal/Multiply.js
var Multiply = class _Multiply extends Signal {
  constructor() {
    const options = optionsFromArguments(_Multiply.getDefaults(), arguments, ["value"]);
    super(options);
    this.name = "Multiply";
    this.override = false;
    this._mult = this.input = this.output = new Gain({
      context: this.context,
      minValue: options.minValue,
      maxValue: options.maxValue
    });
    this.factor = this._param = this._mult.gain;
    this.factor.setValueAtTime(options.value, 0);
  }
  static getDefaults() {
    return Object.assign(Signal.getDefaults(), {
      value: 0
    });
  }
  dispose() {
    super.dispose();
    this._mult.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/oscillator/AMOscillator.js
var AMOscillator = class _AMOscillator extends Source {
  constructor() {
    const options = optionsFromArguments(_AMOscillator.getDefaults(), arguments, ["frequency", "type", "modulationType"]);
    super(options);
    this.name = "AMOscillator";
    this._modulationScale = new AudioToGain({ context: this.context });
    this._modulationNode = new Gain({
      context: this.context
    });
    this._carrier = new Oscillator({
      context: this.context,
      detune: options.detune,
      frequency: options.frequency,
      onstop: () => this.onstop(this),
      phase: options.phase,
      type: options.type
    });
    this.frequency = this._carrier.frequency, this.detune = this._carrier.detune;
    this._modulator = new Oscillator({
      context: this.context,
      phase: options.phase,
      type: options.modulationType
    });
    this.harmonicity = new Multiply({
      context: this.context,
      units: "positive",
      value: options.harmonicity
    });
    this.frequency.chain(this.harmonicity, this._modulator.frequency);
    this._modulator.chain(this._modulationScale, this._modulationNode.gain);
    this._carrier.chain(this._modulationNode, this.output);
    readOnly(this, ["frequency", "detune", "harmonicity"]);
  }
  static getDefaults() {
    return Object.assign(Oscillator.getDefaults(), {
      harmonicity: 1,
      modulationType: "square"
    });
  }
  /**
   * start the oscillator
   */
  _start(time) {
    this._modulator.start(time);
    this._carrier.start(time);
  }
  /**
   * stop the oscillator
   */
  _stop(time) {
    this._modulator.stop(time);
    this._carrier.stop(time);
  }
  _restart(time) {
    this._modulator.restart(time);
    this._carrier.restart(time);
  }
  /**
   * The type of the carrier oscillator
   */
  get type() {
    return this._carrier.type;
  }
  set type(type) {
    this._carrier.type = type;
  }
  get baseType() {
    return this._carrier.baseType;
  }
  set baseType(baseType) {
    this._carrier.baseType = baseType;
  }
  get partialCount() {
    return this._carrier.partialCount;
  }
  set partialCount(partialCount) {
    this._carrier.partialCount = partialCount;
  }
  /**
   * The type of the modulator oscillator
   */
  get modulationType() {
    return this._modulator.type;
  }
  set modulationType(type) {
    this._modulator.type = type;
  }
  get phase() {
    return this._carrier.phase;
  }
  set phase(phase) {
    this._carrier.phase = phase;
    this._modulator.phase = phase;
  }
  get partials() {
    return this._carrier.partials;
  }
  set partials(partials) {
    this._carrier.partials = partials;
  }
  asArray() {
    return __awaiter(this, arguments, void 0, function* (length = 1024) {
      return generateWaveform(this, length);
    });
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this.frequency.dispose();
    this.detune.dispose();
    this.harmonicity.dispose();
    this._carrier.dispose();
    this._modulator.dispose();
    this._modulationNode.dispose();
    this._modulationScale.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/oscillator/FMOscillator.js
var FMOscillator = class _FMOscillator extends Source {
  constructor() {
    const options = optionsFromArguments(_FMOscillator.getDefaults(), arguments, ["frequency", "type", "modulationType"]);
    super(options);
    this.name = "FMOscillator";
    this._modulationNode = new Gain({
      context: this.context,
      gain: 0
    });
    this._carrier = new Oscillator({
      context: this.context,
      detune: options.detune,
      frequency: 0,
      onstop: () => this.onstop(this),
      phase: options.phase,
      type: options.type
    });
    this.detune = this._carrier.detune;
    this.frequency = new Signal({
      context: this.context,
      units: "frequency",
      value: options.frequency
    });
    this._modulator = new Oscillator({
      context: this.context,
      phase: options.phase,
      type: options.modulationType
    });
    this.harmonicity = new Multiply({
      context: this.context,
      units: "positive",
      value: options.harmonicity
    });
    this.modulationIndex = new Multiply({
      context: this.context,
      units: "positive",
      value: options.modulationIndex
    });
    this.frequency.connect(this._carrier.frequency);
    this.frequency.chain(this.harmonicity, this._modulator.frequency);
    this.frequency.chain(this.modulationIndex, this._modulationNode);
    this._modulator.connect(this._modulationNode.gain);
    this._modulationNode.connect(this._carrier.frequency);
    this._carrier.connect(this.output);
    this.detune.connect(this._modulator.detune);
    readOnly(this, [
      "modulationIndex",
      "frequency",
      "detune",
      "harmonicity"
    ]);
  }
  static getDefaults() {
    return Object.assign(Oscillator.getDefaults(), {
      harmonicity: 1,
      modulationIndex: 2,
      modulationType: "square"
    });
  }
  /**
   * start the oscillator
   */
  _start(time) {
    this._modulator.start(time);
    this._carrier.start(time);
  }
  /**
   * stop the oscillator
   */
  _stop(time) {
    this._modulator.stop(time);
    this._carrier.stop(time);
  }
  _restart(time) {
    this._modulator.restart(time);
    this._carrier.restart(time);
    return this;
  }
  get type() {
    return this._carrier.type;
  }
  set type(type) {
    this._carrier.type = type;
  }
  get baseType() {
    return this._carrier.baseType;
  }
  set baseType(baseType) {
    this._carrier.baseType = baseType;
  }
  get partialCount() {
    return this._carrier.partialCount;
  }
  set partialCount(partialCount) {
    this._carrier.partialCount = partialCount;
  }
  /**
   * The type of the modulator oscillator
   */
  get modulationType() {
    return this._modulator.type;
  }
  set modulationType(type) {
    this._modulator.type = type;
  }
  get phase() {
    return this._carrier.phase;
  }
  set phase(phase) {
    this._carrier.phase = phase;
    this._modulator.phase = phase;
  }
  get partials() {
    return this._carrier.partials;
  }
  set partials(partials) {
    this._carrier.partials = partials;
  }
  asArray() {
    return __awaiter(this, arguments, void 0, function* (length = 1024) {
      return generateWaveform(this, length);
    });
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this.frequency.dispose();
    this.harmonicity.dispose();
    this._carrier.dispose();
    this._modulator.dispose();
    this._modulationNode.dispose();
    this.modulationIndex.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/oscillator/PulseOscillator.js
var PulseOscillator = class _PulseOscillator extends Source {
  constructor() {
    const options = optionsFromArguments(_PulseOscillator.getDefaults(), arguments, ["frequency", "width"]);
    super(options);
    this.name = "PulseOscillator";
    this._widthGate = new Gain({
      context: this.context,
      gain: 0
    });
    this._thresh = new WaveShaper({
      context: this.context,
      mapping: (val) => val <= 0 ? -1 : 1
    });
    this.width = new Signal({
      context: this.context,
      units: "audioRange",
      value: options.width
    });
    this._triangle = new Oscillator({
      context: this.context,
      detune: options.detune,
      frequency: options.frequency,
      onstop: () => this.onstop(this),
      phase: options.phase,
      type: "triangle"
    });
    this.frequency = this._triangle.frequency;
    this.detune = this._triangle.detune;
    this._triangle.chain(this._thresh, this.output);
    this.width.chain(this._widthGate, this._thresh);
    readOnly(this, ["width", "frequency", "detune"]);
  }
  static getDefaults() {
    return Object.assign(Source.getDefaults(), {
      detune: 0,
      frequency: 440,
      phase: 0,
      type: "pulse",
      width: 0.2
    });
  }
  /**
   * start the oscillator
   */
  _start(time) {
    time = this.toSeconds(time);
    this._triangle.start(time);
    this._widthGate.gain.setValueAtTime(1, time);
  }
  /**
   * stop the oscillator
   */
  _stop(time) {
    time = this.toSeconds(time);
    this._triangle.stop(time);
    this._widthGate.gain.cancelScheduledValues(time);
    this._widthGate.gain.setValueAtTime(0, time);
  }
  _restart(time) {
    this._triangle.restart(time);
    this._widthGate.gain.cancelScheduledValues(time);
    this._widthGate.gain.setValueAtTime(1, time);
  }
  /**
   * The phase of the oscillator in degrees.
   */
  get phase() {
    return this._triangle.phase;
  }
  set phase(phase) {
    this._triangle.phase = phase;
  }
  /**
   * The type of the oscillator. Always returns "pulse".
   */
  get type() {
    return "pulse";
  }
  /**
   * The baseType of the oscillator. Always returns "pulse".
   */
  get baseType() {
    return "pulse";
  }
  /**
   * The partials of the waveform. Cannot set partials for this waveform type
   */
  get partials() {
    return [];
  }
  /**
   * No partials for this waveform type.
   */
  get partialCount() {
    return 0;
  }
  /**
   * *Internal use* The carrier oscillator type is fed through the
   * waveshaper node to create the pulse. Using different carrier oscillators
   * changes oscillator's behavior.
   */
  set carrierType(type) {
    this._triangle.type = type;
  }
  asArray() {
    return __awaiter(this, arguments, void 0, function* (length = 1024) {
      return generateWaveform(this, length);
    });
  }
  /**
   * Clean up method.
   */
  dispose() {
    super.dispose();
    this._triangle.dispose();
    this.width.dispose();
    this._widthGate.dispose();
    this._thresh.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/oscillator/FatOscillator.js
var FatOscillator = class _FatOscillator extends Source {
  constructor() {
    const options = optionsFromArguments(_FatOscillator.getDefaults(), arguments, ["frequency", "type", "spread"]);
    super(options);
    this.name = "FatOscillator";
    this._oscillators = [];
    this.frequency = new Signal({
      context: this.context,
      units: "frequency",
      value: options.frequency
    });
    this.detune = new Signal({
      context: this.context,
      units: "cents",
      value: options.detune
    });
    this._spread = options.spread;
    this._type = options.type;
    this._phase = options.phase;
    this._partials = options.partials;
    this._partialCount = options.partialCount;
    this.count = options.count;
    readOnly(this, ["frequency", "detune"]);
  }
  static getDefaults() {
    return Object.assign(Oscillator.getDefaults(), {
      count: 3,
      spread: 20,
      type: "sawtooth"
    });
  }
  /**
   * start the oscillator
   */
  _start(time) {
    time = this.toSeconds(time);
    this._forEach((osc) => osc.start(time));
  }
  /**
   * stop the oscillator
   */
  _stop(time) {
    time = this.toSeconds(time);
    this._forEach((osc) => osc.stop(time));
  }
  _restart(time) {
    this._forEach((osc) => osc.restart(time));
  }
  /**
   * Iterate over all of the oscillators
   */
  _forEach(iterator) {
    for (let i = 0; i < this._oscillators.length; i++) {
      iterator(this._oscillators[i], i);
    }
  }
  /**
   * The type of the oscillator
   */
  get type() {
    return this._type;
  }
  set type(type) {
    this._type = type;
    this._forEach((osc) => osc.type = type);
  }
  /**
   * The detune spread between the oscillators. If "count" is
   * set to 3 oscillators and the "spread" is set to 40,
   * the three oscillators would be detuned like this: [-20, 0, 20]
   * for a total detune spread of 40 cents.
   * @example
   * const fatOsc = new Tone.FatOscillator().toDestination().start();
   * fatOsc.spread = 70;
   */
  get spread() {
    return this._spread;
  }
  set spread(spread) {
    this._spread = spread;
    if (this._oscillators.length > 1) {
      const start2 = -spread / 2;
      const step = spread / (this._oscillators.length - 1);
      this._forEach((osc, i) => osc.detune.value = start2 + step * i);
    }
  }
  /**
   * The number of detuned oscillators. Must be an integer greater than 1.
   * @example
   * const fatOsc = new Tone.FatOscillator("C#3", "sawtooth").toDestination().start();
   * // use 4 sawtooth oscillators
   * fatOsc.count = 4;
   */
  get count() {
    return this._oscillators.length;
  }
  set count(count) {
    assertRange(count, 1);
    if (this._oscillators.length !== count) {
      this._forEach((osc) => osc.dispose());
      this._oscillators = [];
      for (let i = 0; i < count; i++) {
        const osc = new Oscillator({
          context: this.context,
          volume: -6 - count * 1.1,
          type: this._type,
          phase: this._phase + i / count * 360,
          partialCount: this._partialCount,
          onstop: i === 0 ? () => this.onstop(this) : noOp
        });
        if (this.type === "custom") {
          osc.partials = this._partials;
        }
        this.frequency.connect(osc.frequency);
        this.detune.connect(osc.detune);
        osc.detune.overridden = false;
        osc.connect(this.output);
        this._oscillators[i] = osc;
      }
      this.spread = this._spread;
      if (this.state === "started") {
        this._forEach((osc) => osc.start());
      }
    }
  }
  get phase() {
    return this._phase;
  }
  set phase(phase) {
    this._phase = phase;
    this._forEach((osc, i) => osc.phase = this._phase + i / this.count * 360);
  }
  get baseType() {
    return this._oscillators[0].baseType;
  }
  set baseType(baseType) {
    this._forEach((osc) => osc.baseType = baseType);
    this._type = this._oscillators[0].type;
  }
  get partials() {
    return this._oscillators[0].partials;
  }
  set partials(partials) {
    this._partials = partials;
    this._partialCount = this._partials.length;
    if (partials.length) {
      this._type = "custom";
      this._forEach((osc) => osc.partials = partials);
    }
  }
  get partialCount() {
    return this._oscillators[0].partialCount;
  }
  set partialCount(partialCount) {
    this._partialCount = partialCount;
    this._forEach((osc) => osc.partialCount = partialCount);
    this._type = this._oscillators[0].type;
  }
  asArray() {
    return __awaiter(this, arguments, void 0, function* (length = 1024) {
      return generateWaveform(this, length);
    });
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this.frequency.dispose();
    this.detune.dispose();
    this._forEach((osc) => osc.dispose());
    return this;
  }
};

// node_modules/tone/build/esm/source/oscillator/PWMOscillator.js
var PWMOscillator = class _PWMOscillator extends Source {
  constructor() {
    const options = optionsFromArguments(_PWMOscillator.getDefaults(), arguments, ["frequency", "modulationFrequency"]);
    super(options);
    this.name = "PWMOscillator";
    this.sourceType = "pwm";
    this._scale = new Multiply({
      context: this.context,
      value: 2
    });
    this._pulse = new PulseOscillator({
      context: this.context,
      frequency: options.modulationFrequency
    });
    this._pulse.carrierType = "sine";
    this.modulationFrequency = this._pulse.frequency;
    this._modulator = new Oscillator({
      context: this.context,
      detune: options.detune,
      frequency: options.frequency,
      onstop: () => this.onstop(this),
      phase: options.phase
    });
    this.frequency = this._modulator.frequency;
    this.detune = this._modulator.detune;
    this._modulator.chain(this._scale, this._pulse.width);
    this._pulse.connect(this.output);
    readOnly(this, ["modulationFrequency", "frequency", "detune"]);
  }
  static getDefaults() {
    return Object.assign(Source.getDefaults(), {
      detune: 0,
      frequency: 440,
      modulationFrequency: 0.4,
      phase: 0,
      type: "pwm"
    });
  }
  /**
   * start the oscillator
   */
  _start(time) {
    time = this.toSeconds(time);
    this._modulator.start(time);
    this._pulse.start(time);
  }
  /**
   * stop the oscillator
   */
  _stop(time) {
    time = this.toSeconds(time);
    this._modulator.stop(time);
    this._pulse.stop(time);
  }
  /**
   * restart the oscillator
   */
  _restart(time) {
    this._modulator.restart(time);
    this._pulse.restart(time);
  }
  /**
   * The type of the oscillator. Always returns "pwm".
   */
  get type() {
    return "pwm";
  }
  /**
   * The baseType of the oscillator. Always returns "pwm".
   */
  get baseType() {
    return "pwm";
  }
  /**
   * The partials of the waveform. Cannot set partials for this waveform type
   */
  get partials() {
    return [];
  }
  /**
   * No partials for this waveform type.
   */
  get partialCount() {
    return 0;
  }
  /**
   * The phase of the oscillator in degrees.
   */
  get phase() {
    return this._modulator.phase;
  }
  set phase(phase) {
    this._modulator.phase = phase;
  }
  asArray() {
    return __awaiter(this, arguments, void 0, function* (length = 1024) {
      return generateWaveform(this, length);
    });
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._pulse.dispose();
    this._scale.dispose();
    this._modulator.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/source/oscillator/OmniOscillator.js
var OmniOscillatorSourceMap = {
  am: AMOscillator,
  fat: FatOscillator,
  fm: FMOscillator,
  oscillator: Oscillator,
  pulse: PulseOscillator,
  pwm: PWMOscillator
};
var OmniOscillator = class _OmniOscillator extends Source {
  constructor() {
    const options = optionsFromArguments(_OmniOscillator.getDefaults(), arguments, ["frequency", "type"]);
    super(options);
    this.name = "OmniOscillator";
    this.frequency = new Signal({
      context: this.context,
      units: "frequency",
      value: options.frequency
    });
    this.detune = new Signal({
      context: this.context,
      units: "cents",
      value: options.detune
    });
    readOnly(this, ["frequency", "detune"]);
    this.set(options);
  }
  static getDefaults() {
    return Object.assign(Oscillator.getDefaults(), FMOscillator.getDefaults(), AMOscillator.getDefaults(), FatOscillator.getDefaults(), PulseOscillator.getDefaults(), PWMOscillator.getDefaults());
  }
  /**
   * start the oscillator
   */
  _start(time) {
    this._oscillator.start(time);
  }
  /**
   * start the oscillator
   */
  _stop(time) {
    this._oscillator.stop(time);
  }
  _restart(time) {
    this._oscillator.restart(time);
    return this;
  }
  /**
   * The type of the oscillator. Can be any of the basic types: sine, square, triangle, sawtooth. Or
   * prefix the basic types with "fm", "am", or "fat" to use the FMOscillator, AMOscillator or FatOscillator
   * types. The oscillator could also be set to "pwm" or "pulse". All of the parameters of the
   * oscillator's class are accessible when the oscillator is set to that type, but throws an error
   * when it's not.
   * @example
   * const omniOsc = new Tone.OmniOscillator().toDestination().start();
   * omniOsc.type = "pwm";
   * // modulationFrequency is parameter which is available
   * // only when the type is "pwm".
   * omniOsc.modulationFrequency.value = 0.5;
   */
  get type() {
    let prefix = "";
    if (["am", "fm", "fat"].some((p) => this._sourceType === p)) {
      prefix = this._sourceType;
    }
    return prefix + this._oscillator.type;
  }
  set type(type) {
    if (type.substr(0, 2) === "fm") {
      this._createNewOscillator("fm");
      this._oscillator = this._oscillator;
      this._oscillator.type = type.substr(2);
    } else if (type.substr(0, 2) === "am") {
      this._createNewOscillator("am");
      this._oscillator = this._oscillator;
      this._oscillator.type = type.substr(2);
    } else if (type.substr(0, 3) === "fat") {
      this._createNewOscillator("fat");
      this._oscillator = this._oscillator;
      this._oscillator.type = type.substr(3);
    } else if (type === "pwm") {
      this._createNewOscillator("pwm");
      this._oscillator = this._oscillator;
    } else if (type === "pulse") {
      this._createNewOscillator("pulse");
    } else {
      this._createNewOscillator("oscillator");
      this._oscillator = this._oscillator;
      this._oscillator.type = type;
    }
  }
  /**
   * The value is an empty array when the type is not "custom".
   * This is not available on "pwm" and "pulse" oscillator types.
   * @see {@link Oscillator.partials}
   */
  get partials() {
    return this._oscillator.partials;
  }
  set partials(partials) {
    if (!this._getOscType(this._oscillator, "pulse") && !this._getOscType(this._oscillator, "pwm")) {
      this._oscillator.partials = partials;
    }
  }
  get partialCount() {
    return this._oscillator.partialCount;
  }
  set partialCount(partialCount) {
    if (!this._getOscType(this._oscillator, "pulse") && !this._getOscType(this._oscillator, "pwm")) {
      this._oscillator.partialCount = partialCount;
    }
  }
  set(props) {
    if (Reflect.has(props, "type") && props.type) {
      this.type = props.type;
    }
    super.set(props);
    return this;
  }
  /**
   * connect the oscillator to the frequency and detune signals
   */
  _createNewOscillator(oscType) {
    if (oscType !== this._sourceType) {
      this._sourceType = oscType;
      const OscConstructor = OmniOscillatorSourceMap[oscType];
      const now = this.now();
      if (this._oscillator) {
        const oldOsc = this._oscillator;
        oldOsc.stop(now);
        this.context.setTimeout(() => oldOsc.dispose(), this.blockTime);
      }
      this._oscillator = new OscConstructor({
        context: this.context
      });
      this.frequency.connect(this._oscillator.frequency);
      this.detune.connect(this._oscillator.detune);
      this._oscillator.connect(this.output);
      this._oscillator.onstop = () => this.onstop(this);
      if (this.state === "started") {
        this._oscillator.start(now);
      }
    }
  }
  get phase() {
    return this._oscillator.phase;
  }
  set phase(phase) {
    this._oscillator.phase = phase;
  }
  /**
   * The source type of the oscillator.
   * @example
   * const omniOsc = new Tone.OmniOscillator(440, "fmsquare");
   * console.log(omniOsc.sourceType); // 'fm'
   */
  get sourceType() {
    return this._sourceType;
  }
  set sourceType(sType) {
    let baseType = "sine";
    if (this._oscillator.type !== "pwm" && this._oscillator.type !== "pulse") {
      baseType = this._oscillator.type;
    }
    if (sType === "fm") {
      this.type = "fm" + baseType;
    } else if (sType === "am") {
      this.type = "am" + baseType;
    } else if (sType === "fat") {
      this.type = "fat" + baseType;
    } else if (sType === "oscillator") {
      this.type = baseType;
    } else if (sType === "pulse") {
      this.type = "pulse";
    } else if (sType === "pwm") {
      this.type = "pwm";
    }
  }
  _getOscType(osc, sourceType) {
    return osc instanceof OmniOscillatorSourceMap[sourceType];
  }
  /**
   * The base type of the oscillator.
   * @see {@link Oscillator.baseType}
   * @example
   * const omniOsc = new Tone.OmniOscillator(440, "fmsquare4");
   * console.log(omniOsc.sourceType, omniOsc.baseType, omniOsc.partialCount);
   */
  get baseType() {
    return this._oscillator.baseType;
  }
  set baseType(baseType) {
    if (!this._getOscType(this._oscillator, "pulse") && !this._getOscType(this._oscillator, "pwm") && baseType !== "pulse" && baseType !== "pwm") {
      this._oscillator.baseType = baseType;
    }
  }
  /**
   * The width of the oscillator when sourceType === "pulse".
   * @see {@link PWMOscillator}
   */
  get width() {
    if (this._getOscType(this._oscillator, "pulse")) {
      return this._oscillator.width;
    } else {
      return void 0;
    }
  }
  /**
   * The number of detuned oscillators when sourceType === "fat".
   * @see {@link FatOscillator.count}
   */
  get count() {
    if (this._getOscType(this._oscillator, "fat")) {
      return this._oscillator.count;
    } else {
      return void 0;
    }
  }
  set count(count) {
    if (this._getOscType(this._oscillator, "fat") && isNumber(count)) {
      this._oscillator.count = count;
    }
  }
  /**
   * The detune spread between the oscillators when sourceType === "fat".
   * @see {@link FatOscillator.count}
   */
  get spread() {
    if (this._getOscType(this._oscillator, "fat")) {
      return this._oscillator.spread;
    } else {
      return void 0;
    }
  }
  set spread(spread) {
    if (this._getOscType(this._oscillator, "fat") && isNumber(spread)) {
      this._oscillator.spread = spread;
    }
  }
  /**
   * The type of the modulator oscillator. Only if the oscillator is set to "am" or "fm" types.
   * @see {@link AMOscillator} or {@link FMOscillator}
   */
  get modulationType() {
    if (this._getOscType(this._oscillator, "fm") || this._getOscType(this._oscillator, "am")) {
      return this._oscillator.modulationType;
    } else {
      return void 0;
    }
  }
  set modulationType(mType) {
    if ((this._getOscType(this._oscillator, "fm") || this._getOscType(this._oscillator, "am")) && isString(mType)) {
      this._oscillator.modulationType = mType;
    }
  }
  /**
   * The modulation index when the sourceType === "fm"
   * @see {@link FMOscillator}.
   */
  get modulationIndex() {
    if (this._getOscType(this._oscillator, "fm")) {
      return this._oscillator.modulationIndex;
    } else {
      return void 0;
    }
  }
  /**
   * Harmonicity is the frequency ratio between the carrier and the modulator oscillators.
   * @see {@link AMOscillator} or {@link FMOscillator}
   */
  get harmonicity() {
    if (this._getOscType(this._oscillator, "fm") || this._getOscType(this._oscillator, "am")) {
      return this._oscillator.harmonicity;
    } else {
      return void 0;
    }
  }
  /**
   * The modulationFrequency Signal of the oscillator when sourceType === "pwm"
   * see {@link PWMOscillator}
   * @min 0.1
   * @max 5
   */
  get modulationFrequency() {
    if (this._getOscType(this._oscillator, "pwm")) {
      return this._oscillator.modulationFrequency;
    } else {
      return void 0;
    }
  }
  asArray() {
    return __awaiter(this, arguments, void 0, function* (length = 1024) {
      return generateWaveform(this, length);
    });
  }
  dispose() {
    super.dispose();
    this.detune.dispose();
    this.frequency.dispose();
    this._oscillator.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/signal/Add.js
var Add = class _Add extends Signal {
  constructor() {
    super(optionsFromArguments(_Add.getDefaults(), arguments, ["value"]));
    this.override = false;
    this.name = "Add";
    this._sum = new Gain({ context: this.context });
    this.input = this._sum;
    this.output = this._sum;
    this.addend = this._param;
    connectSeries(this._constantSource, this._sum);
  }
  static getDefaults() {
    return Object.assign(Signal.getDefaults(), {
      value: 0
    });
  }
  dispose() {
    super.dispose();
    this._sum.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/signal/Scale.js
var Scale = class _Scale extends SignalOperator {
  constructor() {
    const options = optionsFromArguments(_Scale.getDefaults(), arguments, [
      "min",
      "max"
    ]);
    super(options);
    this.name = "Scale";
    this._mult = this.input = new Multiply({
      context: this.context,
      value: options.max - options.min
    });
    this._add = this.output = new Add({
      context: this.context,
      value: options.min
    });
    this._min = options.min;
    this._max = options.max;
    this.input.connect(this.output);
  }
  static getDefaults() {
    return Object.assign(SignalOperator.getDefaults(), {
      max: 1,
      min: 0
    });
  }
  /**
   * The minimum output value. This number is output when the value input value is 0.
   */
  get min() {
    return this._min;
  }
  set min(min) {
    this._min = min;
    this._setRange();
  }
  /**
   * The maximum output value. This number is output when the value input value is 1.
   */
  get max() {
    return this._max;
  }
  set max(max) {
    this._max = max;
    this._setRange();
  }
  /**
   * set the values
   */
  _setRange() {
    this._add.value = this._min;
    this._mult.value = this._max - this._min;
  }
  dispose() {
    super.dispose();
    this._add.dispose();
    this._mult.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/signal/Zero.js
var Zero = class _Zero extends SignalOperator {
  constructor() {
    super(optionsFromArguments(_Zero.getDefaults(), arguments));
    this.name = "Zero";
    this._gain = new Gain({ context: this.context });
    this.output = this._gain;
    this.input = void 0;
    connect(this.context.getConstant(0), this._gain);
  }
  /**
   * clean up
   */
  dispose() {
    super.dispose();
    disconnect(this.context.getConstant(0), this._gain);
    return this;
  }
};

// node_modules/tone/build/esm/source/oscillator/LFO.js
var LFO = class _LFO extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_LFO.getDefaults(), arguments, [
      "frequency",
      "min",
      "max"
    ]);
    super(options);
    this.name = "LFO";
    this._stoppedValue = 0;
    this._units = "number";
    this.convert = true;
    this._fromType = Param.prototype._fromType;
    this._toType = Param.prototype._toType;
    this._is = Param.prototype._is;
    this._clampValue = Param.prototype._clampValue;
    this._oscillator = new Oscillator(options);
    this.frequency = this._oscillator.frequency;
    this._amplitudeGain = new Gain({
      context: this.context,
      gain: options.amplitude,
      units: "normalRange"
    });
    this.amplitude = this._amplitudeGain.gain;
    this._stoppedSignal = new Signal({
      context: this.context,
      units: "audioRange",
      value: 0
    });
    this._zeros = new Zero({ context: this.context });
    this._a2g = new AudioToGain({ context: this.context });
    this._scaler = this.output = new Scale({
      context: this.context,
      max: options.max,
      min: options.min
    });
    this.units = options.units;
    this.min = options.min;
    this.max = options.max;
    this._oscillator.chain(this._amplitudeGain, this._a2g, this._scaler);
    this._zeros.connect(this._a2g);
    this._stoppedSignal.connect(this._a2g);
    readOnly(this, ["amplitude", "frequency"]);
    this.phase = options.phase;
  }
  static getDefaults() {
    return Object.assign(Oscillator.getDefaults(), {
      amplitude: 1,
      frequency: "4n",
      max: 1,
      min: 0,
      type: "sine",
      units: "number"
    });
  }
  /**
   * Start the LFO.
   * @param time The time the LFO will start
   */
  start(time) {
    time = this.toSeconds(time);
    this._stoppedSignal.setValueAtTime(0, time);
    this._oscillator.start(time);
    return this;
  }
  /**
   * Stop the LFO.
   * @param  time The time the LFO will stop
   */
  stop(time) {
    time = this.toSeconds(time);
    this._stoppedSignal.setValueAtTime(this._stoppedValue, time);
    this._oscillator.stop(time);
    return this;
  }
  /**
   * Sync the start/stop/pause to the transport
   * and the frequency to the bpm of the transport
   * @example
   * const lfo = new Tone.LFO("8n");
   * lfo.sync().start(0);
   * // the rate of the LFO will always be an eighth note, even as the tempo changes
   */
  sync() {
    this._oscillator.sync();
    this._oscillator.syncFrequency();
    return this;
  }
  /**
   * unsync the LFO from transport control
   */
  unsync() {
    this._oscillator.unsync();
    this._oscillator.unsyncFrequency();
    return this;
  }
  /**
   * After the oscillator waveform is updated, reset the `_stoppedSignal` value to match the updated waveform
   */
  _setStoppedValue() {
    this._stoppedValue = this._oscillator.getInitialValue();
    this._stoppedSignal.value = this._stoppedValue;
  }
  /**
   * The minimum output of the LFO.
   */
  get min() {
    return this._toType(this._scaler.min);
  }
  set min(min) {
    min = this._fromType(min);
    this._scaler.min = min;
  }
  /**
   * The maximum output of the LFO.
   */
  get max() {
    return this._toType(this._scaler.max);
  }
  set max(max) {
    max = this._fromType(max);
    this._scaler.max = max;
  }
  /**
   * The type of the oscillator.
   * @see {@link Oscillator.type}
   */
  get type() {
    return this._oscillator.type;
  }
  set type(type) {
    this._oscillator.type = type;
    this._setStoppedValue();
  }
  /**
   * The oscillator's partials array.
   * @see {@link Oscillator.partials}
   */
  get partials() {
    return this._oscillator.partials;
  }
  set partials(partials) {
    this._oscillator.partials = partials;
    this._setStoppedValue();
  }
  /**
   * The phase of the LFO.
   */
  get phase() {
    return this._oscillator.phase;
  }
  set phase(phase) {
    this._oscillator.phase = phase;
    this._setStoppedValue();
  }
  /**
   * The output units of the LFO.
   */
  get units() {
    return this._units;
  }
  set units(val) {
    const currentMin = this.min;
    const currentMax = this.max;
    this._units = val;
    this.min = currentMin;
    this.max = currentMax;
  }
  /**
   * Returns the playback state of the source, either "started" or "stopped".
   */
  get state() {
    return this._oscillator.state;
  }
  /**
   * @param node the destination to connect to
   * @param outputNum the optional output number
   * @param inputNum the input number
   */
  connect(node, outputNum, inputNum) {
    if (node instanceof Param || node instanceof Signal) {
      this.convert = node.convert;
      this.units = node.units;
    }
    connectSignal(this, node, outputNum, inputNum);
    return this;
  }
  dispose() {
    super.dispose();
    this._oscillator.dispose();
    this._stoppedSignal.dispose();
    this._zeros.dispose();
    this._scaler.dispose();
    this._a2g.dispose();
    this._amplitudeGain.dispose();
    this.amplitude.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/core/util/Decorator.js
function range(min, max = Infinity) {
  const valueMap = /* @__PURE__ */ new WeakMap();
  return function(target, propertyKey) {
    Reflect.defineProperty(target, propertyKey, {
      configurable: true,
      enumerable: true,
      get: function() {
        return valueMap.get(this);
      },
      set: function(newValue) {
        assertRange(newValue, min, max);
        valueMap.set(this, newValue);
      }
    });
  };
}
function timeRange(min, max = Infinity) {
  const valueMap = /* @__PURE__ */ new WeakMap();
  return function(target, propertyKey) {
    Reflect.defineProperty(target, propertyKey, {
      configurable: true,
      enumerable: true,
      get: function() {
        return valueMap.get(this);
      },
      set: function(newValue) {
        assertRange(this.toSeconds(newValue), min, max);
        valueMap.set(this, newValue);
      }
    });
  };
}

// node_modules/tone/build/esm/source/buffer/Player.js
var Player = class _Player extends Source {
  constructor() {
    const options = optionsFromArguments(_Player.getDefaults(), arguments, [
      "url",
      "onload"
    ]);
    super(options);
    this.name = "Player";
    this._activeSources = /* @__PURE__ */ new Set();
    this._buffer = new ToneAudioBuffer({
      onload: this._onload.bind(this, options.onload),
      onerror: options.onerror,
      reverse: options.reverse,
      url: options.url
    });
    this.autostart = options.autostart;
    this._loop = options.loop;
    this._loopStart = options.loopStart;
    this._loopEnd = options.loopEnd;
    this._playbackRate = options.playbackRate;
    this.fadeIn = options.fadeIn;
    this.fadeOut = options.fadeOut;
  }
  static getDefaults() {
    return Object.assign(Source.getDefaults(), {
      autostart: false,
      fadeIn: 0,
      fadeOut: 0,
      loop: false,
      loopEnd: 0,
      loopStart: 0,
      onload: noOp,
      onerror: noOp,
      playbackRate: 1,
      reverse: false
    });
  }
  /**
   * Load the audio file as an audio buffer.
   * Decodes the audio asynchronously and invokes
   * the callback once the audio buffer loads.
   * Note: this does not need to be called if a url
   * was passed in to the constructor. Only use this
   * if you want to manually load a new url.
   * @param url The url of the buffer to load. Filetype support depends on the browser.
   */
  load(url) {
    return __awaiter(this, void 0, void 0, function* () {
      yield this._buffer.load(url);
      this._onload();
      return this;
    });
  }
  /**
   * Internal callback when the buffer is loaded.
   */
  _onload(callback = noOp) {
    callback();
    if (this.autostart) {
      this.start();
    }
  }
  /**
   * Internal callback when the buffer is done playing.
   */
  _onSourceEnd(source) {
    this.onstop(this);
    this._activeSources.delete(source);
    if (this._activeSources.size === 0 && !this._synced && this._state.getValueAtTime(this.now()) === "started") {
      this._state.cancel(this.now());
      this._state.setStateAtTime("stopped", this.now());
    }
  }
  /**
   * Play the buffer at the given startTime. Optionally add an offset
   * and/or duration which will play the buffer from a position
   * within the buffer for the given duration.
   *
   * @param  time When the player should start.
   * @param  offset The offset from the beginning of the sample to start at.
   * @param  duration How long the sample should play. If no duration is given, it will default to the full length of the sample (minus any offset)
   */
  start(time, offset, duration) {
    super.start(time, offset, duration);
    return this;
  }
  /**
   * Internal start method
   */
  _start(startTime, offset, duration) {
    if (this._loop) {
      offset = defaultArg(offset, this._loopStart);
    } else {
      offset = defaultArg(offset, 0);
    }
    const computedOffset = this.toSeconds(offset);
    const origDuration = duration;
    duration = defaultArg(duration, Math.max(this._buffer.duration - computedOffset, 0));
    let computedDuration = this.toSeconds(duration);
    computedDuration = computedDuration / this._playbackRate;
    startTime = this.toSeconds(startTime);
    const source = new ToneBufferSource({
      url: this._buffer,
      context: this.context,
      fadeIn: this.fadeIn,
      fadeOut: this.fadeOut,
      loop: this._loop,
      loopEnd: this._loopEnd,
      loopStart: this._loopStart,
      onended: this._onSourceEnd.bind(this),
      playbackRate: this._playbackRate
    }).connect(this.output);
    if (!this._loop && !this._synced) {
      this._state.cancel(startTime + computedDuration);
      this._state.setStateAtTime("stopped", startTime + computedDuration, {
        implicitEnd: true
      });
    }
    this._activeSources.add(source);
    if (this._loop && isUndef(origDuration)) {
      source.start(startTime, computedOffset);
    } else {
      source.start(startTime, computedOffset, computedDuration - this.toSeconds(this.fadeOut));
    }
  }
  /**
   * Stop playback.
   */
  _stop(time) {
    const computedTime = this.toSeconds(time);
    this._activeSources.forEach((source) => source.stop(computedTime));
  }
  /**
   * Stop and then restart the player from the beginning (or offset)
   * @param  time When the player should start.
   * @param  offset The offset from the beginning of the sample to start at.
   * @param  duration How long the sample should play. If no duration is given,
   * 					it will default to the full length of the sample (minus any offset)
   */
  restart(time, offset, duration) {
    super.restart(time, offset, duration);
    return this;
  }
  _restart(time, offset, duration) {
    var _a;
    (_a = [...this._activeSources].pop()) === null || _a === void 0 ? void 0 : _a.stop(time);
    this._start(time, offset, duration);
  }
  /**
   * Seek to a specific time in the player's buffer. If the
   * source is no longer playing at that time, it will stop.
   * @param offset The time to seek to.
   * @param when The time for the seek event to occur.
   * @example
   * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/gurgling_theremin_1.mp3", () => {
   * 	player.start();
   * 	// seek to the offset in 1 second from now
   * 	player.seek(0.4, "+1");
   * }).toDestination();
   */
  seek(offset, when) {
    const computedTime = this.toSeconds(when);
    if (this._state.getValueAtTime(computedTime) === "started") {
      const computedOffset = this.toSeconds(offset);
      this._stop(computedTime);
      this._start(computedTime, computedOffset);
    }
    return this;
  }
  /**
   * Set the loop start and end. Will only loop if loop is set to true.
   * @param loopStart The loop start time
   * @param loopEnd The loop end time
   * @example
   * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/malevoices_aa2_F3.mp3").toDestination();
   * // loop between the given points
   * player.setLoopPoints(0.2, 0.3);
   * player.loop = true;
   * player.autostart = true;
   */
  setLoopPoints(loopStart, loopEnd) {
    this.loopStart = loopStart;
    this.loopEnd = loopEnd;
    return this;
  }
  /**
   * If loop is true, the loop will start at this position.
   */
  get loopStart() {
    return this._loopStart;
  }
  set loopStart(loopStart) {
    this._loopStart = loopStart;
    if (this.buffer.loaded) {
      assertRange(this.toSeconds(loopStart), 0, this.buffer.duration);
    }
    this._activeSources.forEach((source) => {
      source.loopStart = loopStart;
    });
  }
  /**
   * If loop is true, the loop will end at this position.
   */
  get loopEnd() {
    return this._loopEnd;
  }
  set loopEnd(loopEnd) {
    this._loopEnd = loopEnd;
    if (this.buffer.loaded) {
      assertRange(this.toSeconds(loopEnd), 0, this.buffer.duration);
    }
    this._activeSources.forEach((source) => {
      source.loopEnd = loopEnd;
    });
  }
  /**
   * The audio buffer belonging to the player.
   */
  get buffer() {
    return this._buffer;
  }
  set buffer(buffer) {
    this._buffer.set(buffer);
  }
  /**
   * If the buffer should loop once it's over.
   * @example
   * const player = new Tone.Player("https://tonejs.github.io/audio/drum-samples/breakbeat.mp3").toDestination();
   * player.loop = true;
   * player.autostart = true;
   */
  get loop() {
    return this._loop;
  }
  set loop(loop) {
    if (this._loop === loop) {
      return;
    }
    this._loop = loop;
    this._activeSources.forEach((source) => {
      source.loop = loop;
    });
    if (loop) {
      const stopEvent = this._state.getNextState("stopped", this.now());
      if (stopEvent) {
        this._state.cancel(stopEvent.time);
      }
    }
  }
  /**
   * Normal speed is 1. The pitch will change with the playback rate.
   * @example
   * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/femalevoices_aa2_A5.mp3").toDestination();
   * // play at 1/4 speed
   * player.playbackRate = 0.25;
   * // play as soon as the buffer is loaded
   * player.autostart = true;
   */
  get playbackRate() {
    return this._playbackRate;
  }
  set playbackRate(rate) {
    this._playbackRate = rate;
    const now = this.now();
    const stopEvent = this._state.getNextState("stopped", now);
    if (stopEvent && stopEvent.implicitEnd) {
      this._state.cancel(stopEvent.time);
      this._activeSources.forEach((source) => source.cancelStop());
    }
    this._activeSources.forEach((source) => {
      source.playbackRate.setValueAtTime(rate, now);
    });
  }
  /**
   * If the buffer should be reversed. Note that this sets the underlying {@link ToneAudioBuffer.reverse}, so
   * if multiple players are pointing at the same ToneAudioBuffer, they will all be reversed.
   * @example
   * const player = new Tone.Player("https://tonejs.github.io/audio/berklee/chime_1.mp3").toDestination();
   * player.autostart = true;
   * player.reverse = true;
   */
  get reverse() {
    return this._buffer.reverse;
  }
  set reverse(rev) {
    this._buffer.reverse = rev;
  }
  /**
   * If the buffer is loaded
   */
  get loaded() {
    return this._buffer.loaded;
  }
  dispose() {
    super.dispose();
    this._activeSources.forEach((source) => source.dispose());
    this._activeSources.clear();
    this._buffer.dispose();
    return this;
  }
};
__decorate([
  timeRange(0)
], Player.prototype, "fadeIn", void 0);
__decorate([
  timeRange(0)
], Player.prototype, "fadeOut", void 0);

// node_modules/tone/build/esm/signal/GainToAudio.js
var GainToAudio = class extends SignalOperator {
  constructor() {
    super(...arguments);
    this.name = "GainToAudio";
    this._norm = new WaveShaper({
      context: this.context,
      mapping: (x) => Math.abs(x) * 2 - 1
    });
    this.input = this._norm;
    this.output = this._norm;
  }
  /**
   * clean up
   */
  dispose() {
    super.dispose();
    this._norm.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/envelope/Envelope.js
var Envelope = class _Envelope extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Envelope.getDefaults(), arguments, ["attack", "decay", "sustain", "release"]);
    super(options);
    this.name = "Envelope";
    this._sig = new Signal({
      context: this.context,
      value: 0
    });
    this.output = this._sig;
    this.input = void 0;
    this.attack = options.attack;
    this.decay = options.decay;
    this.sustain = options.sustain;
    this.release = options.release;
    this.attackCurve = options.attackCurve;
    this.releaseCurve = options.releaseCurve;
    this.decayCurve = options.decayCurve;
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      attack: 0.01,
      attackCurve: "linear",
      decay: 0.1,
      decayCurve: "exponential",
      release: 1,
      releaseCurve: "exponential",
      sustain: 0.5
    });
  }
  /**
   * Read the current value of the envelope. Useful for
   * synchronizing visual output to the envelope.
   */
  get value() {
    return this.getValueAtTime(this.now());
  }
  /**
   * Get the curve
   * @param  curve
   * @param  direction  In/Out
   * @return The curve name
   */
  _getCurve(curve, direction) {
    if (isString(curve)) {
      return curve;
    } else {
      let curveName;
      for (curveName in EnvelopeCurves) {
        if (EnvelopeCurves[curveName][direction] === curve) {
          return curveName;
        }
      }
      return curve;
    }
  }
  /**
   * Assign a the curve to the given name using the direction
   * @param  name
   * @param  direction In/Out
   * @param  curve
   */
  _setCurve(name, direction, curve) {
    if (isString(curve) && Reflect.has(EnvelopeCurves, curve)) {
      const curveDef = EnvelopeCurves[curve];
      if (isObject(curveDef)) {
        if (name !== "_decayCurve") {
          this[name] = curveDef[direction];
        }
      } else {
        this[name] = curveDef;
      }
    } else if (isArray(curve) && name !== "_decayCurve") {
      this[name] = curve;
    } else {
      throw new Error("Envelope: invalid curve: " + curve);
    }
  }
  /**
   * The shape of the attack.
   * Can be any of these strings:
   * * "linear"
   * * "exponential"
   * * "sine"
   * * "cosine"
   * * "bounce"
   * * "ripple"
   * * "step"
   *
   * Can also be an array which describes the curve. Values
   * in the array are evenly subdivided and linearly
   * interpolated over the duration of the attack.
   * @example
   * return Tone.Offline(() => {
   * 	const env = new Tone.Envelope(0.4).toDestination();
   * 	env.attackCurve = "linear";
   * 	env.triggerAttack();
   * }, 1, 1);
   */
  get attackCurve() {
    return this._getCurve(this._attackCurve, "In");
  }
  set attackCurve(curve) {
    this._setCurve("_attackCurve", "In", curve);
  }
  /**
   * The shape of the release. See the attack curve types.
   * @example
   * return Tone.Offline(() => {
   * 	const env = new Tone.Envelope({
   * 		release: 0.8
   * 	}).toDestination();
   * 	env.triggerAttack();
   * 	// release curve could also be defined by an array
   * 	env.releaseCurve = [1, 0.3, 0.4, 0.2, 0.7, 0];
   * 	env.triggerRelease(0.2);
   * }, 1, 1);
   */
  get releaseCurve() {
    return this._getCurve(this._releaseCurve, "Out");
  }
  set releaseCurve(curve) {
    this._setCurve("_releaseCurve", "Out", curve);
  }
  /**
   * The shape of the decay either "linear" or "exponential"
   * @example
   * return Tone.Offline(() => {
   * 	const env = new Tone.Envelope({
   * 		sustain: 0.1,
   * 		decay: 0.5
   * 	}).toDestination();
   * 	env.decayCurve = "linear";
   * 	env.triggerAttack();
   * }, 1, 1);
   */
  get decayCurve() {
    return this._getCurve(this._decayCurve, "Out");
  }
  set decayCurve(curve) {
    this._setCurve("_decayCurve", "Out", curve);
  }
  /**
   * Trigger the attack/decay portion of the ADSR envelope.
   * @param  time When the attack should start.
   * @param velocity The velocity of the envelope scales the vales.
   *                             number between 0-1
   * @example
   * const env = new Tone.AmplitudeEnvelope().toDestination();
   * const osc = new Tone.Oscillator().connect(env).start();
   * // trigger the attack 0.5 seconds from now with a velocity of 0.2
   * env.triggerAttack("+0.5", 0.2);
   */
  triggerAttack(time, velocity = 1) {
    this.log("triggerAttack", time, velocity);
    time = this.toSeconds(time);
    const originalAttack = this.toSeconds(this.attack);
    let attack = originalAttack;
    const decay = this.toSeconds(this.decay);
    const currentValue = this.getValueAtTime(time);
    if (currentValue > 0) {
      const attackRate = 1 / attack;
      const remainingDistance = 1 - currentValue;
      attack = remainingDistance / attackRate;
    }
    if (attack < this.sampleTime) {
      this._sig.cancelScheduledValues(time);
      this._sig.setValueAtTime(velocity, time);
    } else if (this._attackCurve === "linear") {
      this._sig.linearRampTo(velocity, attack, time);
    } else if (this._attackCurve === "exponential") {
      this._sig.targetRampTo(velocity, attack, time);
    } else {
      this._sig.cancelAndHoldAtTime(time);
      let curve = this._attackCurve;
      for (let i = 1; i < curve.length; i++) {
        if (curve[i - 1] <= currentValue && currentValue <= curve[i]) {
          curve = this._attackCurve.slice(i);
          curve[0] = currentValue;
          break;
        }
      }
      this._sig.setValueCurveAtTime(curve, time, attack, velocity);
    }
    if (decay && this.sustain < 1) {
      const decayValue = velocity * this.sustain;
      const decayStart = time + attack;
      this.log("decay", decayStart);
      if (this._decayCurve === "linear") {
        this._sig.linearRampToValueAtTime(decayValue, decay + decayStart);
      } else {
        this._sig.exponentialApproachValueAtTime(decayValue, decayStart, decay);
      }
    }
    return this;
  }
  /**
   * Triggers the release of the envelope.
   * @param  time When the release portion of the envelope should start.
   * @example
   * const env = new Tone.AmplitudeEnvelope().toDestination();
   * const osc = new Tone.Oscillator({
   * 	type: "sawtooth"
   * }).connect(env).start();
   * env.triggerAttack();
   * // trigger the release half a second after the attack
   * env.triggerRelease("+0.5");
   */
  triggerRelease(time) {
    this.log("triggerRelease", time);
    time = this.toSeconds(time);
    const currentValue = this.getValueAtTime(time);
    if (currentValue > 0) {
      const release = this.toSeconds(this.release);
      if (release < this.sampleTime) {
        this._sig.setValueAtTime(0, time);
      } else if (this._releaseCurve === "linear") {
        this._sig.linearRampTo(0, release, time);
      } else if (this._releaseCurve === "exponential") {
        this._sig.targetRampTo(0, release, time);
      } else {
        assert(isArray(this._releaseCurve), "releaseCurve must be either 'linear', 'exponential' or an array");
        this._sig.cancelAndHoldAtTime(time);
        this._sig.setValueCurveAtTime(this._releaseCurve, time, release, currentValue);
      }
    }
    return this;
  }
  /**
   * Get the scheduled value at the given time. This will
   * return the unconverted (raw) value.
   * @example
   * const env = new Tone.Envelope(0.5, 1, 0.4, 2);
   * env.triggerAttackRelease(2);
   * setInterval(() => console.log(env.getValueAtTime(Tone.now())), 100);
   */
  getValueAtTime(time) {
    return this._sig.getValueAtTime(time);
  }
  /**
   * triggerAttackRelease is shorthand for triggerAttack, then waiting
   * some duration, then triggerRelease.
   * @param duration The duration of the sustain.
   * @param time When the attack should be triggered.
   * @param velocity The velocity of the envelope.
   * @example
   * const env = new Tone.AmplitudeEnvelope().toDestination();
   * const osc = new Tone.Oscillator().connect(env).start();
   * // trigger the release 0.5 seconds after the attack
   * env.triggerAttackRelease(0.5);
   */
  triggerAttackRelease(duration, time, velocity = 1) {
    time = this.toSeconds(time);
    this.triggerAttack(time, velocity);
    this.triggerRelease(time + this.toSeconds(duration));
    return this;
  }
  /**
   * Cancels all scheduled envelope changes after the given time.
   */
  cancel(after) {
    this._sig.cancelScheduledValues(this.toSeconds(after));
    return this;
  }
  /**
   * Connect the envelope to a destination node.
   */
  connect(destination, outputNumber = 0, inputNumber = 0) {
    connectSignal(this, destination, outputNumber, inputNumber);
    return this;
  }
  /**
   * Render the envelope curve to an array of the given length.
   * Good for visualizing the envelope curve. Rescales the duration of the
   * envelope to fit the length.
   */
  asArray() {
    return __awaiter(this, arguments, void 0, function* (length = 1024) {
      const duration = length / this.context.sampleRate;
      const context2 = new OfflineContext(1, duration, this.context.sampleRate);
      const attackPortion = this.toSeconds(this.attack) + this.toSeconds(this.decay);
      const envelopeDuration = attackPortion + this.toSeconds(this.release);
      const sustainTime = envelopeDuration * 0.1;
      const totalDuration = envelopeDuration + sustainTime;
      const clone = new this.constructor(Object.assign(this.get(), {
        attack: duration * this.toSeconds(this.attack) / totalDuration,
        decay: duration * this.toSeconds(this.decay) / totalDuration,
        release: duration * this.toSeconds(this.release) / totalDuration,
        context: context2
      }));
      clone._sig.toDestination();
      clone.triggerAttackRelease(duration * (attackPortion + sustainTime) / totalDuration, 0);
      const buffer = yield context2.render();
      return buffer.getChannelData(0);
    });
  }
  dispose() {
    super.dispose();
    this._sig.dispose();
    return this;
  }
};
__decorate([
  timeRange(0)
], Envelope.prototype, "attack", void 0);
__decorate([
  timeRange(0)
], Envelope.prototype, "decay", void 0);
__decorate([
  range(0, 1)
], Envelope.prototype, "sustain", void 0);
__decorate([
  timeRange(0)
], Envelope.prototype, "release", void 0);
var EnvelopeCurves = (() => {
  const curveLen = 128;
  let i;
  let k;
  const cosineCurve = [];
  for (i = 0; i < curveLen; i++) {
    cosineCurve[i] = Math.sin(i / (curveLen - 1) * (Math.PI / 2));
  }
  const rippleCurve = [];
  const rippleCurveFreq = 6.4;
  for (i = 0; i < curveLen - 1; i++) {
    k = i / (curveLen - 1);
    const sineWave = Math.sin(k * (Math.PI * 2) * rippleCurveFreq - Math.PI / 2) + 1;
    rippleCurve[i] = sineWave / 10 + k * 0.83;
  }
  rippleCurve[curveLen - 1] = 1;
  const stairsCurve = [];
  const steps = 5;
  for (i = 0; i < curveLen; i++) {
    stairsCurve[i] = Math.ceil(i / (curveLen - 1) * steps) / steps;
  }
  const sineCurve = [];
  for (i = 0; i < curveLen; i++) {
    k = i / (curveLen - 1);
    sineCurve[i] = 0.5 * (1 - Math.cos(Math.PI * k));
  }
  const bounceCurve = [];
  for (i = 0; i < curveLen; i++) {
    k = i / (curveLen - 1);
    const freq = Math.pow(k, 3) * 4 + 0.2;
    const val = Math.cos(freq * Math.PI * 2 * k);
    bounceCurve[i] = Math.abs(val * (1 - k));
  }
  function invertCurve(curve) {
    const out = new Array(curve.length);
    for (let j = 0; j < curve.length; j++) {
      out[j] = 1 - curve[j];
    }
    return out;
  }
  function reverseCurve(curve) {
    return curve.slice(0).reverse();
  }
  return {
    bounce: {
      In: invertCurve(bounceCurve),
      Out: bounceCurve
    },
    cosine: {
      In: cosineCurve,
      Out: reverseCurve(cosineCurve)
    },
    exponential: "exponential",
    linear: "linear",
    ripple: {
      In: rippleCurve,
      Out: invertCurve(rippleCurve)
    },
    sine: {
      In: sineCurve,
      Out: invertCurve(sineCurve)
    },
    step: {
      In: stairsCurve,
      Out: invertCurve(stairsCurve)
    }
  };
})();

// node_modules/tone/build/esm/instrument/Instrument.js
var Instrument = class _Instrument extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Instrument.getDefaults(), arguments);
    super(options);
    this._scheduledEvents = [];
    this._synced = false;
    this._original_triggerAttack = this.triggerAttack;
    this._original_triggerRelease = this.triggerRelease;
    this._syncedRelease = (time) => this._original_triggerRelease(time);
    this._volume = this.output = new Volume({
      context: this.context,
      volume: options.volume
    });
    this.volume = this._volume.volume;
    readOnly(this, "volume");
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      volume: 0
    });
  }
  /**
   * Sync the instrument to the Transport. All subsequent calls of
   * {@link triggerAttack} and {@link triggerRelease} will be scheduled along the transport.
   * @example
   * const fmSynth = new Tone.FMSynth().toDestination();
   * fmSynth.volume.value = -6;
   * fmSynth.sync();
   * // schedule 3 notes when the transport first starts
   * fmSynth.triggerAttackRelease("C4", "8n", 0);
   * fmSynth.triggerAttackRelease("E4", "8n", "8n");
   * fmSynth.triggerAttackRelease("G4", "8n", "4n");
   * // start the transport to hear the notes
   * Tone.Transport.start();
   */
  sync() {
    if (this._syncState()) {
      this._syncMethod("triggerAttack", 1);
      this._syncMethod("triggerRelease", 0);
      this.context.transport.on("stop", this._syncedRelease);
      this.context.transport.on("pause", this._syncedRelease);
      this.context.transport.on("loopEnd", this._syncedRelease);
    }
    return this;
  }
  /**
   * set _sync
   */
  _syncState() {
    let changed = false;
    if (!this._synced) {
      this._synced = true;
      changed = true;
    }
    return changed;
  }
  /**
   * Wrap the given method so that it can be synchronized
   * @param method Which method to wrap and sync
   * @param  timePosition What position the time argument appears in
   */
  _syncMethod(method, timePosition) {
    const originalMethod = this["_original_" + method] = this[method];
    this[method] = (...args) => {
      const time = args[timePosition];
      const id = this.context.transport.schedule((t) => {
        args[timePosition] = t;
        originalMethod.apply(this, args);
      }, time);
      this._scheduledEvents.push(id);
    };
  }
  /**
   * Unsync the instrument from the Transport
   */
  unsync() {
    this._scheduledEvents.forEach((id) => this.context.transport.clear(id));
    this._scheduledEvents = [];
    if (this._synced) {
      this._synced = false;
      this.triggerAttack = this._original_triggerAttack;
      this.triggerRelease = this._original_triggerRelease;
      this.context.transport.off("stop", this._syncedRelease);
      this.context.transport.off("pause", this._syncedRelease);
      this.context.transport.off("loopEnd", this._syncedRelease);
    }
    return this;
  }
  /**
   * Trigger the attack and then the release after the duration.
   * @param  note     The note to trigger.
   * @param  duration How long the note should be held for before
   *                         triggering the release. This value must be greater than 0.
   * @param time  When the note should be triggered.
   * @param  velocity The velocity the note should be triggered at.
   * @example
   * const synth = new Tone.Synth().toDestination();
   * // trigger "C4" for the duration of an 8th note
   * synth.triggerAttackRelease("C4", "8n");
   */
  triggerAttackRelease(note, duration, time, velocity) {
    const computedTime = this.toSeconds(time);
    const computedDuration = this.toSeconds(duration);
    this.triggerAttack(note, computedTime, velocity);
    this.triggerRelease(computedTime + computedDuration);
    return this;
  }
  /**
   * clean up
   * @returns {Instrument} this
   */
  dispose() {
    super.dispose();
    this._volume.dispose();
    this.unsync();
    this._scheduledEvents = [];
    return this;
  }
};

// node_modules/tone/build/esm/instrument/Monophonic.js
var Monophonic = class _Monophonic extends Instrument {
  constructor() {
    const options = optionsFromArguments(_Monophonic.getDefaults(), arguments);
    super(options);
    this.portamento = options.portamento;
    this.onsilence = options.onsilence;
  }
  static getDefaults() {
    return Object.assign(Instrument.getDefaults(), {
      detune: 0,
      onsilence: noOp,
      portamento: 0
    });
  }
  /**
   * Trigger the attack of the note optionally with a given velocity.
   * @param  note The note to trigger.
   * @param  time When the note should start.
   * @param  velocity The velocity determines how "loud" the note will be.
   * @example
   * const synth = new Tone.Synth().toDestination();
   * // trigger the note a half second from now at half velocity
   * synth.triggerAttack("C4", "+0.5", 0.5);
   */
  triggerAttack(note, time, velocity = 1) {
    this.log("triggerAttack", note, time, velocity);
    const seconds = this.toSeconds(time);
    this._triggerEnvelopeAttack(seconds, velocity);
    this.setNote(note, seconds);
    return this;
  }
  /**
   * Trigger the release portion of the envelope.
   * @param  time If no time is given, the release happens immediately.
   * @example
   * const synth = new Tone.Synth().toDestination();
   * synth.triggerAttack("C4");
   * // trigger the release a second from now
   * synth.triggerRelease("+1");
   */
  triggerRelease(time) {
    this.log("triggerRelease", time);
    const seconds = this.toSeconds(time);
    this._triggerEnvelopeRelease(seconds);
    return this;
  }
  /**
   * Set the note at the given time. If no time is given, the note
   * will set immediately.
   * @param note The note to change to.
   * @param  time The time when the note should be set.
   * @example
   * const synth = new Tone.Synth().toDestination();
   * synth.triggerAttack("C4");
   * // change to F#6 in one quarter note from now.
   * synth.setNote("F#6", "+4n");
   */
  setNote(note, time) {
    const computedTime = this.toSeconds(time);
    const computedFrequency = note instanceof FrequencyClass ? note.toFrequency() : note;
    if (this.portamento > 0 && this.getLevelAtTime(computedTime) > 0.05) {
      const portTime = this.toSeconds(this.portamento);
      this.frequency.exponentialRampTo(computedFrequency, portTime, computedTime);
    } else {
      this.frequency.setValueAtTime(computedFrequency, computedTime);
    }
    return this;
  }
};
__decorate([
  timeRange(0)
], Monophonic.prototype, "portamento", void 0);

// node_modules/tone/build/esm/component/envelope/AmplitudeEnvelope.js
var AmplitudeEnvelope = class _AmplitudeEnvelope extends Envelope {
  constructor() {
    super(optionsFromArguments(_AmplitudeEnvelope.getDefaults(), arguments, [
      "attack",
      "decay",
      "sustain",
      "release"
    ]));
    this.name = "AmplitudeEnvelope";
    this._gainNode = new Gain({
      context: this.context,
      gain: 0
    });
    this.output = this._gainNode;
    this.input = this._gainNode;
    this._sig.connect(this._gainNode.gain);
    this.output = this._gainNode;
    this.input = this._gainNode;
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    this._gainNode.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/instrument/Synth.js
var Synth = class _Synth extends Monophonic {
  constructor() {
    const options = optionsFromArguments(_Synth.getDefaults(), arguments);
    super(options);
    this.name = "Synth";
    this.oscillator = new OmniOscillator(Object.assign({
      context: this.context,
      detune: options.detune,
      onstop: () => this.onsilence(this)
    }, options.oscillator));
    this.frequency = this.oscillator.frequency;
    this.detune = this.oscillator.detune;
    this.envelope = new AmplitudeEnvelope(Object.assign({
      context: this.context
    }, options.envelope));
    this.oscillator.chain(this.envelope, this.output);
    readOnly(this, ["oscillator", "frequency", "detune", "envelope"]);
  }
  static getDefaults() {
    return Object.assign(Monophonic.getDefaults(), {
      envelope: Object.assign(omitFromObject(Envelope.getDefaults(), Object.keys(ToneAudioNode.getDefaults())), {
        attack: 5e-3,
        decay: 0.1,
        release: 1,
        sustain: 0.3
      }),
      oscillator: Object.assign(omitFromObject(OmniOscillator.getDefaults(), [
        ...Object.keys(Source.getDefaults()),
        "frequency",
        "detune"
      ]), {
        type: "triangle"
      })
    });
  }
  /**
   * start the attack portion of the envelope
   * @param time the time the attack should start
   * @param velocity the velocity of the note (0-1)
   */
  _triggerEnvelopeAttack(time, velocity) {
    this.envelope.triggerAttack(time, velocity);
    this.oscillator.start(time);
    if (this.envelope.sustain === 0) {
      const computedAttack = this.toSeconds(this.envelope.attack);
      const computedDecay = this.toSeconds(this.envelope.decay);
      this.oscillator.stop(time + computedAttack + computedDecay);
    }
  }
  /**
   * start the release portion of the envelope
   * @param time the time the release should start
   */
  _triggerEnvelopeRelease(time) {
    this.envelope.triggerRelease(time);
    this.oscillator.stop(time + this.toSeconds(this.envelope.release));
  }
  getLevelAtTime(time) {
    time = this.toSeconds(time);
    return this.envelope.getValueAtTime(time);
  }
  /**
   * clean up
   */
  dispose() {
    super.dispose();
    this.oscillator.dispose();
    this.envelope.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/instrument/ModulationSynth.js
var ModulationSynth = class _ModulationSynth extends Monophonic {
  constructor() {
    const options = optionsFromArguments(_ModulationSynth.getDefaults(), arguments);
    super(options);
    this.name = "ModulationSynth";
    this._carrier = new Synth({
      context: this.context,
      oscillator: options.oscillator,
      envelope: options.envelope,
      onsilence: () => this.onsilence(this),
      volume: -10
    });
    this._modulator = new Synth({
      context: this.context,
      oscillator: options.modulation,
      envelope: options.modulationEnvelope,
      volume: -10
    });
    this.oscillator = this._carrier.oscillator;
    this.envelope = this._carrier.envelope;
    this.modulation = this._modulator.oscillator;
    this.modulationEnvelope = this._modulator.envelope;
    this.frequency = new Signal({
      context: this.context,
      units: "frequency"
    });
    this.detune = new Signal({
      context: this.context,
      value: options.detune,
      units: "cents"
    });
    this.harmonicity = new Multiply({
      context: this.context,
      value: options.harmonicity,
      minValue: 0
    });
    this._modulationNode = new Gain({
      context: this.context,
      gain: 0
    });
    readOnly(this, [
      "frequency",
      "harmonicity",
      "oscillator",
      "envelope",
      "modulation",
      "modulationEnvelope",
      "detune"
    ]);
  }
  static getDefaults() {
    return Object.assign(Monophonic.getDefaults(), {
      harmonicity: 3,
      oscillator: Object.assign(omitFromObject(OmniOscillator.getDefaults(), [
        ...Object.keys(Source.getDefaults()),
        "frequency",
        "detune"
      ]), {
        type: "sine"
      }),
      envelope: Object.assign(omitFromObject(Envelope.getDefaults(), Object.keys(ToneAudioNode.getDefaults())), {
        attack: 0.01,
        decay: 0.01,
        sustain: 1,
        release: 0.5
      }),
      modulation: Object.assign(omitFromObject(OmniOscillator.getDefaults(), [
        ...Object.keys(Source.getDefaults()),
        "frequency",
        "detune"
      ]), {
        type: "square"
      }),
      modulationEnvelope: Object.assign(omitFromObject(Envelope.getDefaults(), Object.keys(ToneAudioNode.getDefaults())), {
        attack: 0.5,
        decay: 0,
        sustain: 1,
        release: 0.5
      })
    });
  }
  /**
   * Trigger the attack portion of the note
   */
  _triggerEnvelopeAttack(time, velocity) {
    this._carrier._triggerEnvelopeAttack(time, velocity);
    this._modulator._triggerEnvelopeAttack(time, velocity);
  }
  /**
   * Trigger the release portion of the note
   */
  _triggerEnvelopeRelease(time) {
    this._carrier._triggerEnvelopeRelease(time);
    this._modulator._triggerEnvelopeRelease(time);
    return this;
  }
  getLevelAtTime(time) {
    time = this.toSeconds(time);
    return this.envelope.getValueAtTime(time);
  }
  dispose() {
    super.dispose();
    this._carrier.dispose();
    this._modulator.dispose();
    this.frequency.dispose();
    this.detune.dispose();
    this.harmonicity.dispose();
    this._modulationNode.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/filter/BiquadFilter.js
var BiquadFilter = class _BiquadFilter extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_BiquadFilter.getDefaults(), arguments, ["frequency", "type"]);
    super(options);
    this.name = "BiquadFilter";
    this._filter = this.context.createBiquadFilter();
    this.input = this.output = this._filter;
    this.Q = new Param({
      context: this.context,
      units: "number",
      value: options.Q,
      param: this._filter.Q
    });
    this.frequency = new Param({
      context: this.context,
      units: "frequency",
      value: options.frequency,
      param: this._filter.frequency
    });
    this.detune = new Param({
      context: this.context,
      units: "cents",
      value: options.detune,
      param: this._filter.detune
    });
    this.gain = new Param({
      context: this.context,
      units: "decibels",
      convert: false,
      value: options.gain,
      param: this._filter.gain
    });
    this.type = options.type;
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      Q: 1,
      type: "lowpass",
      frequency: 350,
      detune: 0,
      gain: 0
    });
  }
  /**
   * The type of this BiquadFilterNode. For a complete list of types and their attributes, see the
   * [Web Audio API](https://webaudio.github.io/web-audio-api/#dom-biquadfiltertype-lowpass)
   */
  get type() {
    return this._filter.type;
  }
  set type(type) {
    const types = [
      "lowpass",
      "highpass",
      "bandpass",
      "lowshelf",
      "highshelf",
      "notch",
      "allpass",
      "peaking"
    ];
    assert(types.indexOf(type) !== -1, `Invalid filter type: ${type}`);
    this._filter.type = type;
  }
  /**
   * Get the frequency response curve. This curve represents how the filter
   * responses to frequencies between 20hz-20khz.
   * @param  len The number of values to return
   * @return The frequency response curve between 20-20kHz
   */
  getFrequencyResponse(len = 128) {
    const freqValues = new Float32Array(len);
    for (let i = 0; i < len; i++) {
      const norm = Math.pow(i / len, 2);
      const freq = norm * (2e4 - 20) + 20;
      freqValues[i] = freq;
    }
    const magValues = new Float32Array(len);
    const phaseValues = new Float32Array(len);
    const filterClone = this.context.createBiquadFilter();
    filterClone.type = this.type;
    filterClone.Q.value = this.Q.value;
    filterClone.frequency.value = this.frequency.value;
    filterClone.gain.value = this.gain.value;
    filterClone.getFrequencyResponse(freqValues, magValues, phaseValues);
    return magValues;
  }
  dispose() {
    super.dispose();
    this._filter.disconnect();
    this.Q.dispose();
    this.frequency.dispose();
    this.gain.dispose();
    this.detune.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/filter/Filter.js
var Filter = class _Filter extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Filter.getDefaults(), arguments, [
      "frequency",
      "type",
      "rolloff"
    ]);
    super(options);
    this.name = "Filter";
    this.input = new Gain({ context: this.context });
    this.output = new Gain({ context: this.context });
    this._filters = [];
    this._filters = [];
    this.Q = new Signal({
      context: this.context,
      units: "positive",
      value: options.Q
    });
    this.frequency = new Signal({
      context: this.context,
      units: "frequency",
      value: options.frequency
    });
    this.detune = new Signal({
      context: this.context,
      units: "cents",
      value: options.detune
    });
    this.gain = new Signal({
      context: this.context,
      units: "decibels",
      convert: false,
      value: options.gain
    });
    this._type = options.type;
    this.rolloff = options.rolloff;
    readOnly(this, ["detune", "frequency", "gain", "Q"]);
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      Q: 1,
      detune: 0,
      frequency: 350,
      gain: 0,
      rolloff: -12,
      type: "lowpass"
    });
  }
  /**
   * The type of the filter. Types: "lowpass", "highpass",
   * "bandpass", "lowshelf", "highshelf", "notch", "allpass", or "peaking".
   */
  get type() {
    return this._type;
  }
  set type(type) {
    const types = [
      "lowpass",
      "highpass",
      "bandpass",
      "lowshelf",
      "highshelf",
      "notch",
      "allpass",
      "peaking"
    ];
    assert(types.indexOf(type) !== -1, `Invalid filter type: ${type}`);
    this._type = type;
    this._filters.forEach((filter) => filter.type = type);
  }
  /**
   * The rolloff of the filter which is the drop in db
   * per octave. Implemented internally by cascading filters.
   * Only accepts the values -12, -24, -48 and -96.
   */
  get rolloff() {
    return this._rolloff;
  }
  set rolloff(rolloff) {
    const rolloffNum = isNumber(rolloff) ? rolloff : parseInt(rolloff, 10);
    const possibilities = [-12, -24, -48, -96];
    let cascadingCount = possibilities.indexOf(rolloffNum);
    assert(cascadingCount !== -1, `rolloff can only be ${possibilities.join(", ")}`);
    cascadingCount += 1;
    this._rolloff = rolloffNum;
    this.input.disconnect();
    this._filters.forEach((filter) => filter.disconnect());
    this._filters = new Array(cascadingCount);
    for (let count = 0; count < cascadingCount; count++) {
      const filter = new BiquadFilter({
        context: this.context
      });
      filter.type = this._type;
      this.frequency.connect(filter.frequency);
      this.detune.connect(filter.detune);
      this.Q.connect(filter.Q);
      this.gain.connect(filter.gain);
      this._filters[count] = filter;
    }
    this._internalChannels = this._filters;
    connectSeries(this.input, ...this._internalChannels, this.output);
  }
  /**
   * Get the frequency response curve. This curve represents how the filter
   * responses to frequencies between 20hz-20khz.
   * @param  len The number of values to return
   * @return The frequency response curve between 20-20kHz
   */
  getFrequencyResponse(len = 128) {
    const filterClone = new BiquadFilter({
      context: this.context,
      frequency: this.frequency.value,
      gain: this.gain.value,
      Q: this.Q.value,
      type: this._type,
      detune: this.detune.value
    });
    const totalResponse = new Float32Array(len).map(() => 1);
    this._filters.forEach(() => {
      const response = filterClone.getFrequencyResponse(len);
      response.forEach((val, i) => totalResponse[i] *= val);
    });
    filterClone.dispose();
    return totalResponse;
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._filters.forEach((filter) => {
      filter.dispose();
    });
    writable(this, ["detune", "frequency", "gain", "Q"]);
    this.frequency.dispose();
    this.Q.dispose();
    this.detune.dispose();
    this.gain.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/envelope/FrequencyEnvelope.js
var FrequencyEnvelope = class _FrequencyEnvelope extends Envelope {
  constructor() {
    const options = optionsFromArguments(_FrequencyEnvelope.getDefaults(), arguments, ["attack", "decay", "sustain", "release"]);
    super(options);
    this.name = "FrequencyEnvelope";
    this._octaves = options.octaves;
    this._baseFrequency = this.toFrequency(options.baseFrequency);
    this._exponent = this.input = new Pow({
      context: this.context,
      value: options.exponent
    });
    this._scale = this.output = new Scale({
      context: this.context,
      min: this._baseFrequency,
      max: this._baseFrequency * Math.pow(2, this._octaves)
    });
    this._sig.chain(this._exponent, this._scale);
  }
  static getDefaults() {
    return Object.assign(Envelope.getDefaults(), {
      baseFrequency: 200,
      exponent: 1,
      octaves: 4
    });
  }
  /**
   * The envelope's minimum output value. This is the value which it
   * starts at.
   */
  get baseFrequency() {
    return this._baseFrequency;
  }
  set baseFrequency(min) {
    const freq = this.toFrequency(min);
    assertRange(freq, 0);
    this._baseFrequency = freq;
    this._scale.min = this._baseFrequency;
    this.octaves = this._octaves;
  }
  /**
   * The number of octaves above the baseFrequency that the
   * envelope will scale to.
   */
  get octaves() {
    return this._octaves;
  }
  set octaves(octaves) {
    this._octaves = octaves;
    this._scale.max = this._baseFrequency * Math.pow(2, octaves);
  }
  /**
   * The envelope's exponent value.
   */
  get exponent() {
    return this._exponent.value;
  }
  set exponent(exponent) {
    this._exponent.value = exponent;
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    this._exponent.dispose();
    this._scale.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/instrument/MonoSynth.js
var MonoSynth = class _MonoSynth extends Monophonic {
  constructor() {
    const options = optionsFromArguments(_MonoSynth.getDefaults(), arguments);
    super(options);
    this.name = "MonoSynth";
    this.oscillator = new OmniOscillator(Object.assign(options.oscillator, {
      context: this.context,
      detune: options.detune,
      onstop: () => this.onsilence(this)
    }));
    this.frequency = this.oscillator.frequency;
    this.detune = this.oscillator.detune;
    this.filter = new Filter(Object.assign(options.filter, { context: this.context }));
    this.filterEnvelope = new FrequencyEnvelope(Object.assign(options.filterEnvelope, { context: this.context }));
    this.envelope = new AmplitudeEnvelope(Object.assign(options.envelope, { context: this.context }));
    this.oscillator.chain(this.filter, this.envelope, this.output);
    this.filterEnvelope.connect(this.filter.frequency);
    readOnly(this, [
      "oscillator",
      "frequency",
      "detune",
      "filter",
      "filterEnvelope",
      "envelope"
    ]);
  }
  static getDefaults() {
    return Object.assign(Monophonic.getDefaults(), {
      envelope: Object.assign(omitFromObject(Envelope.getDefaults(), Object.keys(ToneAudioNode.getDefaults())), {
        attack: 5e-3,
        decay: 0.1,
        release: 1,
        sustain: 0.9
      }),
      filter: Object.assign(omitFromObject(Filter.getDefaults(), Object.keys(ToneAudioNode.getDefaults())), {
        Q: 1,
        rolloff: -12,
        type: "lowpass"
      }),
      filterEnvelope: Object.assign(omitFromObject(FrequencyEnvelope.getDefaults(), Object.keys(ToneAudioNode.getDefaults())), {
        attack: 0.6,
        baseFrequency: 200,
        decay: 0.2,
        exponent: 2,
        octaves: 3,
        release: 2,
        sustain: 0.5
      }),
      oscillator: Object.assign(omitFromObject(OmniOscillator.getDefaults(), Object.keys(Source.getDefaults())), {
        type: "sawtooth"
      })
    });
  }
  /**
   * start the attack portion of the envelope
   * @param time the time the attack should start
   * @param velocity the velocity of the note (0-1)
   */
  _triggerEnvelopeAttack(time, velocity = 1) {
    this.envelope.triggerAttack(time, velocity);
    this.filterEnvelope.triggerAttack(time);
    this.oscillator.start(time);
    if (this.envelope.sustain === 0) {
      const computedAttack = this.toSeconds(this.envelope.attack);
      const computedDecay = this.toSeconds(this.envelope.decay);
      this.oscillator.stop(time + computedAttack + computedDecay);
    }
  }
  /**
   * start the release portion of the envelope
   * @param time the time the release should start
   */
  _triggerEnvelopeRelease(time) {
    this.envelope.triggerRelease(time);
    this.filterEnvelope.triggerRelease(time);
    this.oscillator.stop(time + this.toSeconds(this.envelope.release));
  }
  getLevelAtTime(time) {
    time = this.toSeconds(time);
    return this.envelope.getValueAtTime(time);
  }
  dispose() {
    super.dispose();
    this.oscillator.dispose();
    this.envelope.dispose();
    this.filterEnvelope.dispose();
    this.filter.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/instrument/FMSynth.js
var FMSynth = class _FMSynth extends ModulationSynth {
  constructor() {
    const options = optionsFromArguments(_FMSynth.getDefaults(), arguments);
    super(options);
    this.name = "FMSynth";
    this.modulationIndex = new Multiply({
      context: this.context,
      value: options.modulationIndex
    });
    this.frequency.connect(this._carrier.frequency);
    this.frequency.chain(this.harmonicity, this._modulator.frequency);
    this.frequency.chain(this.modulationIndex, this._modulationNode);
    this.detune.fan(this._carrier.detune, this._modulator.detune);
    this._modulator.connect(this._modulationNode.gain);
    this._modulationNode.connect(this._carrier.frequency);
    this._carrier.connect(this.output);
  }
  static getDefaults() {
    return Object.assign(ModulationSynth.getDefaults(), {
      modulationIndex: 10
    });
  }
  dispose() {
    super.dispose();
    this.modulationIndex.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/instrument/MetalSynth.js
var inharmRatios = [1, 1.483, 1.932, 2.546, 2.63, 3.897];
var MetalSynth = class _MetalSynth extends Monophonic {
  constructor() {
    const options = optionsFromArguments(_MetalSynth.getDefaults(), arguments);
    super(options);
    this.name = "MetalSynth";
    this._oscillators = [];
    this._freqMultipliers = [];
    this.detune = new Signal({
      context: this.context,
      units: "cents",
      value: options.detune
    });
    this.frequency = new Signal({
      context: this.context,
      units: "frequency"
    });
    this._amplitude = new Gain({
      context: this.context,
      gain: 0
    }).connect(this.output);
    this._highpass = new Filter({
      // Q: -3.0102999566398125,
      Q: 0,
      context: this.context,
      type: "highpass"
    }).connect(this._amplitude);
    for (let i = 0; i < inharmRatios.length; i++) {
      const osc = new FMOscillator({
        context: this.context,
        harmonicity: options.harmonicity,
        modulationIndex: options.modulationIndex,
        modulationType: "square",
        onstop: i === 0 ? () => this.onsilence(this) : noOp,
        type: "square"
      });
      osc.connect(this._highpass);
      this._oscillators[i] = osc;
      const mult = new Multiply({
        context: this.context,
        value: inharmRatios[i]
      });
      this._freqMultipliers[i] = mult;
      this.frequency.chain(mult, osc.frequency);
      this.detune.connect(osc.detune);
    }
    this._filterFreqScaler = new Scale({
      context: this.context,
      max: 7e3,
      min: this.toFrequency(options.resonance)
    });
    this.envelope = new Envelope({
      attack: options.envelope.attack,
      attackCurve: "linear",
      context: this.context,
      decay: options.envelope.decay,
      release: options.envelope.release,
      sustain: 0
    });
    this.envelope.chain(this._filterFreqScaler, this._highpass.frequency);
    this.envelope.connect(this._amplitude.gain);
    this._octaves = options.octaves;
    this.octaves = options.octaves;
  }
  static getDefaults() {
    return deepMerge(Monophonic.getDefaults(), {
      envelope: Object.assign(omitFromObject(Envelope.getDefaults(), Object.keys(ToneAudioNode.getDefaults())), {
        attack: 1e-3,
        decay: 1.4,
        release: 0.2
      }),
      harmonicity: 5.1,
      modulationIndex: 32,
      octaves: 1.5,
      resonance: 4e3
    });
  }
  /**
   * Trigger the attack.
   * @param time When the attack should be triggered.
   * @param velocity The velocity that the envelope should be triggered at.
   */
  _triggerEnvelopeAttack(time, velocity = 1) {
    this.envelope.triggerAttack(time, velocity);
    this._oscillators.forEach((osc) => osc.start(time));
    if (this.envelope.sustain === 0) {
      this._oscillators.forEach((osc) => {
        osc.stop(time + this.toSeconds(this.envelope.attack) + this.toSeconds(this.envelope.decay));
      });
    }
    return this;
  }
  /**
   * Trigger the release of the envelope.
   * @param time When the release should be triggered.
   */
  _triggerEnvelopeRelease(time) {
    this.envelope.triggerRelease(time);
    this._oscillators.forEach((osc) => osc.stop(time + this.toSeconds(this.envelope.release)));
    return this;
  }
  getLevelAtTime(time) {
    time = this.toSeconds(time);
    return this.envelope.getValueAtTime(time);
  }
  /**
   * The modulationIndex of the oscillators which make up the source.
   * see {@link FMOscillator.modulationIndex}
   * @min 1
   * @max 100
   */
  get modulationIndex() {
    return this._oscillators[0].modulationIndex.value;
  }
  set modulationIndex(val) {
    this._oscillators.forEach((osc) => osc.modulationIndex.value = val);
  }
  /**
   * The harmonicity of the oscillators which make up the source.
   * see Tone.FMOscillator.harmonicity
   * @min 0.1
   * @max 10
   */
  get harmonicity() {
    return this._oscillators[0].harmonicity.value;
  }
  set harmonicity(val) {
    this._oscillators.forEach((osc) => osc.harmonicity.value = val);
  }
  /**
   * The lower level of the highpass filter which is attached to the envelope.
   * This value should be between [0, 7000]
   * @min 0
   * @max 7000
   */
  get resonance() {
    return this._filterFreqScaler.min;
  }
  set resonance(val) {
    this._filterFreqScaler.min = this.toFrequency(val);
    this.octaves = this._octaves;
  }
  /**
   * The number of octaves above the "resonance" frequency
   * that the filter ramps during the attack/decay envelope
   * @min 0
   * @max 8
   */
  get octaves() {
    return this._octaves;
  }
  set octaves(val) {
    this._octaves = val;
    this._filterFreqScaler.max = this._filterFreqScaler.min * Math.pow(2, val);
  }
  dispose() {
    super.dispose();
    this._oscillators.forEach((osc) => osc.dispose());
    this._freqMultipliers.forEach((freqMult) => freqMult.dispose());
    this.frequency.dispose();
    this.detune.dispose();
    this._filterFreqScaler.dispose();
    this._amplitude.dispose();
    this.envelope.dispose();
    this._highpass.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/instrument/MembraneSynth.js
var MembraneSynth = class _MembraneSynth extends Synth {
  constructor() {
    const options = optionsFromArguments(_MembraneSynth.getDefaults(), arguments);
    super(options);
    this.name = "MembraneSynth";
    this.portamento = 0;
    this.pitchDecay = options.pitchDecay;
    this.octaves = options.octaves;
    readOnly(this, ["oscillator", "envelope"]);
  }
  static getDefaults() {
    return deepMerge(Monophonic.getDefaults(), Synth.getDefaults(), {
      envelope: {
        attack: 1e-3,
        attackCurve: "exponential",
        decay: 0.4,
        release: 1.4,
        sustain: 0.01
      },
      octaves: 10,
      oscillator: {
        type: "sine"
      },
      pitchDecay: 0.05
    });
  }
  setNote(note, time) {
    const seconds = this.toSeconds(time);
    const hertz = this.toFrequency(note instanceof FrequencyClass ? note.toFrequency() : note);
    const maxNote = hertz * this.octaves;
    this.oscillator.frequency.setValueAtTime(maxNote, seconds);
    this.oscillator.frequency.exponentialRampToValueAtTime(hertz, seconds + this.toSeconds(this.pitchDecay));
    return this;
  }
  dispose() {
    super.dispose();
    return this;
  }
};
__decorate([
  range(0)
], MembraneSynth.prototype, "octaves", void 0);
__decorate([
  timeRange(0)
], MembraneSynth.prototype, "pitchDecay", void 0);

// node_modules/tone/build/esm/instrument/NoiseSynth.js
var NoiseSynth = class _NoiseSynth extends Instrument {
  constructor() {
    const options = optionsFromArguments(_NoiseSynth.getDefaults(), arguments);
    super(options);
    this.name = "NoiseSynth";
    this.noise = new Noise(Object.assign({
      context: this.context
    }, options.noise));
    this.envelope = new AmplitudeEnvelope(Object.assign({
      context: this.context
    }, options.envelope));
    this.noise.chain(this.envelope, this.output);
  }
  static getDefaults() {
    return Object.assign(Instrument.getDefaults(), {
      envelope: Object.assign(omitFromObject(Envelope.getDefaults(), Object.keys(ToneAudioNode.getDefaults())), {
        decay: 0.1,
        sustain: 0
      }),
      noise: Object.assign(omitFromObject(Noise.getDefaults(), Object.keys(Source.getDefaults())), {
        type: "white"
      })
    });
  }
  /**
   * Start the attack portion of the envelopes. Unlike other
   * instruments, Tone.NoiseSynth doesn't have a note.
   * @example
   * const noiseSynth = new Tone.NoiseSynth().toDestination();
   * noiseSynth.triggerAttack();
   */
  triggerAttack(time, velocity = 1) {
    time = this.toSeconds(time);
    this.envelope.triggerAttack(time, velocity);
    this.noise.start(time);
    if (this.envelope.sustain === 0) {
      this.noise.stop(time + this.toSeconds(this.envelope.attack) + this.toSeconds(this.envelope.decay));
    }
    return this;
  }
  /**
   * Start the release portion of the envelopes.
   */
  triggerRelease(time) {
    time = this.toSeconds(time);
    this.envelope.triggerRelease(time);
    this.noise.stop(time + this.toSeconds(this.envelope.release));
    return this;
  }
  sync() {
    if (this._syncState()) {
      this._syncMethod("triggerAttack", 0);
      this._syncMethod("triggerRelease", 0);
    }
    return this;
  }
  /**
   * Trigger the attack and then the release after the duration.
   * @param duration The amount of time to hold the note for
   * @param time The time the note should start
   * @param velocity The volume of the note (0-1)
   * @example
   * const noiseSynth = new Tone.NoiseSynth().toDestination();
   * // hold the note for 0.5 seconds
   * noiseSynth.triggerAttackRelease(0.5);
   */
  triggerAttackRelease(duration, time, velocity = 1) {
    time = this.toSeconds(time);
    duration = this.toSeconds(duration);
    this.triggerAttack(time, velocity);
    this.triggerRelease(time + duration);
    return this;
  }
  dispose() {
    super.dispose();
    this.noise.dispose();
    this.envelope.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/core/worklet/WorkletGlobalScope.js
var workletContext = /* @__PURE__ */ new Set();
function addToWorklet(classOrFunction) {
  workletContext.add(classOrFunction);
}
function registerProcessor(name, classDesc) {
  const processor = (
    /* javascript */
    `registerProcessor("${name}", ${classDesc})`
  );
  workletContext.add(processor);
}
function getWorkletGlobalScope() {
  return Array.from(workletContext).join("\n");
}

// node_modules/tone/build/esm/core/worklet/ToneAudioWorklet.js
var ToneAudioWorklet = class extends ToneAudioNode {
  constructor(options) {
    super(options);
    this.name = "ToneAudioWorklet";
    this.workletOptions = {};
    this.onprocessorerror = noOp;
    const blobUrl = URL.createObjectURL(new Blob([getWorkletGlobalScope()], { type: "text/javascript" }));
    const name = this._audioWorkletName();
    this._dummyGain = this.context.createGain();
    this._dummyParam = this._dummyGain.gain;
    this.context.addAudioWorkletModule(blobUrl).then(() => {
      if (!this.disposed) {
        this._worklet = this.context.createAudioWorkletNode(name, this.workletOptions);
        this._worklet.onprocessorerror = this.onprocessorerror.bind(this);
        this.onReady(this._worklet);
      }
    });
  }
  dispose() {
    super.dispose();
    this._dummyGain.disconnect();
    if (this._worklet) {
      this._worklet.port.postMessage("dispose");
      this._worklet.disconnect();
    }
    return this;
  }
};

// node_modules/tone/build/esm/core/worklet/ToneAudioWorkletProcessor.worklet.js
var toneAudioWorkletProcessor = (
  /* javascript */
  `
	/**
	 * The base AudioWorkletProcessor for use in Tone.js. Works with the {@link ToneAudioWorklet}. 
	 */
	class ToneAudioWorkletProcessor extends AudioWorkletProcessor {

		constructor(options) {
			
			super(options);
			/**
			 * If the processor was disposed or not. Keep alive until it's disposed.
			 */
			this.disposed = false;
		   	/** 
			 * The number of samples in the processing block
			 */
			this.blockSize = 128;
			/**
			 * the sample rate
			 */
			this.sampleRate = sampleRate;

			this.port.onmessage = (event) => {
				// when it receives a dispose 
				if (event.data === "dispose") {
					this.disposed = true;
				}
			};
		}
	}
`
);
addToWorklet(toneAudioWorkletProcessor);

// node_modules/tone/build/esm/core/worklet/SingleIOProcessor.worklet.js
var singleIOProcess = (
  /* javascript */
  `
	/**
	 * Abstract class for a single input/output processor. 
	 * has a 'generate' function which processes one sample at a time
	 */
	class SingleIOProcessor extends ToneAudioWorkletProcessor {

		constructor(options) {
			super(Object.assign(options, {
				numberOfInputs: 1,
				numberOfOutputs: 1
			}));
			/**
			 * Holds the name of the parameter and a single value of that
			 * parameter at the current sample
			 * @type { [name: string]: number }
			 */
			this.params = {}
		}

		/**
		 * Generate an output sample from the input sample and parameters
		 * @abstract
		 * @param input number
		 * @param channel number
		 * @param parameters { [name: string]: number }
		 * @returns number
		 */
		generate(){}

		/**
		 * Update the private params object with the 
		 * values of the parameters at the given index
		 * @param parameters { [name: string]: Float32Array },
		 * @param index number
		 */
		updateParams(parameters, index) {
			for (const paramName in parameters) {
				const param = parameters[paramName];
				if (param.length > 1) {
					this.params[paramName] = parameters[paramName][index];
				} else {
					this.params[paramName] = parameters[paramName][0];
				}
			}
		}

		/**
		 * Process a single frame of the audio
		 * @param inputs Float32Array[][]
		 * @param outputs Float32Array[][]
		 */
		process(inputs, outputs, parameters) {
			const input = inputs[0];
			const output = outputs[0];
			// get the parameter values
			const channelCount = Math.max(input && input.length || 0, output.length);
			for (let sample = 0; sample < this.blockSize; sample++) {
				this.updateParams(parameters, sample);
				for (let channel = 0; channel < channelCount; channel++) {
					const inputSample = input && input.length ? input[channel][sample] : 0;
					output[channel][sample] = this.generate(inputSample, channel, this.params);
				}
			}
			return !this.disposed;
		}
	};
`
);
addToWorklet(singleIOProcess);

// node_modules/tone/build/esm/core/worklet/DelayLine.worklet.js
var delayLine = (
  /* javascript */
  `
	/**
	 * A multichannel buffer for use within an AudioWorkletProcessor as a delay line
	 */
	class DelayLine {
		
		constructor(size, channels) {
			this.buffer = [];
			this.writeHead = []
			this.size = size;

			// create the empty channels
			for (let i = 0; i < channels; i++) {
				this.buffer[i] = new Float32Array(this.size);
				this.writeHead[i] = 0;
			}
		}

		/**
		 * Push a value onto the end
		 * @param channel number
		 * @param value number
		 */
		push(channel, value) {
			this.writeHead[channel] += 1;
			if (this.writeHead[channel] > this.size) {
				this.writeHead[channel] = 0;
			}
			this.buffer[channel][this.writeHead[channel]] = value;
		}

		/**
		 * Get the recorded value of the channel given the delay
		 * @param channel number
		 * @param delay number delay samples
		 */
		get(channel, delay) {
			let readHead = this.writeHead[channel] - Math.floor(delay);
			if (readHead < 0) {
				readHead += this.size;
			}
			return this.buffer[channel][readHead];
		}
	}
`
);
addToWorklet(delayLine);

// node_modules/tone/build/esm/component/filter/FeedbackCombFilter.worklet.js
var workletName = "feedback-comb-filter";
var feedbackCombFilter = (
  /* javascript */
  `
	class FeedbackCombFilterWorklet extends SingleIOProcessor {

		constructor(options) {
			super(options);
			this.delayLine = new DelayLine(this.sampleRate, options.channelCount || 2);
		}

		static get parameterDescriptors() {
			return [{
				name: "delayTime",
				defaultValue: 0.1,
				minValue: 0,
				maxValue: 1,
				automationRate: "k-rate"
			}, {
				name: "feedback",
				defaultValue: 0.5,
				minValue: 0,
				maxValue: 0.9999,
				automationRate: "k-rate"
			}];
		}

		generate(input, channel, parameters) {
			const delayedSample = this.delayLine.get(channel, parameters.delayTime * this.sampleRate);
			this.delayLine.push(channel, input + delayedSample * parameters.feedback);
			return delayedSample;
		}
	}
`
);
registerProcessor(workletName, feedbackCombFilter);

// node_modules/tone/build/esm/instrument/PolySynth.js
var PolySynth = class _PolySynth extends Instrument {
  constructor() {
    const options = optionsFromArguments(_PolySynth.getDefaults(), arguments, ["voice", "options"]);
    super(options);
    this.name = "PolySynth";
    this._availableVoices = [];
    this._activeVoices = [];
    this._voices = [];
    this._gcTimeout = -1;
    this._averageActiveVoices = 0;
    this._syncedRelease = (time) => this.releaseAll(time);
    assert(!isNumber(options.voice), "DEPRECATED: The polyphony count is no longer the first argument.");
    const defaults = options.voice.getDefaults();
    this.options = Object.assign(defaults, options.options);
    this.voice = options.voice;
    this.maxPolyphony = options.maxPolyphony;
    this._dummyVoice = this._getNextAvailableVoice();
    const index = this._voices.indexOf(this._dummyVoice);
    this._voices.splice(index, 1);
    this._gcTimeout = this.context.setInterval(this._collectGarbage.bind(this), 1);
  }
  static getDefaults() {
    return Object.assign(Instrument.getDefaults(), {
      maxPolyphony: 32,
      options: {},
      voice: Synth
    });
  }
  /**
   * The number of active voices.
   */
  get activeVoices() {
    return this._activeVoices.length;
  }
  /**
   * Invoked when the source is done making sound, so that it can be
   * readded to the pool of available voices
   */
  _makeVoiceAvailable(voice) {
    this._availableVoices.push(voice);
    const activeVoiceIndex = this._activeVoices.findIndex((e) => e.voice === voice);
    this._activeVoices.splice(activeVoiceIndex, 1);
  }
  /**
   * Get an available voice from the pool of available voices.
   * If one is not available and the maxPolyphony limit is reached,
   * steal a voice, otherwise return null.
   */
  _getNextAvailableVoice() {
    if (this._availableVoices.length) {
      return this._availableVoices.shift();
    } else if (this._voices.length < this.maxPolyphony) {
      const voice = new this.voice(Object.assign(this.options, {
        context: this.context,
        onsilence: this._makeVoiceAvailable.bind(this)
      }));
      assert(voice instanceof Monophonic, "Voice must extend Monophonic class");
      voice.connect(this.output);
      this._voices.push(voice);
      return voice;
    } else {
      warn("Max polyphony exceeded. Note dropped.");
    }
  }
  /**
   * Occasionally check if there are any allocated voices which can be cleaned up.
   */
  _collectGarbage() {
    this._averageActiveVoices = Math.max(this._averageActiveVoices * 0.95, this.activeVoices);
    if (this._availableVoices.length && this._voices.length > Math.ceil(this._averageActiveVoices + 1)) {
      const firstAvail = this._availableVoices.shift();
      const index = this._voices.indexOf(firstAvail);
      this._voices.splice(index, 1);
      if (!this.context.isOffline) {
        firstAvail.dispose();
      }
    }
  }
  /**
   * Internal method which triggers the attack
   */
  _triggerAttack(notes, time, velocity) {
    notes.forEach((note) => {
      const midiNote = new MidiClass(this.context, note).toMidi();
      const voice = this._getNextAvailableVoice();
      if (voice) {
        voice.triggerAttack(note, time, velocity);
        this._activeVoices.push({
          midi: midiNote,
          voice,
          released: false
        });
        this.log("triggerAttack", note, time);
      }
    });
  }
  /**
   * Internal method which triggers the release
   */
  _triggerRelease(notes, time) {
    notes.forEach((note) => {
      const midiNote = new MidiClass(this.context, note).toMidi();
      const event = this._activeVoices.find(({ midi, released }) => midi === midiNote && !released);
      if (event) {
        event.voice.triggerRelease(time);
        event.released = true;
        this.log("triggerRelease", note, time);
      }
    });
  }
  /**
   * Schedule the attack/release events. If the time is in the future, then it should set a timeout
   * to wait for just-in-time scheduling
   */
  _scheduleEvent(type, notes, time, velocity) {
    assert(!this.disposed, "Synth was already disposed");
    if (time <= this.now()) {
      if (type === "attack") {
        this._triggerAttack(notes, time, velocity);
      } else {
        this._triggerRelease(notes, time);
      }
    } else {
      this.context.setTimeout(() => {
        if (!this.disposed) {
          this._scheduleEvent(type, notes, time, velocity);
        }
      }, time - this.now());
    }
  }
  /**
   * Trigger the attack portion of the note
   * @param  notes The notes to play. Accepts a single Frequency or an array of frequencies.
   * @param  time  The start time of the note.
   * @param velocity The velocity of the note.
   * @example
   * const synth = new Tone.PolySynth(Tone.FMSynth).toDestination();
   * // trigger a chord immediately with a velocity of 0.2
   * synth.triggerAttack(["Ab3", "C4", "F5"], Tone.now(), 0.2);
   */
  triggerAttack(notes, time, velocity) {
    if (!Array.isArray(notes)) {
      notes = [notes];
    }
    const computedTime = this.toSeconds(time);
    this._scheduleEvent("attack", notes, computedTime, velocity);
    return this;
  }
  /**
   * Trigger the release of the note. Unlike monophonic instruments,
   * a note (or array of notes) needs to be passed in as the first argument.
   * @param  notes The notes to play. Accepts a single Frequency or an array of frequencies.
   * @param  time  When the release will be triggered.
   * @example
   * const poly = new Tone.PolySynth(Tone.AMSynth).toDestination();
   * poly.triggerAttack(["Ab3", "C4", "F5"]);
   * // trigger the release of the given notes.
   * poly.triggerRelease(["Ab3", "C4"], "+1");
   * poly.triggerRelease("F5", "+3");
   */
  triggerRelease(notes, time) {
    if (!Array.isArray(notes)) {
      notes = [notes];
    }
    const computedTime = this.toSeconds(time);
    this._scheduleEvent("release", notes, computedTime);
    return this;
  }
  /**
   * Trigger the attack and release after the specified duration
   * @param  notes The notes to play. Accepts a single  Frequency or an array of frequencies.
   * @param  duration the duration of the note
   * @param  time  if no time is given, defaults to now
   * @param  velocity the velocity of the attack (0-1)
   * @example
   * const poly = new Tone.PolySynth(Tone.AMSynth).toDestination();
   * // can pass in an array of durations as well
   * poly.triggerAttackRelease(["Eb3", "G4", "Bb4", "D5"], [4, 3, 2, 1]);
   */
  triggerAttackRelease(notes, duration, time, velocity) {
    const computedTime = this.toSeconds(time);
    this.triggerAttack(notes, computedTime, velocity);
    if (isArray(duration)) {
      assert(isArray(notes), "If the duration is an array, the notes must also be an array");
      notes = notes;
      for (let i = 0; i < notes.length; i++) {
        const d = duration[Math.min(i, duration.length - 1)];
        const durationSeconds = this.toSeconds(d);
        assert(durationSeconds > 0, "The duration must be greater than 0");
        this.triggerRelease(notes[i], computedTime + durationSeconds);
      }
    } else {
      const durationSeconds = this.toSeconds(duration);
      assert(durationSeconds > 0, "The duration must be greater than 0");
      this.triggerRelease(notes, computedTime + durationSeconds);
    }
    return this;
  }
  sync() {
    if (this._syncState()) {
      this._syncMethod("triggerAttack", 1);
      this._syncMethod("triggerRelease", 1);
      this.context.transport.on("stop", this._syncedRelease);
      this.context.transport.on("pause", this._syncedRelease);
      this.context.transport.on("loopEnd", this._syncedRelease);
    }
    return this;
  }
  /**
   * Set a member/attribute of the voices
   * @example
   * const poly = new Tone.PolySynth().toDestination();
   * // set all of the voices using an options object for the synth type
   * poly.set({
   * 	envelope: {
   * 		attack: 0.25
   * 	}
   * });
   * poly.triggerAttackRelease("Bb3", 0.2);
   */
  set(options) {
    const sanitizedOptions = omitFromObject(options, [
      "onsilence",
      "context"
    ]);
    this.options = deepMerge(this.options, sanitizedOptions);
    this._voices.forEach((voice) => voice.set(sanitizedOptions));
    this._dummyVoice.set(sanitizedOptions);
    return this;
  }
  get() {
    return this._dummyVoice.get();
  }
  /**
   * Trigger the release portion of all the currently active voices immediately.
   * Useful for silencing the synth.
   */
  releaseAll(time) {
    const computedTime = this.toSeconds(time);
    this._activeVoices.forEach(({ voice }) => {
      voice.triggerRelease(computedTime);
    });
    return this;
  }
  dispose() {
    super.dispose();
    this._dummyVoice.dispose();
    this._voices.forEach((v) => v.dispose());
    this._activeVoices = [];
    this._availableVoices = [];
    this.context.clearInterval(this._gcTimeout);
    return this;
  }
};

// node_modules/tone/build/esm/instrument/Sampler.js
var Sampler = class _Sampler extends Instrument {
  constructor() {
    const options = optionsFromArguments(_Sampler.getDefaults(), arguments, ["urls", "onload", "baseUrl"], "urls");
    super(options);
    this.name = "Sampler";
    this._activeSources = /* @__PURE__ */ new Map();
    const urlMap = {};
    Object.keys(options.urls).forEach((note) => {
      const noteNumber = parseInt(note, 10);
      assert(isNote(note) || isNumber(noteNumber) && isFinite(noteNumber), `url key is neither a note or midi pitch: ${note}`);
      if (isNote(note)) {
        const mid = new FrequencyClass(this.context, note).toMidi();
        urlMap[mid] = options.urls[note];
      } else if (isNumber(noteNumber) && isFinite(noteNumber)) {
        urlMap[noteNumber] = options.urls[noteNumber];
      }
    });
    this._buffers = new ToneAudioBuffers({
      urls: urlMap,
      onload: options.onload,
      baseUrl: options.baseUrl,
      onerror: options.onerror
    });
    this.attack = options.attack;
    this.release = options.release;
    this.curve = options.curve;
    if (this._buffers.loaded) {
      Promise.resolve().then(options.onload);
    }
  }
  static getDefaults() {
    return Object.assign(Instrument.getDefaults(), {
      attack: 0,
      baseUrl: "",
      curve: "exponential",
      onload: noOp,
      onerror: noOp,
      release: 0.1,
      urls: {}
    });
  }
  /**
   * Returns the difference in steps between the given midi note at the closets sample.
   */
  _findClosest(midi) {
    const MAX_INTERVAL = 96;
    let interval = 0;
    while (interval < MAX_INTERVAL) {
      if (this._buffers.has(midi + interval)) {
        return -interval;
      } else if (this._buffers.has(midi - interval)) {
        return interval;
      }
      interval++;
    }
    throw new Error(`No available buffers for note: ${midi}`);
  }
  /**
   * @param  notes	The note to play, or an array of notes.
   * @param  time     When to play the note
   * @param  velocity The velocity to play the sample back.
   */
  triggerAttack(notes, time, velocity = 1) {
    this.log("triggerAttack", notes, time, velocity);
    if (!Array.isArray(notes)) {
      notes = [notes];
    }
    notes.forEach((note) => {
      const midiFloat = ftomf(new FrequencyClass(this.context, note).toFrequency());
      const midi = Math.round(midiFloat);
      const remainder = midiFloat - midi;
      const difference = this._findClosest(midi);
      const closestNote = midi - difference;
      const buffer = this._buffers.get(closestNote);
      const playbackRate = intervalToFrequencyRatio(difference + remainder);
      const source = new ToneBufferSource({
        url: buffer,
        context: this.context,
        curve: this.curve,
        fadeIn: this.attack,
        fadeOut: this.release,
        playbackRate
      }).connect(this.output);
      source.start(time, 0, buffer.duration / playbackRate, velocity);
      if (!isArray(this._activeSources.get(midi))) {
        this._activeSources.set(midi, []);
      }
      this._activeSources.get(midi).push(source);
      source.onended = () => {
        if (this._activeSources && this._activeSources.has(midi)) {
          const sources = this._activeSources.get(midi);
          const index = sources.indexOf(source);
          if (index !== -1) {
            sources.splice(index, 1);
          }
        }
      };
    });
    return this;
  }
  /**
   * @param  notes	The note to release, or an array of notes.
   * @param  time     	When to release the note.
   */
  triggerRelease(notes, time) {
    this.log("triggerRelease", notes, time);
    if (!Array.isArray(notes)) {
      notes = [notes];
    }
    notes.forEach((note) => {
      const midi = new FrequencyClass(this.context, note).toMidi();
      if (this._activeSources.has(midi) && this._activeSources.get(midi).length) {
        const sources = this._activeSources.get(midi);
        time = this.toSeconds(time);
        sources.forEach((source) => {
          source.stop(time);
        });
        this._activeSources.set(midi, []);
      }
    });
    return this;
  }
  /**
   * Release all currently active notes.
   * @param  time     	When to release the notes.
   */
  releaseAll(time) {
    const computedTime = this.toSeconds(time);
    this._activeSources.forEach((sources) => {
      while (sources.length) {
        const source = sources.shift();
        source.stop(computedTime);
      }
    });
    return this;
  }
  sync() {
    if (this._syncState()) {
      this._syncMethod("triggerAttack", 1);
      this._syncMethod("triggerRelease", 1);
    }
    return this;
  }
  /**
   * Invoke the attack phase, then after the duration, invoke the release.
   * @param  notes	The note to play and release, or an array of notes.
   * @param  duration The time the note should be held
   * @param  time     When to start the attack
   * @param  velocity The velocity of the attack
   */
  triggerAttackRelease(notes, duration, time, velocity = 1) {
    const computedTime = this.toSeconds(time);
    this.triggerAttack(notes, computedTime, velocity);
    if (isArray(duration)) {
      assert(isArray(notes), "notes must be an array when duration is array");
      notes.forEach((note, index) => {
        const d = duration[Math.min(index, duration.length - 1)];
        this.triggerRelease(note, computedTime + this.toSeconds(d));
      });
    } else {
      this.triggerRelease(notes, computedTime + this.toSeconds(duration));
    }
    return this;
  }
  /**
   * Add a note to the sampler.
   * @param  note      The buffer's pitch.
   * @param  url  Either the url of the buffer, or a buffer which will be added with the given name.
   * @param  callback  The callback to invoke when the url is loaded.
   */
  add(note, url, callback) {
    assert(isNote(note) || isFinite(note), `note must be a pitch or midi: ${note}`);
    if (isNote(note)) {
      const mid = new FrequencyClass(this.context, note).toMidi();
      this._buffers.add(mid, url, callback);
    } else {
      this._buffers.add(note, url, callback);
    }
    return this;
  }
  /**
   * If the buffers are loaded or not
   */
  get loaded() {
    return this._buffers.loaded;
  }
  /**
   * Clean up
   */
  dispose() {
    super.dispose();
    this._buffers.dispose();
    this._activeSources.forEach((sources) => {
      sources.forEach((source) => source.dispose());
    });
    this._activeSources.clear();
    return this;
  }
};
__decorate([
  timeRange(0)
], Sampler.prototype, "attack", void 0);
__decorate([
  timeRange(0)
], Sampler.prototype, "release", void 0);

// node_modules/tone/build/esm/component/channel/CrossFade.js
var CrossFade = class _CrossFade extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_CrossFade.getDefaults(), arguments, ["fade"]);
    super(options);
    this.name = "CrossFade";
    this._panner = this.context.createStereoPanner();
    this._split = this.context.createChannelSplitter(2);
    this._g2a = new GainToAudio({ context: this.context });
    this.a = new Gain({
      context: this.context,
      gain: 0
    });
    this.b = new Gain({
      context: this.context,
      gain: 0
    });
    this.output = new Gain({ context: this.context });
    this._internalChannels = [this.a, this.b];
    this.fade = new Signal({
      context: this.context,
      units: "normalRange",
      value: options.fade
    });
    readOnly(this, "fade");
    this.context.getConstant(1).connect(this._panner);
    this._panner.connect(this._split);
    this._panner.channelCount = 1;
    this._panner.channelCountMode = "explicit";
    connect(this._split, this.a.gain, 0);
    connect(this._split, this.b.gain, 1);
    this.fade.chain(this._g2a, this._panner.pan);
    this.a.connect(this.output);
    this.b.connect(this.output);
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      fade: 0.5
    });
  }
  dispose() {
    super.dispose();
    this.a.dispose();
    this.b.dispose();
    this.output.dispose();
    this.fade.dispose();
    this._g2a.dispose();
    this._panner.disconnect();
    this._split.disconnect();
    return this;
  }
};

// node_modules/tone/build/esm/effect/Effect.js
var Effect = class extends ToneAudioNode {
  constructor(options) {
    super(options);
    this.name = "Effect";
    this._dryWet = new CrossFade({ context: this.context });
    this.wet = this._dryWet.fade;
    this.effectSend = new Gain({ context: this.context });
    this.effectReturn = new Gain({ context: this.context });
    this.input = new Gain({ context: this.context });
    this.output = this._dryWet;
    this.input.fan(this._dryWet.a, this.effectSend);
    this.effectReturn.connect(this._dryWet.b);
    this.wet.setValueAtTime(options.wet, 0);
    this._internalChannels = [this.effectReturn, this.effectSend];
    readOnly(this, "wet");
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      wet: 1
    });
  }
  /**
   * chains the effect in between the effectSend and effectReturn
   */
  connectEffect(effect) {
    this._internalChannels.push(effect);
    this.effectSend.chain(effect, this.effectReturn);
    return this;
  }
  dispose() {
    super.dispose();
    this._dryWet.dispose();
    this.effectSend.dispose();
    this.effectReturn.dispose();
    this.wet.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/channel/Panner.js
var Panner = class _Panner extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Panner.getDefaults(), arguments, [
      "pan"
    ]);
    super(options);
    this.name = "Panner";
    this._panner = this.context.createStereoPanner();
    this.input = this._panner;
    this.output = this._panner;
    this.pan = new Param({
      context: this.context,
      param: this._panner.pan,
      value: options.pan,
      minValue: -1,
      maxValue: 1
    });
    this._panner.channelCount = options.channelCount;
    this._panner.channelCountMode = "explicit";
    readOnly(this, "pan");
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      pan: 0,
      channelCount: 1
    });
  }
  dispose() {
    super.dispose();
    this._panner.disconnect();
    this.pan.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/effect/BitCrusher.worklet.js
var workletName2 = "bit-crusher";
var bitCrusherWorklet = (
  /* javascript */
  `
	class BitCrusherWorklet extends SingleIOProcessor {

		static get parameterDescriptors() {
			return [{
				name: "bits",
				defaultValue: 12,
				minValue: 1,
				maxValue: 16,
				automationRate: 'k-rate'
			}];
		}

		generate(input, _channel, parameters) {
			const step = Math.pow(0.5, parameters.bits - 1);
			const val = step * Math.floor(input / step + 0.5);
			return val;
		}
	}
`
);
registerProcessor(workletName2, bitCrusherWorklet);

// node_modules/tone/build/esm/effect/BitCrusher.js
var BitCrusher = class _BitCrusher extends Effect {
  constructor() {
    const options = optionsFromArguments(_BitCrusher.getDefaults(), arguments, ["bits"]);
    super(options);
    this.name = "BitCrusher";
    this._bitCrusherWorklet = new BitCrusherWorklet({
      context: this.context,
      bits: options.bits
    });
    this.connectEffect(this._bitCrusherWorklet);
    this.bits = this._bitCrusherWorklet.bits;
  }
  static getDefaults() {
    return Object.assign(Effect.getDefaults(), {
      bits: 4
    });
  }
  dispose() {
    super.dispose();
    this._bitCrusherWorklet.dispose();
    return this;
  }
};
var BitCrusherWorklet = class _BitCrusherWorklet extends ToneAudioWorklet {
  constructor() {
    const options = optionsFromArguments(_BitCrusherWorklet.getDefaults(), arguments);
    super(options);
    this.name = "BitCrusherWorklet";
    this.input = new Gain({ context: this.context });
    this.output = new Gain({ context: this.context });
    this.bits = new Param({
      context: this.context,
      value: options.bits,
      units: "positive",
      minValue: 1,
      maxValue: 16,
      param: this._dummyParam,
      swappable: true
    });
  }
  static getDefaults() {
    return Object.assign(ToneAudioWorklet.getDefaults(), {
      bits: 12
    });
  }
  _audioWorkletName() {
    return workletName2;
  }
  onReady(node) {
    connectSeries(this.input, node, this.output);
    const bits = node.parameters.get("bits");
    this.bits.setParam(bits);
  }
  dispose() {
    super.dispose();
    this.input.dispose();
    this.output.dispose();
    this.bits.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/channel/Split.js
var Split = class _Split extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Split.getDefaults(), arguments, [
      "channels"
    ]);
    super(options);
    this.name = "Split";
    this._splitter = this.input = this.output = this.context.createChannelSplitter(options.channels);
    this._internalChannels = [this._splitter];
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      channels: 2
    });
  }
  dispose() {
    super.dispose();
    this._splitter.disconnect();
    return this;
  }
};

// node_modules/tone/build/esm/component/channel/Merge.js
var Merge = class _Merge extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Merge.getDefaults(), arguments, [
      "channels"
    ]);
    super(options);
    this.name = "Merge";
    this._merger = this.output = this.input = this.context.createChannelMerger(options.channels);
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      channels: 2
    });
  }
  dispose() {
    super.dispose();
    this._merger.disconnect();
    return this;
  }
};

// node_modules/tone/build/esm/effect/StereoEffect.js
var StereoEffect = class extends ToneAudioNode {
  constructor(options) {
    super(options);
    this.name = "StereoEffect";
    this.input = new Gain({ context: this.context });
    this.input.channelCount = 2;
    this.input.channelCountMode = "explicit";
    this._dryWet = this.output = new CrossFade({
      context: this.context,
      fade: options.wet
    });
    this.wet = this._dryWet.fade;
    this._split = new Split({ context: this.context, channels: 2 });
    this._merge = new Merge({ context: this.context, channels: 2 });
    this.input.connect(this._split);
    this.input.connect(this._dryWet.a);
    this._merge.connect(this._dryWet.b);
    readOnly(this, ["wet"]);
  }
  /**
   * Connect the left part of the effect
   */
  connectEffectLeft(...nodes) {
    this._split.connect(nodes[0], 0, 0);
    connectSeries(...nodes);
    connect(nodes[nodes.length - 1], this._merge, 0, 0);
  }
  /**
   * Connect the right part of the effect
   */
  connectEffectRight(...nodes) {
    this._split.connect(nodes[0], 1, 0);
    connectSeries(...nodes);
    connect(nodes[nodes.length - 1], this._merge, 0, 1);
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      wet: 1
    });
  }
  dispose() {
    super.dispose();
    this._dryWet.dispose();
    this._split.dispose();
    this._merge.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/effect/StereoFeedbackEffect.js
var StereoFeedbackEffect = class extends StereoEffect {
  constructor(options) {
    super(options);
    this.feedback = new Signal({
      context: this.context,
      value: options.feedback,
      units: "normalRange"
    });
    this._feedbackL = new Gain({ context: this.context });
    this._feedbackR = new Gain({ context: this.context });
    this._feedbackSplit = new Split({ context: this.context, channels: 2 });
    this._feedbackMerge = new Merge({ context: this.context, channels: 2 });
    this._merge.connect(this._feedbackSplit);
    this._feedbackMerge.connect(this._split);
    this._feedbackSplit.connect(this._feedbackL, 0, 0);
    this._feedbackL.connect(this._feedbackMerge, 0, 0);
    this._feedbackSplit.connect(this._feedbackR, 1, 0);
    this._feedbackR.connect(this._feedbackMerge, 0, 1);
    this.feedback.fan(this._feedbackL.gain, this._feedbackR.gain);
    readOnly(this, ["feedback"]);
  }
  static getDefaults() {
    return Object.assign(StereoEffect.getDefaults(), {
      feedback: 0.5
    });
  }
  dispose() {
    super.dispose();
    this.feedback.dispose();
    this._feedbackL.dispose();
    this._feedbackR.dispose();
    this._feedbackSplit.dispose();
    this._feedbackMerge.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/effect/Chorus.js
var Chorus = class _Chorus extends StereoFeedbackEffect {
  constructor() {
    const options = optionsFromArguments(_Chorus.getDefaults(), arguments, [
      "frequency",
      "delayTime",
      "depth"
    ]);
    super(options);
    this.name = "Chorus";
    this._depth = options.depth;
    this._delayTime = options.delayTime / 1e3;
    this._lfoL = new LFO({
      context: this.context,
      frequency: options.frequency,
      min: 0,
      max: 1
    });
    this._lfoR = new LFO({
      context: this.context,
      frequency: options.frequency,
      min: 0,
      max: 1,
      phase: 180
    });
    this._delayNodeL = new Delay({ context: this.context });
    this._delayNodeR = new Delay({ context: this.context });
    this.frequency = this._lfoL.frequency;
    readOnly(this, ["frequency"]);
    this._lfoL.frequency.connect(this._lfoR.frequency);
    this.connectEffectLeft(this._delayNodeL);
    this.connectEffectRight(this._delayNodeR);
    this._lfoL.connect(this._delayNodeL.delayTime);
    this._lfoR.connect(this._delayNodeR.delayTime);
    this.depth = this._depth;
    this.type = options.type;
    this.spread = options.spread;
  }
  static getDefaults() {
    return Object.assign(StereoFeedbackEffect.getDefaults(), {
      frequency: 1.5,
      delayTime: 3.5,
      depth: 0.7,
      type: "sine",
      spread: 180,
      feedback: 0,
      wet: 0.5
    });
  }
  /**
   * The depth of the effect. A depth of 1 makes the delayTime
   * modulate between 0 and 2*delayTime (centered around the delayTime).
   */
  get depth() {
    return this._depth;
  }
  set depth(depth) {
    this._depth = depth;
    const deviation = this._delayTime * depth;
    this._lfoL.min = Math.max(this._delayTime - deviation, 0);
    this._lfoL.max = this._delayTime + deviation;
    this._lfoR.min = Math.max(this._delayTime - deviation, 0);
    this._lfoR.max = this._delayTime + deviation;
  }
  /**
   * The delayTime in milliseconds of the chorus. A larger delayTime
   * will give a more pronounced effect. Nominal range a delayTime
   * is between 2 and 20ms.
   */
  get delayTime() {
    return this._delayTime * 1e3;
  }
  set delayTime(delayTime) {
    this._delayTime = delayTime / 1e3;
    this.depth = this._depth;
  }
  /**
   * The oscillator type of the LFO.
   */
  get type() {
    return this._lfoL.type;
  }
  set type(type) {
    this._lfoL.type = type;
    this._lfoR.type = type;
  }
  /**
   * Amount of stereo spread. When set to 0, both LFO's will be panned centrally.
   * When set to 180, LFO's will be panned hard left and right respectively.
   */
  get spread() {
    return this._lfoR.phase - this._lfoL.phase;
  }
  set spread(spread) {
    this._lfoL.phase = 90 - spread / 2;
    this._lfoR.phase = spread / 2 + 90;
  }
  /**
   * Start the effect.
   */
  start(time) {
    this._lfoL.start(time);
    this._lfoR.start(time);
    return this;
  }
  /**
   * Stop the lfo
   */
  stop(time) {
    this._lfoL.stop(time);
    this._lfoR.stop(time);
    return this;
  }
  /**
   * Sync the filter to the transport.
   * @see {@link LFO.sync}
   */
  sync() {
    this._lfoL.sync();
    this._lfoR.sync();
    return this;
  }
  /**
   * Unsync the filter from the transport.
   */
  unsync() {
    this._lfoL.unsync();
    this._lfoR.unsync();
    return this;
  }
  dispose() {
    super.dispose();
    this._lfoL.dispose();
    this._lfoR.dispose();
    this._delayNodeL.dispose();
    this._delayNodeR.dispose();
    this.frequency.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/effect/Distortion.js
var Distortion = class _Distortion extends Effect {
  constructor() {
    const options = optionsFromArguments(_Distortion.getDefaults(), arguments, ["distortion"]);
    super(options);
    this.name = "Distortion";
    this._shaper = new WaveShaper({
      context: this.context,
      length: 4096
    });
    this._distortion = options.distortion;
    this.connectEffect(this._shaper);
    this.distortion = options.distortion;
    this.oversample = options.oversample;
  }
  static getDefaults() {
    return Object.assign(Effect.getDefaults(), {
      distortion: 0.4,
      oversample: "none"
    });
  }
  /**
   * The amount of distortion. Nominal range is between 0 and 1.
   */
  get distortion() {
    return this._distortion;
  }
  set distortion(amount) {
    this._distortion = amount;
    const k = amount * 100;
    const deg = Math.PI / 180;
    this._shaper.setMap((x) => {
      if (Math.abs(x) < 1e-3) {
        return 0;
      } else {
        return (3 + k) * x * 20 * deg / (Math.PI + k * Math.abs(x));
      }
    });
  }
  /**
   * The oversampling of the effect. Can either be "none", "2x" or "4x".
   */
  get oversample() {
    return this._shaper.oversample;
  }
  set oversample(oversampling) {
    this._shaper.oversample = oversampling;
  }
  dispose() {
    super.dispose();
    this._shaper.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/effect/FeedbackEffect.js
var FeedbackEffect = class extends Effect {
  constructor(options) {
    super(options);
    this.name = "FeedbackEffect";
    this._feedbackGain = new Gain({
      context: this.context,
      gain: options.feedback,
      units: "normalRange"
    });
    this.feedback = this._feedbackGain.gain;
    readOnly(this, "feedback");
    this.effectReturn.chain(this._feedbackGain, this.effectSend);
  }
  static getDefaults() {
    return Object.assign(Effect.getDefaults(), {
      feedback: 0.125
    });
  }
  dispose() {
    super.dispose();
    this._feedbackGain.dispose();
    this.feedback.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/effect/FeedbackDelay.js
var FeedbackDelay = class _FeedbackDelay extends FeedbackEffect {
  constructor() {
    const options = optionsFromArguments(_FeedbackDelay.getDefaults(), arguments, ["delayTime", "feedback"]);
    super(options);
    this.name = "FeedbackDelay";
    this._delayNode = new Delay({
      context: this.context,
      delayTime: options.delayTime,
      maxDelay: options.maxDelay
    });
    this.delayTime = this._delayNode.delayTime;
    this.connectEffect(this._delayNode);
    readOnly(this, "delayTime");
  }
  static getDefaults() {
    return Object.assign(FeedbackEffect.getDefaults(), {
      delayTime: 0.25,
      maxDelay: 1
    });
  }
  dispose() {
    super.dispose();
    this._delayNode.dispose();
    this.delayTime.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/effect/Freeverb.js
var combFilterTunings = [
  1557 / 44100,
  1617 / 44100,
  1491 / 44100,
  1422 / 44100,
  1277 / 44100,
  1356 / 44100,
  1188 / 44100,
  1116 / 44100
];

// node_modules/tone/build/esm/effect/JCReverb.js
var combFilterDelayTimes = [
  1687 / 25e3,
  1601 / 25e3,
  2053 / 25e3,
  2251 / 25e3
];

// node_modules/tone/build/esm/effect/Phaser.js
var Phaser = class _Phaser extends StereoEffect {
  constructor() {
    const options = optionsFromArguments(_Phaser.getDefaults(), arguments, [
      "frequency",
      "octaves",
      "baseFrequency"
    ]);
    super(options);
    this.name = "Phaser";
    this._lfoL = new LFO({
      context: this.context,
      frequency: options.frequency,
      min: 0,
      max: 1
    });
    this._lfoR = new LFO({
      context: this.context,
      frequency: options.frequency,
      min: 0,
      max: 1,
      phase: 180
    });
    this._baseFrequency = this.toFrequency(options.baseFrequency);
    this._octaves = options.octaves;
    this.Q = new Signal({
      context: this.context,
      value: options.Q,
      units: "positive"
    });
    this._filtersL = this._makeFilters(options.stages, this._lfoL);
    this._filtersR = this._makeFilters(options.stages, this._lfoR);
    this.frequency = this._lfoL.frequency;
    this.frequency.value = options.frequency;
    this.connectEffectLeft(...this._filtersL);
    this.connectEffectRight(...this._filtersR);
    this._lfoL.frequency.connect(this._lfoR.frequency);
    this.baseFrequency = options.baseFrequency;
    this.octaves = options.octaves;
    this._lfoL.start();
    this._lfoR.start();
    readOnly(this, ["frequency", "Q"]);
  }
  static getDefaults() {
    return Object.assign(StereoEffect.getDefaults(), {
      frequency: 0.5,
      octaves: 3,
      stages: 10,
      Q: 10,
      baseFrequency: 350
    });
  }
  _makeFilters(stages, connectToFreq) {
    const filters = [];
    for (let i = 0; i < stages; i++) {
      const filter = this.context.createBiquadFilter();
      filter.type = "allpass";
      this.Q.connect(filter.Q);
      connectToFreq.connect(filter.frequency);
      filters.push(filter);
    }
    return filters;
  }
  /**
   * The number of octaves the phase goes above the baseFrequency
   */
  get octaves() {
    return this._octaves;
  }
  set octaves(octaves) {
    this._octaves = octaves;
    const max = this._baseFrequency * Math.pow(2, octaves);
    this._lfoL.max = max;
    this._lfoR.max = max;
  }
  /**
   * The the base frequency of the filters.
   */
  get baseFrequency() {
    return this._baseFrequency;
  }
  set baseFrequency(freq) {
    this._baseFrequency = this.toFrequency(freq);
    this._lfoL.min = this._baseFrequency;
    this._lfoR.min = this._baseFrequency;
    this.octaves = this._octaves;
  }
  dispose() {
    super.dispose();
    this.Q.dispose();
    this._lfoL.dispose();
    this._lfoR.dispose();
    this._filtersL.forEach((f) => f.disconnect());
    this._filtersR.forEach((f) => f.disconnect());
    this.frequency.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/effect/Reverb.js
var Reverb = class _Reverb extends Effect {
  constructor() {
    const options = optionsFromArguments(_Reverb.getDefaults(), arguments, [
      "decay"
    ]);
    super(options);
    this.name = "Reverb";
    this._convolver = this.context.createConvolver();
    this.ready = Promise.resolve();
    const decayTime = this.toSeconds(options.decay);
    assertRange(decayTime, 1e-3);
    this._decay = decayTime;
    const preDelayTime = this.toSeconds(options.preDelay);
    assertRange(preDelayTime, 0);
    this._preDelay = preDelayTime;
    this.generate();
    this.connectEffect(this._convolver);
  }
  static getDefaults() {
    return Object.assign(Effect.getDefaults(), {
      decay: 1.5,
      preDelay: 0.01
    });
  }
  /**
   * The duration of the reverb.
   */
  get decay() {
    return this._decay;
  }
  set decay(time) {
    time = this.toSeconds(time);
    assertRange(time, 1e-3);
    this._decay = time;
    this.generate();
  }
  /**
   * The amount of time before the reverb is fully ramped in.
   */
  get preDelay() {
    return this._preDelay;
  }
  set preDelay(time) {
    time = this.toSeconds(time);
    assertRange(time, 0);
    this._preDelay = time;
    this.generate();
  }
  /**
   * Generate the Impulse Response. Returns a promise while the IR is being generated.
   * @return Promise which returns this object.
   */
  generate() {
    return __awaiter(this, void 0, void 0, function* () {
      const previousReady = this.ready;
      const context2 = new OfflineContext(2, this._decay + this._preDelay, this.context.sampleRate);
      const noiseL = new Noise({ context: context2 });
      const noiseR = new Noise({ context: context2 });
      const merge = new Merge({ context: context2 });
      noiseL.connect(merge, 0, 0);
      noiseR.connect(merge, 0, 1);
      const gainNode = new Gain({ context: context2 }).toDestination();
      merge.connect(gainNode);
      noiseL.start(0);
      noiseR.start(0);
      gainNode.gain.setValueAtTime(0, 0);
      gainNode.gain.setValueAtTime(1, this._preDelay);
      gainNode.gain.exponentialApproachValueAtTime(0, this._preDelay, this.decay);
      const renderPromise = context2.render();
      this.ready = renderPromise.then(noOp);
      yield previousReady;
      this._convolver.buffer = (yield renderPromise).get();
      return this;
    });
  }
  dispose() {
    super.dispose();
    this._convolver.disconnect();
    return this;
  }
};

// node_modules/tone/build/esm/effect/Vibrato.js
var Vibrato = class _Vibrato extends Effect {
  constructor() {
    const options = optionsFromArguments(_Vibrato.getDefaults(), arguments, [
      "frequency",
      "depth"
    ]);
    super(options);
    this.name = "Vibrato";
    this._delayNode = new Delay({
      context: this.context,
      delayTime: 0,
      maxDelay: options.maxDelay
    });
    this._lfo = new LFO({
      context: this.context,
      type: options.type,
      min: 0,
      max: options.maxDelay,
      frequency: options.frequency,
      phase: -90
      // offse the phase so the resting position is in the center
    }).start().connect(this._delayNode.delayTime);
    this.frequency = this._lfo.frequency;
    this.depth = this._lfo.amplitude;
    this.depth.value = options.depth;
    readOnly(this, ["frequency", "depth"]);
    this.effectSend.chain(this._delayNode, this.effectReturn);
  }
  static getDefaults() {
    return Object.assign(Effect.getDefaults(), {
      maxDelay: 5e-3,
      frequency: 5,
      depth: 0.1,
      type: "sine"
    });
  }
  /**
   * Type of oscillator attached to the Vibrato.
   */
  get type() {
    return this._lfo.type;
  }
  set type(type) {
    this._lfo.type = type;
  }
  dispose() {
    super.dispose();
    this._delayNode.dispose();
    this._lfo.dispose();
    this.frequency.dispose();
    this.depth.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/analysis/Analyser.js
var Analyser = class _Analyser extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Analyser.getDefaults(), arguments, ["type", "size"]);
    super(options);
    this.name = "Analyser";
    this._analysers = [];
    this._buffers = [];
    this.input = this.output = this._gain = new Gain({ context: this.context });
    this._split = new Split({
      context: this.context,
      channels: options.channels
    });
    this.input.connect(this._split);
    assertRange(options.channels, 1);
    for (let channel = 0; channel < options.channels; channel++) {
      this._analysers[channel] = this.context.createAnalyser();
      this._split.connect(this._analysers[channel], channel, 0);
    }
    this.size = options.size;
    this.type = options.type;
    this.smoothing = options.smoothing;
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      size: 1024,
      smoothing: 0.8,
      type: "fft",
      channels: 1
    });
  }
  /**
   * Run the analysis given the current settings. If {@link channels} = 1,
   * it will return a Float32Array. If {@link channels} > 1, it will
   * return an array of Float32Arrays where each index in the array
   * represents the analysis done on a channel.
   */
  getValue() {
    this._analysers.forEach((analyser, index) => {
      const buffer = this._buffers[index];
      if (this._type === "fft") {
        analyser.getFloatFrequencyData(buffer);
      } else if (this._type === "waveform") {
        analyser.getFloatTimeDomainData(buffer);
      }
    });
    if (this.channels === 1) {
      return this._buffers[0];
    } else {
      return this._buffers;
    }
  }
  /**
   * The size of analysis. This must be a power of two in the range 16 to 16384.
   */
  get size() {
    return this._analysers[0].frequencyBinCount;
  }
  set size(size) {
    this._analysers.forEach((analyser, index) => {
      analyser.fftSize = size * 2;
      this._buffers[index] = new Float32Array(size);
    });
  }
  /**
   * The number of channels the analyser does the analysis on. Channel
   * separation is done using {@link Split}
   */
  get channels() {
    return this._analysers.length;
  }
  /**
   * The analysis function returned by analyser.getValue(), either "fft" or "waveform".
   */
  get type() {
    return this._type;
  }
  set type(type) {
    assert(type === "waveform" || type === "fft", `Analyser: invalid type: ${type}`);
    this._type = type;
  }
  /**
   * 0 represents no time averaging with the last analysis frame.
   */
  get smoothing() {
    return this._analysers[0].smoothingTimeConstant;
  }
  set smoothing(val) {
    this._analysers.forEach((a) => a.smoothingTimeConstant = val);
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    this._analysers.forEach((a) => a.disconnect());
    this._split.dispose();
    this._gain.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/channel/Solo.js
var Solo = class _Solo extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Solo.getDefaults(), arguments, [
      "solo"
    ]);
    super(options);
    this.name = "Solo";
    this.input = this.output = new Gain({
      context: this.context
    });
    if (!_Solo._allSolos.has(this.context)) {
      _Solo._allSolos.set(this.context, /* @__PURE__ */ new Set());
    }
    _Solo._allSolos.get(this.context).add(this);
    this.solo = options.solo;
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      solo: false
    });
  }
  /**
   * Isolates this instance and mutes all other instances of Solo.
   * Only one instance can be soloed at a time. A soloed
   * instance will report `solo=false` when another instance is soloed.
   */
  get solo() {
    return this._isSoloed();
  }
  set solo(solo) {
    if (solo) {
      this._addSolo();
    } else {
      this._removeSolo();
    }
    _Solo._allSolos.get(this.context).forEach((instance) => instance._updateSolo());
  }
  /**
   * If the current instance is muted, i.e. another instance is soloed
   */
  get muted() {
    return this.input.gain.value === 0;
  }
  /**
   * Add this to the soloed array
   */
  _addSolo() {
    if (!_Solo._soloed.has(this.context)) {
      _Solo._soloed.set(this.context, /* @__PURE__ */ new Set());
    }
    _Solo._soloed.get(this.context).add(this);
  }
  /**
   * Remove this from the soloed array
   */
  _removeSolo() {
    if (_Solo._soloed.has(this.context)) {
      _Solo._soloed.get(this.context).delete(this);
    }
  }
  /**
   * Is this on the soloed array
   */
  _isSoloed() {
    return _Solo._soloed.has(this.context) && _Solo._soloed.get(this.context).has(this);
  }
  /**
   * Returns true if no one is soloed
   */
  _noSolos() {
    return !_Solo._soloed.has(this.context) || // or has a solo set but doesn't include any items
    _Solo._soloed.has(this.context) && _Solo._soloed.get(this.context).size === 0;
  }
  /**
   * Solo the current instance and unsolo all other instances.
   */
  _updateSolo() {
    if (this._isSoloed()) {
      this.input.gain.value = 1;
    } else if (this._noSolos()) {
      this.input.gain.value = 1;
    } else {
      this.input.gain.value = 0;
    }
  }
  dispose() {
    super.dispose();
    _Solo._allSolos.get(this.context).delete(this);
    this._removeSolo();
    return this;
  }
};
Solo._allSolos = /* @__PURE__ */ new Map();
Solo._soloed = /* @__PURE__ */ new Map();

// node_modules/tone/build/esm/component/channel/PanVol.js
var PanVol = class _PanVol extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_PanVol.getDefaults(), arguments, [
      "pan",
      "volume"
    ]);
    super(options);
    this.name = "PanVol";
    this._panner = this.input = new Panner({
      context: this.context,
      pan: options.pan,
      channelCount: options.channelCount
    });
    this.pan = this._panner.pan;
    this._volume = this.output = new Volume({
      context: this.context,
      volume: options.volume
    });
    this.volume = this._volume.volume;
    this._panner.connect(this._volume);
    this.mute = options.mute;
    readOnly(this, ["pan", "volume"]);
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      mute: false,
      pan: 0,
      volume: 0,
      channelCount: 1
    });
  }
  /**
   * Mute/unmute the volume
   */
  get mute() {
    return this._volume.mute;
  }
  set mute(mute) {
    this._volume.mute = mute;
  }
  dispose() {
    super.dispose();
    this._panner.dispose();
    this.pan.dispose();
    this._volume.dispose();
    this.volume.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/channel/Channel.js
var Channel = class _Channel extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Channel.getDefaults(), arguments, [
      "volume",
      "pan"
    ]);
    super(options);
    this.name = "Channel";
    this._solo = this.input = new Solo({
      solo: options.solo,
      context: this.context
    });
    this._panVol = this.output = new PanVol({
      context: this.context,
      pan: options.pan,
      volume: options.volume,
      mute: options.mute,
      channelCount: options.channelCount
    });
    this.pan = this._panVol.pan;
    this.volume = this._panVol.volume;
    this._solo.connect(this._panVol);
    readOnly(this, ["pan", "volume"]);
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      pan: 0,
      volume: 0,
      mute: false,
      solo: false,
      channelCount: 1
    });
  }
  /**
   * Solo/unsolo the channel. Soloing is only relative to other {@link Channel}s and {@link Solo} instances
   */
  get solo() {
    return this._solo.solo;
  }
  set solo(solo) {
    this._solo.solo = solo;
  }
  /**
   * If the current instance is muted, i.e. another instance is soloed,
   * or the channel is muted
   */
  get muted() {
    return this._solo.muted || this.mute;
  }
  /**
   * Mute/unmute the volume
   */
  get mute() {
    return this._panVol.mute;
  }
  set mute(mute) {
    this._panVol.mute = mute;
  }
  /**
   * Get the gain node belonging to the bus name. Create it if
   * it doesn't exist
   * @param name The bus name
   */
  _getBus(name) {
    if (!_Channel.buses.has(name)) {
      _Channel.buses.set(name, new Gain({ context: this.context }));
    }
    return _Channel.buses.get(name);
  }
  /**
   * Send audio to another channel using a string. `send` is a lot like
   * {@link connect}, except it uses a string instead of an object. This can
   * be useful in large applications to decouple sections since {@link send}
   * and {@link receive} can be invoked separately in order to connect an object
   * @param name The channel name to send the audio
   * @param volume The amount of the signal to send.
   * 	Defaults to 0db, i.e. send the entire signal
   * @returns Returns the gain node of this connection.
   */
  send(name, volume = 0) {
    const bus = this._getBus(name);
    const sendKnob = new Gain({
      context: this.context,
      units: "decibels",
      gain: volume
    });
    this.connect(sendKnob);
    sendKnob.connect(bus);
    return sendKnob;
  }
  /**
   * Receive audio from a channel which was connected with {@link send}.
   * @param name The channel name to receive audio from.
   */
  receive(name) {
    const bus = this._getBus(name);
    bus.connect(this);
    return this;
  }
  dispose() {
    super.dispose();
    this._panVol.dispose();
    this.pan.dispose();
    this.volume.dispose();
    this._solo.dispose();
    return this;
  }
};
Channel.buses = /* @__PURE__ */ new Map();

// node_modules/tone/build/esm/component/channel/MultibandSplit.js
var MultibandSplit = class _MultibandSplit extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_MultibandSplit.getDefaults(), arguments, ["lowFrequency", "highFrequency"]);
    super(options);
    this.name = "MultibandSplit";
    this.input = new Gain({ context: this.context });
    this.output = void 0;
    this.low = new Filter({
      context: this.context,
      frequency: 0,
      type: "lowpass"
    });
    this._lowMidFilter = new Filter({
      context: this.context,
      frequency: 0,
      type: "highpass"
    });
    this.mid = new Filter({
      context: this.context,
      frequency: 0,
      type: "lowpass"
    });
    this.high = new Filter({
      context: this.context,
      frequency: 0,
      type: "highpass"
    });
    this._internalChannels = [this.low, this.mid, this.high];
    this.lowFrequency = new Signal({
      context: this.context,
      units: "frequency",
      value: options.lowFrequency
    });
    this.highFrequency = new Signal({
      context: this.context,
      units: "frequency",
      value: options.highFrequency
    });
    this.Q = new Signal({
      context: this.context,
      units: "positive",
      value: options.Q
    });
    this.input.fan(this.low, this.high);
    this.input.chain(this._lowMidFilter, this.mid);
    this.lowFrequency.fan(this.low.frequency, this._lowMidFilter.frequency);
    this.highFrequency.fan(this.mid.frequency, this.high.frequency);
    this.Q.connect(this.low.Q);
    this.Q.connect(this._lowMidFilter.Q);
    this.Q.connect(this.mid.Q);
    this.Q.connect(this.high.Q);
    readOnly(this, ["high", "mid", "low", "highFrequency", "lowFrequency"]);
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      Q: 1,
      highFrequency: 2500,
      lowFrequency: 400
    });
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    writable(this, ["high", "mid", "low", "highFrequency", "lowFrequency"]);
    this.low.dispose();
    this._lowMidFilter.dispose();
    this.mid.dispose();
    this.high.dispose();
    this.lowFrequency.dispose();
    this.highFrequency.dispose();
    this.Q.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/dynamics/Compressor.js
var Compressor = class _Compressor extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Compressor.getDefaults(), arguments, ["threshold", "ratio"]);
    super(options);
    this.name = "Compressor";
    this._compressor = this.context.createDynamicsCompressor();
    this.input = this._compressor;
    this.output = this._compressor;
    this.threshold = new Param({
      minValue: this._compressor.threshold.minValue,
      maxValue: this._compressor.threshold.maxValue,
      context: this.context,
      convert: false,
      param: this._compressor.threshold,
      units: "decibels",
      value: options.threshold
    });
    this.attack = new Param({
      minValue: this._compressor.attack.minValue,
      maxValue: this._compressor.attack.maxValue,
      context: this.context,
      param: this._compressor.attack,
      units: "time",
      value: options.attack
    });
    this.release = new Param({
      minValue: this._compressor.release.minValue,
      maxValue: this._compressor.release.maxValue,
      context: this.context,
      param: this._compressor.release,
      units: "time",
      value: options.release
    });
    this.knee = new Param({
      minValue: this._compressor.knee.minValue,
      maxValue: this._compressor.knee.maxValue,
      context: this.context,
      convert: false,
      param: this._compressor.knee,
      units: "decibels",
      value: options.knee
    });
    this.ratio = new Param({
      minValue: this._compressor.ratio.minValue,
      maxValue: this._compressor.ratio.maxValue,
      context: this.context,
      convert: false,
      param: this._compressor.ratio,
      units: "positive",
      value: options.ratio
    });
    readOnly(this, ["knee", "release", "attack", "ratio", "threshold"]);
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      attack: 3e-3,
      knee: 30,
      ratio: 12,
      release: 0.25,
      threshold: -24
    });
  }
  /**
   * A read-only decibel value for metering purposes, representing the current amount of gain
   * reduction that the compressor is applying to the signal. If fed no signal the value will be 0 (no gain reduction).
   */
  get reduction() {
    return this._compressor.reduction;
  }
  dispose() {
    super.dispose();
    this._compressor.disconnect();
    this.attack.dispose();
    this.release.dispose();
    this.threshold.dispose();
    this.ratio.dispose();
    this.knee.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/dynamics/Limiter.js
var Limiter = class _Limiter extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_Limiter.getDefaults(), arguments, [
      "threshold"
    ]);
    super(options);
    this.name = "Limiter";
    this._compressor = this.input = this.output = new Compressor({
      context: this.context,
      ratio: 20,
      attack: 3e-3,
      release: 0.01,
      threshold: options.threshold
    });
    this.threshold = this._compressor.threshold;
    readOnly(this, "threshold");
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      threshold: -12
    });
  }
  /**
   * A read-only decibel value for metering purposes, representing the current amount of gain
   * reduction that the compressor is applying to the signal.
   */
  get reduction() {
    return this._compressor.reduction;
  }
  dispose() {
    super.dispose();
    this._compressor.dispose();
    this.threshold.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/component/filter/EQ3.js
var EQ3 = class _EQ3 extends ToneAudioNode {
  constructor() {
    const options = optionsFromArguments(_EQ3.getDefaults(), arguments, [
      "low",
      "mid",
      "high"
    ]);
    super(options);
    this.name = "EQ3";
    this.output = new Gain({ context: this.context });
    this._internalChannels = [];
    this.input = this._multibandSplit = new MultibandSplit({
      context: this.context,
      highFrequency: options.highFrequency,
      lowFrequency: options.lowFrequency
    });
    this._lowGain = new Gain({
      context: this.context,
      gain: options.low,
      units: "decibels"
    });
    this._midGain = new Gain({
      context: this.context,
      gain: options.mid,
      units: "decibels"
    });
    this._highGain = new Gain({
      context: this.context,
      gain: options.high,
      units: "decibels"
    });
    this.low = this._lowGain.gain;
    this.mid = this._midGain.gain;
    this.high = this._highGain.gain;
    this.Q = this._multibandSplit.Q;
    this.lowFrequency = this._multibandSplit.lowFrequency;
    this.highFrequency = this._multibandSplit.highFrequency;
    this._multibandSplit.low.chain(this._lowGain, this.output);
    this._multibandSplit.mid.chain(this._midGain, this.output);
    this._multibandSplit.high.chain(this._highGain, this.output);
    readOnly(this, ["low", "mid", "high", "lowFrequency", "highFrequency"]);
    this._internalChannels = [this._multibandSplit];
  }
  static getDefaults() {
    return Object.assign(ToneAudioNode.getDefaults(), {
      high: 0,
      highFrequency: 2500,
      low: 0,
      lowFrequency: 400,
      mid: 0
    });
  }
  /**
   * Clean up.
   */
  dispose() {
    super.dispose();
    writable(this, ["low", "mid", "high", "lowFrequency", "highFrequency"]);
    this._multibandSplit.dispose();
    this.lowFrequency.dispose();
    this.highFrequency.dispose();
    this._lowGain.dispose();
    this._midGain.dispose();
    this._highGain.dispose();
    this.low.dispose();
    this.mid.dispose();
    this.high.dispose();
    this.Q.dispose();
    return this;
  }
};

// node_modules/tone/build/esm/index.js
var Transport = getContext().transport;
function getTransport() {
  return getContext().transport;
}
var Destination = getContext().destination;
var Master = getContext().destination;
var Listener = getContext().listener;
var Draw = getContext().draw;
var context = getContext();

// src/presets/synth.ts
var SYNTH_PRESETS = {
  synth: {
    name: "Basic Synth",
    category: "synth",
    description: "Versatile triangle wave synth - good all-around sound",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.02, decay: 0.1, sustain: 0.3, release: 0.8 }
    },
    semanticDefaults: { brightness: 0.4, attack: 0.1, decay: 0.3, sustain: 0.3, release: 0.4 },
    tags: ["versatile", "neutral", "default"]
  },
  sine: {
    name: "Sine Wave",
    category: "synth",
    description: "Pure, clean sine wave - dark and simple",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.5, release: 0.5 }
    },
    semanticDefaults: { brightness: 0.1, warmth: 0.8 },
    tags: ["pure", "dark", "simple", "clean"]
  },
  square: {
    name: "Square Wave",
    category: "synth",
    description: "Retro hollow square wave - 8-bit character",
    type: "polysynth",
    base: {
      oscillator: { type: "square" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.4, release: 0.3 }
    },
    semanticDefaults: { brightness: 0.6 },
    tags: ["retro", "8-bit", "hollow", "chiptune"]
  },
  sawtooth: {
    name: "Sawtooth",
    category: "synth",
    description: "Bright, buzzy sawtooth - classic synth sound",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.2, sustain: 0.3, release: 0.4 }
    },
    semanticDefaults: { brightness: 0.8 },
    tags: ["bright", "buzzy", "classic", "rich"]
  },
  arp_synth: {
    name: "Arpeggio Synth",
    category: "synth",
    description: "Bright, punchy sawtooth for arpeggios - short and clear",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 5e-3, decay: 0.2, sustain: 0.2, release: 0.15 }
    },
    semanticDefaults: { attack: 0.01, decay: 0.2, sustain: 0.2, release: 0.1, brightness: 0.7 },
    tags: ["arpeggio", "punchy", "short", "clear"]
  }
};

// src/presets/bass.ts
var BASS_PRESETS = {
  synth_bass: {
    name: "Synth Bass",
    category: "bass",
    description: "Classic filtered sawtooth bass with punch",
    type: "monosynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.2, sustain: 0.4, release: 0.3 },
      filterEnvelope: {
        attack: 0.01,
        decay: 0.2,
        sustain: 0.3,
        release: 0.2,
        baseFrequency: 200,
        octaves: 2.5
      }
    },
    semanticDefaults: { brightness: 0.5, punch: 0.7, attack: 0.05 },
    tags: ["classic", "punchy", "filtered"]
  },
  sub_bass: {
    name: "Sub Bass",
    category: "bass",
    description: "Deep, clean sine sub-bass for foundation",
    type: "monosynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 0.02, decay: 0.1, sustain: 0.8, release: 0.5 }
    },
    semanticDefaults: { brightness: 0.1, warmth: 0.9, sustain: 0.8 },
    tags: ["deep", "clean", "sub", "foundation"]
  },
  pluck_bass: {
    name: "Pluck Bass",
    category: "bass",
    description: "Short, plucky bass for staccato lines",
    type: "monosynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.01, decay: 0.3, sustain: 0.1, release: 0.2 },
      filterEnvelope: {
        attack: 0.01,
        decay: 0.15,
        sustain: 0.1,
        release: 0.1,
        baseFrequency: 300,
        octaves: 2
      }
    },
    semanticDefaults: { punch: 0.8, decay: 0.3, sustain: 0.1 },
    tags: ["plucky", "staccato", "short"]
  },
  fm_bass: {
    name: "FM Bass",
    category: "bass",
    description: "Punchy FM bass with fast modulation decay",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.3, sustain: 0.5, release: 0.2 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.1, sustain: 0.2, release: 0.1 }
    },
    semanticDefaults: { punch: 0.9, brightness: 0.6 },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 12 }
    },
    tags: ["punchy", "fm", "modern"]
  },
  synthwave_bass: {
    name: "Synthwave Bass",
    category: "bass",
    description: "Punchy 80s FM bass for driving synthwave tracks",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.3, sustain: 0.5, release: 0.2 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.1, sustain: 0.2, release: 0.1 }
    },
    semanticDefaults: { punch: 0.8, brightness: 0.6 },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 12 }
    },
    tags: ["80s", "synthwave", "punchy", "driving"]
  }
};

// src/presets/pad.ts
var PAD_PRESETS = {
  warm_pad: {
    name: "Warm Pad",
    category: "pad",
    description: "Slow-attack triangle pad - soft and warm",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.5, decay: 0.3, sustain: 0.8, release: 1.5 }
    },
    semanticDefaults: { attack: 0.5, sustain: 0.8, release: 0.6, warmth: 0.8 },
    tags: ["warm", "soft", "gentle", "mellow"]
  },
  string_pad: {
    name: "String Pad",
    category: "pad",
    description: "Orchestral string-like pad with shimmer",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.4, decay: 0.2, sustain: 0.7, release: 1.2 }
    },
    semanticDefaults: { attack: 0.4, brightness: 0.6, richness: 0.7 },
    tags: ["strings", "orchestral", "shimmer", "lush"]
  },
  ambient_pad: {
    name: "Ambient Pad",
    category: "pad",
    description: "Ethereal, evolving ambient texture",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 1, decay: 0.5, sustain: 0.9, release: 2 }
    },
    semanticDefaults: { attack: 0.8, sustain: 0.9, release: 0.8, warmth: 0.7 },
    tags: ["ambient", "ethereal", "evolving", "atmospheric"]
  },
  synthwave_pad: {
    name: "Synthwave Pad",
    category: "pad",
    description: "Lush Juno-style sawtooth pad with slow attack",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 1.5, decay: 0.5, sustain: 0.8, release: 2 }
    },
    semanticDefaults: { attack: 0.7, sustain: 0.8, release: 0.7, warmth: 0.7 },
    tags: ["80s", "synthwave", "lush", "juno"]
  }
};

// src/presets/lead.ts
var LEAD_PRESETS = {
  lead: {
    name: "Lead Synth",
    category: "lead",
    description: "Bright sawtooth lead with filter sweep",
    type: "monosynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.6, release: 0.3 },
      filterEnvelope: {
        attack: 0.01,
        decay: 0.1,
        sustain: 0.5,
        release: 0.3,
        baseFrequency: 800,
        octaves: 2
      }
    },
    semanticDefaults: { brightness: 0.7, punch: 0.6 },
    tags: ["bright", "classic", "sawtooth"]
  },
  soft_lead: {
    name: "Soft Lead",
    category: "lead",
    description: "Mellow triangle lead for gentle melodies",
    type: "monosynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.05, decay: 0.2, sustain: 0.5, release: 0.5 }
    },
    semanticDefaults: { brightness: 0.3, warmth: 0.7, attack: 0.15 },
    tags: ["soft", "mellow", "gentle", "warm"]
  },
  synthwave_lead: {
    name: "Synthwave Lead",
    category: "lead",
    description: "Bright FM lead for soaring 80s melodies",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 8,
      oscillator: { type: "sine" },
      envelope: { attack: 0.02, decay: 0.3, sustain: 0.6, release: 0.4 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.02, decay: 0.2, sustain: 0.3, release: 0.3 }
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 4, max: 16 }
    },
    tags: ["80s", "synthwave", "soaring", "fm"]
  }
};

// src/presets/keys.ts
var KEYS_PRESETS = {
  /**
   * Acoustic Piano (v0.9.2)
   *
   * Concert grand piano using FM synthesis. Targets warm romantic character
   * with spectral centroid ~700 Hz (similar to reference recordings).
   *
   * Key design choices:
   * - Fast attack (2ms) for hammer strike
   * - Long decay (4.5s) for sustained resonance
   * - Low harmonicity (1.0) for fundamental focus
   * - Moderate modulation index (3.5) for brightness without harshness
   * - Modulation envelope decays faster than amplitude (natural piano characteristic)
   * - Extended release (3s) for sustain pedal simulation
   */
  acoustic_piano: {
    name: "Acoustic Piano",
    category: "keys",
    description: "Concert grand piano with warm, resonant tone. Long decay and natural timbre.",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 3.5,
      oscillator: { type: "sine" },
      envelope: { attack: 2e-3, decay: 4.5, sustain: 0.05, release: 3 },
      modulation: { type: "sine" },
      // Modulation decays faster than amplitude for natural piano brightness
      modulationEnvelope: { attack: 2e-3, decay: 1.2, sustain: 0.02, release: 0.5 }
    },
    semanticDefaults: { warmth: 0.7, brightness: 0.4, decay: 0.8, release: 0.7 },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1, max: 8 },
      warmth: { param: "harmonicity", min: 0.5, max: 1.5 },
      decay: { param: "envelope.decay", min: 1, max: 8 },
      release: { param: "envelope.release", min: 0.5, max: 5 }
    },
    tags: ["piano", "acoustic", "concert", "grand", "classical", "romantic", "warm"]
  },
  electric_piano: {
    name: "Electric Piano",
    category: "keys",
    description: "Classic Rhodes-like tone with bell attack",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 0.8, sustain: 0.2, release: 0.8 }
    },
    semanticDefaults: { punch: 0.5, decay: 0.6 },
    tags: ["rhodes", "classic", "bell", "vintage"]
  },
  organ: {
    name: "Organ",
    category: "keys",
    description: "Sustained organ tone with no decay",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 0.01, sustain: 1, release: 0.1 }
    },
    semanticDefaults: { sustain: 1, release: 0.05 },
    tags: ["organ", "sustained", "church"]
  }
};

// src/presets/pluck.ts
var PLUCK_PRESETS = {
  pluck: {
    name: "Pluck",
    category: "pluck",
    description: "Basic plucked string sound",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 1e-3, decay: 0.4, sustain: 0, release: 0.2 }
    },
    semanticDefaults: { attack: 0.01, decay: 0.4, sustain: 0 },
    tags: ["pluck", "short", "percussive"]
  },
  bell: {
    name: "Bell",
    category: "pluck",
    description: "Bright bell tone with long decay",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1, sustain: 0, release: 1 }
    },
    semanticDefaults: { attack: 0.01, decay: 0.7, sustain: 0, release: 0.6 },
    tags: ["bell", "bright", "resonant"]
  },
  marimba: {
    name: "Marimba",
    category: "pluck",
    description: "Mallet percussion tone",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.5, sustain: 0, release: 0.3 }
    },
    semanticDefaults: { decay: 0.4, warmth: 0.6 },
    tags: ["marimba", "mallet", "wooden", "warm"]
  },
  // Guitar presets (v0.9.2)
  clean_guitar: {
    name: "Clean Guitar",
    category: "pluck",
    description: "Clean electric guitar tone - versatile and warm",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 2e-3, decay: 0.6, sustain: 0.15, release: 0.4 }
    },
    semanticDefaults: { brightness: 0.5, warmth: 0.6, decay: 0.5 },
    tags: ["guitar", "clean", "electric", "warm"]
  },
  rhythm_guitar: {
    name: "Rhythm Guitar",
    category: "pluck",
    description: "Staccato rhythm guitar for funk/Afrobeat - choppy and tight",
    type: "polysynth",
    base: {
      // Triangle with slight harmonic content for guitar-like timbre
      oscillator: { type: "triangle" },
      // Very short decay for choppy rhythm playing
      envelope: { attack: 1e-3, decay: 0.15, sustain: 0, release: 0.08 }
    },
    semanticDefaults: { brightness: 0.55, attack: 0.01, decay: 0.15 },
    tags: ["guitar", "rhythm", "funk", "afrobeat", "staccato", "choppy"]
  },
  muted_guitar: {
    name: "Muted Guitar",
    category: "pluck",
    description: "Palm-muted guitar - tight and percussive",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      // Very tight, almost percussive
      envelope: { attack: 1e-3, decay: 0.08, sustain: 0, release: 0.05 }
    },
    semanticDefaults: { brightness: 0.4, warmth: 0.7, decay: 0.1 },
    tags: ["guitar", "muted", "palm-mute", "tight", "percussive"]
  }
};

// src/presets/fm.ts
var FM_PRESETS = {
  fm_epiano: {
    name: "FM Electric Piano",
    category: "fm",
    description: "DX7-style Rhodes with warm bell-like attack",
    type: "fmsynth",
    base: {
      harmonicity: 1.0007,
      modulationIndex: 4,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 2.5, sustain: 0.1, release: 1.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.4, sustain: 0.1, release: 0.3 }
    },
    semanticDefaults: { brightness: 0.5, warmth: 0.6 },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1, max: 12 },
      warmth: { param: "harmonicity", min: 0.5, max: 2 }
    },
    tags: ["dx7", "rhodes", "warm", "bell", "classic"]
  },
  fm_brass: {
    name: "FM Brass",
    category: "fm",
    description: "Brass stab where brightness tracks loudness",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 12,
      oscillator: { type: "sine" },
      envelope: { attack: 0.05, decay: 0.2, sustain: 0.7, release: 0.3 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.08, decay: 0.3, sustain: 0.6, release: 0.2 }
    },
    semanticDefaults: { punch: 0.7, brightness: 0.7 },
    tags: ["brass", "stab", "punchy", "bright"]
  },
  fm_church_bell: {
    name: "FM Church Bell",
    category: "fm",
    description: "Large church bell with inharmonic partials",
    type: "fmsynth",
    base: {
      harmonicity: 14,
      modulationIndex: 8,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 4, sustain: 0, release: 3 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 2, sustain: 0, release: 1.5 }
    },
    semanticDefaults: { decay: 0.9, release: 0.8 },
    tags: ["bell", "church", "inharmonic", "long"]
  },
  fm_tubular_bell: {
    name: "FM Tubular Bell",
    category: "fm",
    description: "Classic DX7 chime bell",
    type: "fmsynth",
    base: {
      harmonicity: 5.07,
      modulationIndex: 10,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0, release: 1.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 1, sustain: 0, release: 1 }
    },
    tags: ["bell", "tubular", "dx7", "chime"]
  },
  fm_glass: {
    name: "FM Glass",
    category: "fm",
    description: "Crystal/glass sound with delicate timbre",
    type: "fmsynth",
    base: {
      harmonicity: 7,
      modulationIndex: 3,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0, release: 1 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.3, sustain: 0, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.8, decay: 0.5 },
    tags: ["glass", "crystal", "delicate", "bright"]
  },
  fm_vibraphone: {
    name: "FM Vibraphone",
    category: "fm",
    description: "Mallet percussion with FM warmth",
    type: "fmsynth",
    base: {
      harmonicity: 4,
      modulationIndex: 3.5,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.8, sustain: 0.1, release: 1.2 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.5, sustain: 0, release: 0.3 }
    },
    semanticDefaults: { warmth: 0.7, decay: 0.6 },
    tags: ["vibraphone", "mallet", "warm", "jazz"]
  },
  fm_organ: {
    name: "FM Organ",
    category: "fm",
    description: "FM organ with sustained brightness",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 2,
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 0.01, sustain: 1, release: 0.1 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.01, decay: 0.01, sustain: 1, release: 0.1 }
    },
    semanticDefaults: { sustain: 1, brightness: 0.4 },
    tags: ["organ", "sustained", "fm"]
  },
  fm_bell: {
    name: "FM Bell",
    category: "fm",
    description: "Classic DX7 chime bell",
    type: "fmsynth",
    base: {
      harmonicity: 5.07,
      modulationIndex: 10,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0, release: 1.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 1, sustain: 0, release: 1 }
    },
    tags: ["bell", "dx7", "chime", "bright"]
  },
  synthwave_stab: {
    name: "Synthwave Stab",
    category: "fm",
    description: "Short punchy FM stab for accents and hits",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 15,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.2, sustain: 0.1, release: 0.15 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.15, sustain: 0, release: 0.1 }
    },
    tags: ["80s", "synthwave", "stab", "punchy", "accent"]
  }
};

// src/presets/texture.ts
var TEXTURE_PRESETS = {
  noise: {
    name: "White Noise",
    category: "texture",
    description: "White noise for texture, risers, and ambient effects",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.5, release: 0.3 }
    },
    semanticDefaults: { brightness: 1 },
    tags: ["noise", "white", "bright", "texture"]
  },
  pink_noise: {
    name: "Pink Noise",
    category: "texture",
    description: "Pink noise (1/f) - warmer, more natural sounding",
    type: "noise",
    base: {
      noise: { type: "pink" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.5, release: 0.3 }
    },
    semanticDefaults: { brightness: 0.7, warmth: 0.6 },
    tags: ["noise", "pink", "warm", "natural"]
  },
  brown_noise: {
    name: "Brown Noise",
    category: "texture",
    description: "Brown noise (1/f^2) - deepest, smoothest noise",
    type: "noise",
    base: {
      noise: { type: "brown" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.5, release: 0.3 }
    },
    semanticDefaults: { brightness: 0.3, warmth: 0.9 },
    tags: ["noise", "brown", "deep", "smooth"]
  },
  noise_sweep: {
    name: "Noise Sweep",
    category: "texture",
    description: "White noise with longer attack for sweeps and risers",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 0.5, decay: 0.3, sustain: 0.3, release: 0.5 }
    },
    semanticDefaults: { brightness: 1 },
    tags: ["noise", "sweep", "riser", "build"]
  }
};

// src/presets/drums.ts
var DRUM_PRESETS = {
  kick_deep: {
    name: "Deep Kick",
    category: "drums",
    description: "Deep booming kick drum with slow pitch decay",
    type: "membrane",
    base: {
      pitchDecay: 0.08,
      octaves: 4,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.5, sustain: 0, release: 0.1 },
      pitch: "C2"
    },
    tags: ["kick", "deep", "booming"]
  },
  kick_909: {
    name: "909 Kick",
    category: "drums",
    description: "Classic TR-909 style kick drum",
    type: "membrane",
    base: {
      pitchDecay: 0.05,
      octaves: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.4, sustain: 0, release: 0.1 },
      pitch: "C2"
    },
    tags: ["kick", "909", "classic", "electronic"]
  },
  hihat_closed: {
    name: "Closed Hi-Hat",
    category: "drums",
    description: "Short, tight closed hi-hat",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.06, sustain: 0, release: 0.01 }
    },
    tags: ["hihat", "closed", "tight", "short"]
  },
  hihat_open: {
    name: "Open Hi-Hat",
    category: "drums",
    description: "Longer, ringing open hi-hat",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.2, sustain: 0.05, release: 0.1 }
    },
    tags: ["hihat", "open", "ringing"]
  },
  clap_909: {
    name: "909 Clap",
    category: "drums",
    description: "Classic TR-909 style clap",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.15, sustain: 0, release: 0.05 }
    },
    tags: ["clap", "909", "classic"]
  },
  snare_house: {
    name: "House Snare",
    category: "drums",
    description: "Pink noise snare for house and electronic music",
    type: "noise",
    base: {
      noise: { type: "pink" },
      envelope: { attack: 1e-3, decay: 0.2, sustain: 0, release: 0.05 }
    },
    tags: ["snare", "house", "electronic", "pink"]
  }
};

// src/presets/lofi.ts
var LOFI_PRESETS = {
  lofi_keys: {
    name: "Lo-fi Keys",
    category: "lofi",
    description: "Dusty, warm keys with tape-like character",
    type: "fmsynth",
    base: {
      harmonicity: 1.005,
      // Slight detune for warmth
      modulationIndex: 2.5,
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 1.2, sustain: 0.1, release: 0.8 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.01, decay: 0.3, sustain: 0.05, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.3, warmth: 0.9, punch: 0.4 },
    tags: ["lofi", "dusty", "warm", "tape", "vintage"]
  },
  lofi_pad: {
    name: "Lo-fi Pad",
    category: "lofi",
    description: "Tape-saturated pad with gentle warmth",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.8, decay: 0.5, sustain: 0.6, release: 1.5 }
    },
    semanticDefaults: { brightness: 0.25, warmth: 0.95, attack: 0.6 },
    tags: ["lofi", "pad", "tape", "warm", "saturated"]
  },
  vinyl_texture: {
    name: "Vinyl Texture",
    category: "lofi",
    description: "Subtle crackle and warmth for atmosphere",
    type: "noise",
    base: {
      noise: { type: "brown" },
      envelope: { attack: 0.5, decay: 0.5, sustain: 0.3, release: 1 }
    },
    semanticDefaults: { brightness: 0.1, warmth: 0.8 },
    tags: ["vinyl", "crackle", "texture", "atmosphere"]
  },
  dusty_piano: {
    name: "Dusty Piano",
    category: "lofi",
    description: "Worn piano with vintage character",
    type: "fmsynth",
    base: {
      harmonicity: 1.01,
      // Slightly detuned
      modulationIndex: 3,
      oscillator: { type: "sine" },
      envelope: { attack: 5e-3, decay: 1.8, sustain: 0.05, release: 1 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 5e-3, decay: 0.25, sustain: 0.02, release: 0.15 }
    },
    semanticDefaults: { brightness: 0.35, warmth: 0.85, decay: 0.7 },
    tags: ["piano", "dusty", "vintage", "worn"]
  },
  vinyl_crackle: {
    name: "Vinyl Crackle",
    category: "lofi",
    description: "Lo-fi vinyl crackle texture with short decay",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.02, sustain: 0, release: 0.01 }
    },
    semanticDefaults: { brightness: 0.9 },
    tags: ["vinyl", "crackle", "lofi", "texture"]
  }
};

// src/presets/cinematic.ts
var CINEMATIC_PRESETS = {
  cinematic_brass: {
    name: "Cinematic Brass",
    category: "cinematic",
    description: "Massive orchestral brass for epic moments",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 18,
      oscillator: { type: "sine" },
      envelope: { attack: 0.15, decay: 0.3, sustain: 0.8, release: 0.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.2, decay: 0.4, sustain: 0.7, release: 0.4 }
    },
    semanticDefaults: { punch: 0.8, brightness: 0.75, attack: 0.3 },
    tags: ["brass", "epic", "orchestral", "massive"]
  },
  tension_strings: {
    name: "Tension Strings",
    category: "cinematic",
    description: "Dark, suspenseful string texture",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.6, decay: 0.4, sustain: 0.7, release: 1.8 }
    },
    semanticDefaults: { brightness: 0.3, warmth: 0.4, attack: 0.5 },
    tags: ["strings", "tension", "dark", "suspense"]
  },
  impact_hit: {
    name: "Impact Hit",
    category: "cinematic",
    description: "Deep cinematic impact for transitions",
    type: "membrane",
    base: {
      pitchDecay: 0.15,
      octaves: 8,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0, release: 2 },
      pitch: "C1"
    },
    semanticDefaults: { punch: 1, decay: 0.8 },
    tags: ["impact", "hit", "deep", "transition"]
  },
  epic_pad: {
    name: "Epic Pad",
    category: "cinematic",
    description: "Huge evolving pad for emotional moments",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 1.5, decay: 0.8, sustain: 0.9, release: 3 }
    },
    semanticDefaults: { brightness: 0.6, warmth: 0.7, attack: 0.8, release: 0.9 },
    tags: ["pad", "epic", "evolving", "emotional"]
  },
  riser: {
    name: "Riser",
    category: "cinematic",
    description: "Building tension riser sound",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 4, decay: 0.5, sustain: 0.8, release: 0.5 }
    },
    semanticDefaults: { brightness: 0.7, attack: 0.95 },
    tags: ["riser", "build", "tension", "transition"]
  }
};

// src/presets/world.ts
var WORLD_PRESETS = {
  kalimba: {
    name: "Kalimba",
    category: "world",
    description: "African thumb piano with metallic tines",
    type: "fmsynth",
    base: {
      harmonicity: 5.5,
      modulationIndex: 4,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.2, sustain: 0.05, release: 0.8 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.4, sustain: 0, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.6, warmth: 0.5, decay: 0.5 },
    tags: ["kalimba", "african", "metallic", "thumb piano"]
  },
  sitar_lead: {
    name: "Sitar Lead",
    category: "world",
    description: "Sitar-inspired lead with characteristic buzz",
    type: "fmsynth",
    base: {
      harmonicity: 3,
      modulationIndex: 7,
      oscillator: { type: "sine" },
      envelope: { attack: 0.02, decay: 0.5, sustain: 0.4, release: 0.6 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.01, decay: 0.3, sustain: 0.5, release: 0.4 }
    },
    semanticDefaults: { brightness: 0.7, warmth: 0.6 },
    tags: ["sitar", "indian", "buzz", "lead"]
  },
  steel_drum: {
    name: "Steel Drum",
    category: "world",
    description: "Caribbean steel pan with bright overtones",
    type: "fmsynth",
    base: {
      harmonicity: 4.5,
      modulationIndex: 5,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1, sustain: 0.1, release: 0.6 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.3, sustain: 0.05, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.75, warmth: 0.5, decay: 0.5 },
    tags: ["steel drum", "caribbean", "pan", "bright"]
  },
  koto: {
    name: "Koto",
    category: "world",
    description: "Japanese stringed instrument with delicate attack",
    type: "fmsynth",
    base: {
      harmonicity: 6,
      modulationIndex: 2.5,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0.02, release: 0.8 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.2, sustain: 0, release: 0.1 }
    },
    semanticDefaults: { brightness: 0.55, warmth: 0.4, decay: 0.6 },
    tags: ["koto", "japanese", "delicate", "pluck"]
  },
  // African Percussion (v0.9.2)
  conga_high: {
    name: "Conga High",
    category: "world",
    description: "High-pitched conga (quinto) - slap and open tones",
    type: "membrane",
    base: {
      pitchDecay: 0.015,
      octaves: 3,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.35, sustain: 0, release: 0.15 },
      pitch: "D3"
    },
    semanticDefaults: { brightness: 0.6, punch: 0.7 },
    tags: ["conga", "african", "percussion", "hand drum"]
  },
  conga_low: {
    name: "Conga Low",
    category: "world",
    description: "Low-pitched conga (tumba) - deep resonant tone",
    type: "membrane",
    base: {
      pitchDecay: 0.02,
      octaves: 4,
      oscillator: { type: "sine" },
      envelope: { attack: 2e-3, decay: 0.5, sustain: 0, release: 0.2 },
      pitch: "G2"
    },
    semanticDefaults: { brightness: 0.4, warmth: 0.7, punch: 0.6 },
    tags: ["conga", "african", "percussion", "hand drum", "deep"]
  },
  djembe: {
    name: "Djembe",
    category: "world",
    description: "West African goblet drum - bass, tone, and slap",
    type: "membrane",
    base: {
      pitchDecay: 0.012,
      octaves: 5,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.4, sustain: 0, release: 0.18 },
      pitch: "C3"
    },
    semanticDefaults: { brightness: 0.55, warmth: 0.6, punch: 0.8 },
    tags: ["djembe", "african", "percussion", "goblet drum"]
  },
  talking_drum: {
    name: "Talking Drum",
    category: "world",
    description: "Nigerian hourglass tension drum - pitch bends with pressure",
    type: "membrane",
    base: {
      pitchDecay: 0.08,
      octaves: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.6, sustain: 0.1, release: 0.3 },
      pitch: "A2"
    },
    semanticDefaults: { brightness: 0.5, warmth: 0.7 },
    tags: ["talking drum", "african", "nigerian", "yoruba", "pitch bend"]
  },
  agogo: {
    name: "Agogo",
    category: "world",
    description: "Double bell - essential for Afrobeat clave patterns",
    type: "metal",
    base: {
      // Bell frequencies: higher base with complex inharmonic ratios
      frequency: 1800,
      harmonicity: 3.17,
      // Non-integer for inharmonic "clangy" partials
      modulationIndex: 20,
      // High modulation for metallic complexity
      resonance: 3500,
      // Medium decay - agogo has a quick "ping" character
      envelope: { attack: 1e-3, decay: 0.35, sustain: 0, release: 0.15 }
    },
    semanticDefaults: { brightness: 0.75 },
    tags: ["agogo", "bell", "african", "clave", "afrobeat"]
  },
  shekere: {
    name: "Shekere",
    category: "world",
    description: "Gourd shaker with beads - rhythmic texture",
    type: "noise",
    base: {
      // Pink noise is more natural sounding than white for shakers
      noise: { type: "pink" },
      // Shaker: quick attack, short decay with slight sustain
      envelope: { attack: 2e-3, decay: 0.06, sustain: 0.02, release: 0.05 }
    },
    semanticDefaults: { brightness: 0.5 },
    tags: ["shekere", "shaker", "african", "gourd", "beads"]
  }
};

// src/presets/ambient.ts
var AMBIENT_PRESETS = {
  granular_pad: {
    name: "Granular Pad",
    category: "ambient",
    description: "Textured evolving pad with cloud-like quality",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 2, decay: 1, sustain: 0.95, release: 4 }
    },
    semanticDefaults: { brightness: 0.35, warmth: 0.8, attack: 0.9, release: 0.95 },
    tags: ["granular", "cloud", "evolving", "texture"]
  },
  drone: {
    name: "Drone",
    category: "ambient",
    description: "Deep sustained drone for atmospheric beds",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 3, decay: 1, sustain: 1, release: 5 }
    },
    semanticDefaults: { brightness: 0.15, warmth: 0.9, attack: 0.95, sustain: 1, release: 1 },
    tags: ["drone", "deep", "sustained", "bed"]
  },
  shimmer: {
    name: "Shimmer",
    category: "ambient",
    description: "Ethereal shimmering texture with high harmonics",
    type: "fmsynth",
    base: {
      harmonicity: 7.5,
      modulationIndex: 2,
      oscillator: { type: "sine" },
      envelope: { attack: 1.5, decay: 0.8, sustain: 0.7, release: 3 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1, decay: 0.5, sustain: 0.4, release: 2 }
    },
    semanticDefaults: { brightness: 0.8, warmth: 0.5, attack: 0.7 },
    tags: ["shimmer", "ethereal", "high", "bright"]
  },
  atmosphere: {
    name: "Atmosphere",
    category: "ambient",
    description: "Breathy, wind-like atmospheric texture",
    type: "noise",
    base: {
      noise: { type: "pink" },
      envelope: { attack: 2, decay: 1, sustain: 0.6, release: 3 }
    },
    semanticDefaults: { brightness: 0.4, warmth: 0.6, attack: 0.8 },
    tags: ["atmosphere", "wind", "breathy", "texture"]
  },
  space_pad: {
    name: "Space Pad",
    category: "ambient",
    description: "Vast, cosmic pad with deep reverb character",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 1.5,
      oscillator: { type: "sine" },
      envelope: { attack: 2.5, decay: 1.5, sustain: 0.85, release: 4.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 2, decay: 1, sustain: 0.6, release: 3 }
    },
    semanticDefaults: { brightness: 0.4, warmth: 0.7, attack: 0.85, release: 0.9 },
    tags: ["space", "cosmic", "vast", "pad"]
  }
};

// src/presets/modern.ts
var MODERN_PRESETS = {
  "808_bass": {
    name: "808 Bass",
    category: "modern",
    description: "Classic 808 bass with long sustain",
    type: "membrane",
    base: {
      pitchDecay: 0.08,
      octaves: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.8, sustain: 0.3, release: 0.4 },
      pitch: "C2"
    },
    semanticDefaults: { punch: 0.9, decay: 0.6, sustain: 0.3 },
    tags: ["808", "bass", "trap", "hip-hop"]
  },
  trap_hihat: {
    name: "Trap Hi-Hat",
    category: "modern",
    description: "Crisp trap hi-hat with tight envelope",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.04, sustain: 0, release: 0.02 }
    },
    semanticDefaults: { brightness: 0.9, decay: 0.1 },
    tags: ["trap", "hihat", "crisp", "tight"]
  },
  future_bass_lead: {
    name: "Future Bass Lead",
    category: "modern",
    description: "Supersawed lead for future bass drops",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.15, sustain: 0.5, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.85, richness: 0.9, punch: 0.7 },
    tags: ["future bass", "supersaw", "lead", "drop"]
  },
  wobble_bass: {
    name: "Wobble Bass",
    category: "modern",
    description: "Dubstep/EDM wobble bass foundation",
    type: "monosynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.8, release: 0.2 },
      filterEnvelope: {
        attack: 0.01,
        decay: 0.2,
        sustain: 0.5,
        release: 0.2,
        baseFrequency: 100,
        octaves: 4
      }
    },
    semanticDefaults: { brightness: 0.7, punch: 0.8, warmth: 0.5 },
    tags: ["wobble", "dubstep", "bass", "edm"]
  },
  pluck_lead: {
    name: "Pluck Lead",
    category: "modern",
    description: "Bright pluck for modern pop melodies",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 1e-3, decay: 0.3, sustain: 0.1, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.7, punch: 0.6, decay: 0.3 },
    tags: ["pluck", "pop", "bright", "modern"]
  },
  supersaw: {
    name: "Supersaw",
    category: "modern",
    description: "Classic supersaw for trance and EDM leads",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.02, decay: 0.2, sustain: 0.6, release: 0.4 }
    },
    semanticDefaults: { brightness: 0.9, richness: 1, warmth: 0.4 },
    tags: ["supersaw", "trance", "edm", "lead"]
  },
  chiptune: {
    name: "Chiptune",
    category: "modern",
    description: "Retro 8-bit square wave for chiptune style",
    type: "polysynth",
    base: {
      oscillator: { type: "square" },
      envelope: { attack: 1e-3, decay: 0.1, sustain: 0.4, release: 0.1 }
    },
    semanticDefaults: { brightness: 0.8, punch: 0.5 },
    tags: ["chiptune", "8-bit", "retro", "square"]
  }
};

// src/presets/strings.ts
var STRINGS_PRESETS = {
  /**
   * Solo Violin
   *
   * Expressive, singing tone in the high register.
   * Sawtooth-based with vibrato for warmth.
   */
  solo_violin: {
    name: "Solo Violin",
    category: "strings",
    description: "Expressive solo violin with vibrato. High register, singing quality.",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: {
        attack: 0.15,
        // Bow contact time
        decay: 0.3,
        sustain: 0.8,
        release: 0.8
      }
    },
    semanticDefaults: {
      brightness: 0.6,
      warmth: 0.5,
      attack: 0.3,
      sustain: 0.8,
      release: 0.6
    },
    semanticMappings: {
      brightness: { param: "filterEnvelope.baseFrequency", min: 1500, max: 6e3 },
      warmth: { param: "filterEnvelope.octaves", min: 1, max: 4 },
      attack: { param: "envelope.attack", min: 0.05, max: 0.4 },
      release: { param: "envelope.release", min: 0.3, max: 2 }
    },
    tags: ["violin", "solo", "strings", "orchestral", "expressive", "classical", "high"]
  },
  /**
   * Solo Viola
   *
   * Warm, rich tone in the mid register.
   * Darker than violin, fuller sound.
   */
  solo_viola: {
    name: "Solo Viola",
    category: "strings",
    description: "Warm, rich viola tone. Mid register with full-bodied sound.",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: {
        attack: 0.18,
        decay: 0.35,
        sustain: 0.75,
        release: 0.9
      }
    },
    semanticDefaults: {
      brightness: 0.45,
      warmth: 0.65,
      attack: 0.35,
      sustain: 0.75,
      release: 0.65
    },
    semanticMappings: {
      brightness: { param: "filterEnvelope.baseFrequency", min: 1200, max: 4500 },
      warmth: { param: "filterEnvelope.octaves", min: 1, max: 3.5 },
      attack: { param: "envelope.attack", min: 0.08, max: 0.45 },
      release: { param: "envelope.release", min: 0.4, max: 2.2 }
    },
    tags: ["viola", "solo", "strings", "orchestral", "warm", "classical", "mid"]
  },
  /**
   * Solo Cello
   *
   * Deep, lyrical tone in the low-mid register.
   * Excellent for bass melodies and emotional passages.
   */
  solo_cello: {
    name: "Solo Cello",
    category: "strings",
    description: "Deep, lyrical cello with expressive vibrato. Low-mid register.",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: {
        attack: 0.2,
        decay: 0.4,
        sustain: 0.7,
        release: 1
      }
    },
    semanticDefaults: {
      brightness: 0.4,
      warmth: 0.7,
      attack: 0.4,
      sustain: 0.7,
      release: 0.7
    },
    semanticMappings: {
      brightness: { param: "filterEnvelope.baseFrequency", min: 800, max: 3500 },
      warmth: { param: "filterEnvelope.octaves", min: 1, max: 3 },
      attack: { param: "envelope.attack", min: 0.1, max: 0.5 },
      release: { param: "envelope.release", min: 0.5, max: 2.5 }
    },
    tags: ["cello", "solo", "strings", "orchestral", "lyrical", "classical", "low", "emotional"]
  },
  /**
   * Contrabass (Double Bass)
   *
   * Very low register, foundation of the string section.
   * Provides warmth and depth.
   */
  contrabass: {
    name: "Contrabass",
    category: "strings",
    description: "Deep contrabass providing orchestral foundation. Very low register.",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: {
        attack: 0.25,
        decay: 0.5,
        sustain: 0.65,
        release: 1.2
      }
    },
    semanticDefaults: {
      brightness: 0.3,
      warmth: 0.8,
      attack: 0.5,
      sustain: 0.65,
      release: 0.75
    },
    semanticMappings: {
      brightness: { param: "filterEnvelope.baseFrequency", min: 500, max: 2500 },
      warmth: { param: "filterEnvelope.octaves", min: 1, max: 2.5 },
      attack: { param: "envelope.attack", min: 0.15, max: 0.6 },
      release: { param: "envelope.release", min: 0.6, max: 3 }
    },
    tags: ["contrabass", "double bass", "bass", "strings", "orchestral", "foundation", "low"]
  },
  /**
   * String Ensemble
   *
   * Full section blend with chorus effect.
   * Rich, blended sound for pads and accompaniment.
   */
  string_ensemble: {
    name: "String Ensemble",
    category: "strings",
    description: "Full string section blend. Rich, lush orchestral pad sound.",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: {
        attack: 0.35,
        // Slower attack for blend
        decay: 0.4,
        sustain: 0.85,
        release: 1.5
      }
    },
    semanticDefaults: {
      brightness: 0.5,
      warmth: 0.6,
      richness: 0.8,
      attack: 0.6,
      sustain: 0.85,
      release: 0.8
    },
    semanticMappings: {
      brightness: { param: "filterEnvelope.baseFrequency", min: 1e3, max: 4e3 },
      warmth: { param: "filterEnvelope.octaves", min: 1, max: 3 },
      attack: { param: "envelope.attack", min: 0.2, max: 0.8 },
      release: { param: "envelope.release", min: 0.8, max: 3 }
    },
    tags: ["ensemble", "section", "strings", "orchestral", "lush", "pad", "cinematic"]
  },
  /**
   * String Pizzicato
   *
   * Plucked strings with short decay.
   * Triangle wave for softer character.
   */
  string_pizzicato: {
    name: "String Pizzicato",
    category: "strings",
    description: "Plucked strings with short, defined attack. Pizzicato articulation.",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: {
        attack: 5e-3,
        // Very fast - pluck
        decay: 0.4,
        sustain: 0.1,
        release: 0.3
      }
    },
    semanticDefaults: {
      brightness: 0.5,
      warmth: 0.6,
      punch: 0.7,
      decay: 0.4,
      release: 0.3
    },
    semanticMappings: {
      brightness: { param: "filterEnvelope.baseFrequency", min: 1500, max: 5e3 },
      punch: { param: "envelope.attack", min: 1e-3, max: 0.02 },
      decay: { param: "envelope.decay", min: 0.2, max: 0.8 },
      release: { param: "envelope.release", min: 0.1, max: 0.6 }
    },
    tags: ["pizzicato", "pluck", "strings", "orchestral", "short", "rhythmic"]
  },
  /**
   * String Tremolo
   *
   * Trembling bow effect with amplitude LFO.
   * Creates tension and drama.
   */
  string_tremolo: {
    name: "String Tremolo",
    category: "strings",
    description: "Trembling bow technique. Creates tension and dramatic intensity.",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: {
        attack: 0.1,
        decay: 0.2,
        sustain: 0.9,
        release: 0.6
      }
    },
    semanticDefaults: {
      brightness: 0.55,
      warmth: 0.5,
      movement: 0.8,
      // Tremolo amount
      attack: 0.2,
      sustain: 0.9
    },
    semanticMappings: {
      brightness: { param: "filterEnvelope.baseFrequency", min: 1200, max: 4500 },
      warmth: { param: "filterEnvelope.octaves", min: 1, max: 3 },
      attack: { param: "envelope.attack", min: 0.05, max: 0.3 }
    },
    tags: ["tremolo", "strings", "orchestral", "tension", "dramatic", "cinematic"]
  },
  /**
   * String Spiccato
   *
   * Bouncing bow articulation.
   * Short, separated notes with defined attack.
   */
  string_spiccato: {
    name: "String Spiccato",
    category: "strings",
    description: "Bouncing bow articulation. Short, separated notes for fast passages.",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: {
        attack: 0.02,
        // Quick bow bounce
        decay: 0.15,
        sustain: 0.2,
        release: 0.2
      }
    },
    semanticDefaults: {
      brightness: 0.6,
      warmth: 0.45,
      punch: 0.6,
      decay: 0.3,
      release: 0.25
    },
    semanticMappings: {
      brightness: { param: "filterEnvelope.baseFrequency", min: 1500, max: 5500 },
      punch: { param: "envelope.attack", min: 0.01, max: 0.05 },
      decay: { param: "envelope.decay", min: 0.08, max: 0.3 },
      release: { param: "envelope.release", min: 0.1, max: 0.4 }
    },
    tags: ["spiccato", "strings", "orchestral", "bouncing", "fast", "articulated"]
  }
};

// src/presets/brass.ts
var BRASS_PRESETS = {
  /**
   * Trumpet
   *
   * Bright, heroic brass with cutting tone.
   * High modulation index for rich harmonics.
   */
  trumpet: {
    name: "Trumpet",
    category: "brass",
    description: "Bright, heroic trumpet tone. Cutting and brilliant for fanfares.",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 8,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.05,
        decay: 0.2,
        sustain: 0.85,
        release: 0.3
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.05,
        decay: 0.3,
        sustain: 0.6,
        release: 0.3
      }
    },
    semanticDefaults: {
      brightness: 0.75,
      warmth: 0.4,
      punch: 0.7,
      attack: 0.15,
      sustain: 0.85
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 4, max: 14 },
      warmth: { param: "harmonicity", min: 0.5, max: 2 },
      punch: { param: "envelope.attack", min: 0.01, max: 0.1 }
    },
    tags: ["trumpet", "brass", "orchestral", "bright", "heroic", "fanfare"]
  },
  /**
   * French Horn
   *
   * Warm, noble brass with rounded tone.
   * Lower modulation index for mellower sound.
   */
  french_horn: {
    name: "French Horn",
    category: "brass",
    description: "Warm, noble French horn. Rich and mellow for sustained harmonies.",
    type: "fmsynth",
    base: {
      harmonicity: 0.5,
      // Sub-harmonic relationship for warmth
      modulationIndex: 5,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.08,
        decay: 0.3,
        sustain: 0.8,
        release: 0.5
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.1,
        decay: 0.4,
        sustain: 0.5,
        release: 0.4
      }
    },
    semanticDefaults: {
      brightness: 0.45,
      warmth: 0.7,
      attack: 0.25,
      sustain: 0.8,
      release: 0.5
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 8 },
      warmth: { param: "harmonicity", min: 0.25, max: 1 },
      attack: { param: "envelope.attack", min: 0.05, max: 0.2 },
      release: { param: "envelope.release", min: 0.3, max: 1 }
    },
    tags: ["french horn", "horn", "brass", "orchestral", "warm", "noble", "mellow"]
  },
  /**
   * Trombone
   *
   * Rich, powerful brass in the mid-low register.
   * Good for bass lines and harmonies.
   */
  trombone: {
    name: "Trombone",
    category: "brass",
    description: "Rich, powerful trombone. Mid-low register for bass and harmonies.",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 7,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.06,
        decay: 0.25,
        sustain: 0.8,
        release: 0.4
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.08,
        decay: 0.35,
        sustain: 0.55,
        release: 0.35
      }
    },
    semanticDefaults: {
      brightness: 0.6,
      warmth: 0.55,
      punch: 0.6,
      attack: 0.2,
      sustain: 0.8
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 3, max: 12 },
      warmth: { param: "harmonicity", min: 0.5, max: 1.5 },
      punch: { param: "envelope.attack", min: 0.02, max: 0.12 }
    },
    tags: ["trombone", "brass", "orchestral", "powerful", "mid", "bass"]
  },
  /**
   * Tuba
   *
   * Deep, solid brass foundation.
   * Very low register, steady tone.
   */
  tuba: {
    name: "Tuba",
    category: "brass",
    description: "Deep, solid tuba foundation. Very low register for bass support.",
    type: "fmsynth",
    base: {
      harmonicity: 0.5,
      modulationIndex: 4,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.1,
        decay: 0.35,
        sustain: 0.75,
        release: 0.6
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.12,
        decay: 0.4,
        sustain: 0.4,
        release: 0.5
      }
    },
    semanticDefaults: {
      brightness: 0.35,
      warmth: 0.75,
      attack: 0.35,
      sustain: 0.75,
      release: 0.55
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1.5, max: 7 },
      warmth: { param: "harmonicity", min: 0.25, max: 0.75 },
      attack: { param: "envelope.attack", min: 0.06, max: 0.2 },
      release: { param: "envelope.release", min: 0.4, max: 1.2 }
    },
    tags: ["tuba", "brass", "orchestral", "deep", "bass", "foundation", "low"]
  },
  /**
   * Brass Ensemble
   *
   * Full brass section blend.
   * Rich, powerful for orchestral climaxes.
   */
  brass_ensemble: {
    name: "Brass Ensemble",
    category: "brass",
    description: "Full brass section blend. Powerful and majestic for climaxes.",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 6,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.1,
        decay: 0.3,
        sustain: 0.85,
        release: 0.6
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.12,
        decay: 0.4,
        sustain: 0.5,
        release: 0.5
      }
    },
    semanticDefaults: {
      brightness: 0.6,
      warmth: 0.55,
      richness: 0.8,
      attack: 0.3,
      sustain: 0.85,
      release: 0.55
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 3, max: 10 },
      warmth: { param: "harmonicity", min: 0.5, max: 1.5 },
      attack: { param: "envelope.attack", min: 0.05, max: 0.25 },
      release: { param: "envelope.release", min: 0.4, max: 1.2 }
    },
    tags: ["ensemble", "brass", "orchestral", "powerful", "majestic", "section", "cinematic"]
  },
  /**
   * Muted Trumpet
   *
   * Soft, jazzy tone with harmon mute character.
   * Filtered and intimate.
   */
  muted_trumpet: {
    name: "Muted Trumpet",
    category: "brass",
    description: "Soft, jazzy muted trumpet. Filtered, intimate harmon mute sound.",
    type: "fmsynth",
    base: {
      harmonicity: 1.5,
      // Slightly higher for nasal quality
      modulationIndex: 5,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.04,
        decay: 0.2,
        sustain: 0.7,
        release: 0.25
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.05,
        decay: 0.25,
        sustain: 0.4,
        release: 0.2
      }
    },
    semanticDefaults: {
      brightness: 0.5,
      warmth: 0.5,
      punch: 0.5,
      attack: 0.12,
      sustain: 0.7
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 8 },
      warmth: { param: "harmonicity", min: 1, max: 2.5 },
      punch: { param: "envelope.attack", min: 0.02, max: 0.08 }
    },
    tags: ["muted", "trumpet", "brass", "jazz", "intimate", "soft", "harmon"]
  }
};

// src/presets/woodwinds.ts
var WOODWINDS_PRESETS = {
  /**
   * Flute
   *
   * Airy, pure tone with breath component.
   * Low modulation index for clean, almost sine-like quality.
   */
  flute: {
    name: "Flute",
    category: "woodwinds",
    description: "Airy, pure flute tone. Breathy attack with clear sustain.",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 1.5,
      // Very low for pure tone
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.08,
        decay: 0.15,
        sustain: 0.9,
        release: 0.2
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.1,
        decay: 0.2,
        sustain: 0.3,
        release: 0.15
      }
    },
    semanticDefaults: {
      brightness: 0.7,
      warmth: 0.4,
      attack: 0.25,
      sustain: 0.9,
      release: 0.25
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 0.5, max: 3 },
      warmth: { param: "harmonicity", min: 0.5, max: 1.5 },
      attack: { param: "envelope.attack", min: 0.03, max: 0.15 },
      release: { param: "envelope.release", min: 0.1, max: 0.4 }
    },
    tags: ["flute", "woodwind", "orchestral", "airy", "pure", "high", "breathy"]
  },
  /**
   * Clarinet
   *
   * Warm, woody tone with characteristic odd harmonics.
   * Covers a wide range from chalumeau to altissimo.
   */
  clarinet: {
    name: "Clarinet",
    category: "woodwinds",
    description: "Warm, woody clarinet. Rich chalumeau register, clear upper tones.",
    type: "fmsynth",
    base: {
      harmonicity: 3,
      // Third harmonic emphasis (odd harmonics characteristic)
      modulationIndex: 2.5,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.06,
        decay: 0.2,
        sustain: 0.85,
        release: 0.25
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.08,
        decay: 0.25,
        sustain: 0.4,
        release: 0.2
      }
    },
    semanticDefaults: {
      brightness: 0.5,
      warmth: 0.65,
      attack: 0.2,
      sustain: 0.85,
      release: 0.3
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1, max: 5 },
      warmth: { param: "harmonicity", min: 2, max: 4 },
      attack: { param: "envelope.attack", min: 0.03, max: 0.12 },
      release: { param: "envelope.release", min: 0.15, max: 0.5 }
    },
    tags: ["clarinet", "woodwind", "orchestral", "warm", "woody", "jazz"]
  },
  /**
   * Oboe
   *
   * Reedy, expressive double-reed character.
   * Nasal quality with penetrating tone.
   */
  oboe: {
    name: "Oboe",
    category: "woodwinds",
    description: "Reedy, expressive oboe. Nasal, penetrating double-reed character.",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 4,
      // Higher for reedy buzz
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.05,
        decay: 0.15,
        sustain: 0.88,
        release: 0.2
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.06,
        decay: 0.2,
        sustain: 0.5,
        release: 0.18
      }
    },
    semanticDefaults: {
      brightness: 0.6,
      warmth: 0.45,
      attack: 0.15,
      sustain: 0.88,
      release: 0.25
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 7 },
      warmth: { param: "harmonicity", min: 1.5, max: 3 },
      attack: { param: "envelope.attack", min: 0.02, max: 0.1 },
      release: { param: "envelope.release", min: 0.1, max: 0.35 }
    },
    tags: ["oboe", "woodwind", "orchestral", "reedy", "expressive", "nasal", "double-reed"]
  },
  /**
   * Bassoon
   *
   * Dark, rich bass woodwind.
   * Deep register with comedic to noble character.
   */
  bassoon: {
    name: "Bassoon",
    category: "woodwinds",
    description: "Dark, rich bassoon. Deep bass woodwind with character.",
    type: "fmsynth",
    base: {
      harmonicity: 1.5,
      modulationIndex: 3,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.08,
        decay: 0.25,
        sustain: 0.8,
        release: 0.35
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.1,
        decay: 0.3,
        sustain: 0.35,
        release: 0.3
      }
    },
    semanticDefaults: {
      brightness: 0.4,
      warmth: 0.7,
      attack: 0.25,
      sustain: 0.8,
      release: 0.4
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1.5, max: 5 },
      warmth: { param: "harmonicity", min: 1, max: 2.5 },
      attack: { param: "envelope.attack", min: 0.05, max: 0.15 },
      release: { param: "envelope.release", min: 0.2, max: 0.6 }
    },
    tags: ["bassoon", "woodwind", "orchestral", "dark", "bass", "rich", "double-reed"]
  },
  /**
   * Piccolo
   *
   * Bright, piercing high register.
   * Very pure tone at extreme high frequencies.
   */
  piccolo: {
    name: "Piccolo",
    category: "woodwinds",
    description: "Bright, piercing piccolo. Very high register, brilliant tone.",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 1,
      // Very pure, like flute but brighter
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.05,
        decay: 0.1,
        sustain: 0.92,
        release: 0.15
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.06,
        decay: 0.15,
        sustain: 0.25,
        release: 0.12
      }
    },
    semanticDefaults: {
      brightness: 0.85,
      warmth: 0.3,
      attack: 0.15,
      sustain: 0.92,
      release: 0.2
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 0.5, max: 2.5 },
      warmth: { param: "harmonicity", min: 0.5, max: 1.5 },
      attack: { param: "envelope.attack", min: 0.02, max: 0.1 },
      release: { param: "envelope.release", min: 0.08, max: 0.3 }
    },
    tags: ["piccolo", "woodwind", "orchestral", "bright", "high", "piercing", "brilliant"]
  },
  /**
   * English Horn
   *
   * Melancholic, mellow alto oboe.
   * Darker and more rounded than oboe.
   */
  english_horn: {
    name: "English Horn",
    category: "woodwinds",
    description: "Melancholic, mellow English horn. Darker alto oboe character.",
    type: "fmsynth",
    base: {
      harmonicity: 1.5,
      modulationIndex: 3,
      // Less harsh than oboe
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.07,
        decay: 0.2,
        sustain: 0.85,
        release: 0.3
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.08,
        decay: 0.25,
        sustain: 0.4,
        release: 0.25
      }
    },
    semanticDefaults: {
      brightness: 0.45,
      warmth: 0.6,
      attack: 0.2,
      sustain: 0.85,
      release: 0.35
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1.5, max: 5 },
      warmth: { param: "harmonicity", min: 1, max: 2.5 },
      attack: { param: "envelope.attack", min: 0.04, max: 0.12 },
      release: { param: "envelope.release", min: 0.2, max: 0.5 }
    },
    tags: ["english horn", "cor anglais", "woodwind", "orchestral", "melancholic", "mellow", "alto"]
  }
};

// src/presets/orchestral.ts
var ORCHESTRAL_PRESETS = {
  // =========================================================================
  // CHOIR (4 presets)
  // =========================================================================
  /**
   * Choir Aah
   *
   * Open vowel sustained choir sound.
   * Bright and full.
   */
  choir_aah: {
    name: "Choir Aah",
    category: "orchestral",
    description: 'Open "aah" vowel choir. Bright, full sustained voices.',
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 3,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.4,
        decay: 0.3,
        sustain: 0.85,
        release: 0.8
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.5,
        decay: 0.4,
        sustain: 0.5,
        release: 0.6
      }
    },
    semanticDefaults: {
      brightness: 0.55,
      warmth: 0.6,
      attack: 0.6,
      sustain: 0.85,
      release: 0.6
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1.5, max: 5 },
      warmth: { param: "harmonicity", min: 0.5, max: 1.5 },
      attack: { param: "envelope.attack", min: 0.2, max: 0.8 },
      release: { param: "envelope.release", min: 0.5, max: 1.5 }
    },
    tags: ["choir", "aah", "vocal", "orchestral", "voices", "bright", "sustained"]
  },
  /**
   * Choir Ooh
   *
   * Rounded vowel for warmer passages.
   * More intimate than aah.
   */
  choir_ooh: {
    name: "Choir Ooh",
    category: "orchestral",
    description: 'Rounded "ooh" vowel choir. Warm, intimate sustained voices.',
    type: "fmsynth",
    base: {
      harmonicity: 0.5,
      // Lower for darker vowel
      modulationIndex: 2,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.45,
        decay: 0.35,
        sustain: 0.8,
        release: 0.9
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.55,
        decay: 0.5,
        sustain: 0.4,
        release: 0.7
      }
    },
    semanticDefaults: {
      brightness: 0.35,
      warmth: 0.75,
      attack: 0.65,
      sustain: 0.8,
      release: 0.65
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1, max: 4 },
      warmth: { param: "harmonicity", min: 0.25, max: 1 },
      attack: { param: "envelope.attack", min: 0.25, max: 0.9 },
      release: { param: "envelope.release", min: 0.6, max: 1.8 }
    },
    tags: ["choir", "ooh", "vocal", "orchestral", "voices", "warm", "intimate"]
  },
  /**
   * Choir Mmm
   *
   * Closed humming sound.
   * Very soft and ethereal.
   */
  choir_mmm: {
    name: "Choir Mmm",
    category: "orchestral",
    description: 'Closed "mmm" humming choir. Very soft, ethereal texture.',
    type: "fmsynth",
    base: {
      harmonicity: 0.25,
      // Very low for closed vowel
      modulationIndex: 1.5,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.5,
        decay: 0.4,
        sustain: 0.75,
        release: 1
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.6,
        decay: 0.6,
        sustain: 0.3,
        release: 0.8
      }
    },
    semanticDefaults: {
      brightness: 0.25,
      warmth: 0.85,
      attack: 0.7,
      sustain: 0.75,
      release: 0.7
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 0.5, max: 3 },
      warmth: { param: "harmonicity", min: 0.1, max: 0.5 },
      attack: { param: "envelope.attack", min: 0.3, max: 1 },
      release: { param: "envelope.release", min: 0.7, max: 2 }
    },
    tags: ["choir", "mmm", "hum", "vocal", "orchestral", "voices", "soft", "ethereal"]
  },
  /**
   * Mixed Choir
   *
   * Blended vowels for general choral sound.
   * Versatile, full-bodied.
   */
  mixed_choir: {
    name: "Mixed Choir",
    category: "orchestral",
    description: "Blended mixed choir. Full-bodied choral sound for all contexts.",
    type: "fmsynth",
    base: {
      harmonicity: 0.75,
      modulationIndex: 2.5,
      oscillator: { type: "sine" },
      envelope: {
        attack: 0.4,
        decay: 0.35,
        sustain: 0.82,
        release: 0.85
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 0.5,
        decay: 0.45,
        sustain: 0.45,
        release: 0.65
      }
    },
    semanticDefaults: {
      brightness: 0.45,
      warmth: 0.65,
      richness: 0.7,
      attack: 0.6,
      sustain: 0.82,
      release: 0.65
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1.5, max: 4.5 },
      warmth: { param: "harmonicity", min: 0.4, max: 1.2 },
      attack: { param: "envelope.attack", min: 0.2, max: 0.8 },
      release: { param: "envelope.release", min: 0.5, max: 1.5 }
    },
    tags: ["choir", "mixed", "vocal", "orchestral", "voices", "full", "versatile", "choral"]
  },
  // =========================================================================
  // ORCHESTRAL PERCUSSION (7 presets) - v0.9.4.1 FM Redesign
  // =========================================================================
  /**
   * Timpani (v0.9.4.1)
   *
   * Tuned orchestral kettledrum using membrane synthesis.
   * Lower octaves for deeper fundamental, longer decay for resonance.
   */
  timpani: {
    name: "Timpani",
    category: "orchestral",
    description: "Orchestral kettledrum. Deep, booming, tuned percussion.",
    type: "membrane",
    base: {
      pitchDecay: 0.05,
      // Slower pitch drop for more defined pitch
      octaves: 2.5,
      // Lower for deeper, more resonant fundamental
      envelope: {
        attack: 1e-3,
        decay: 2.5,
        // Longer decay for room resonance
        sustain: 0.1,
        release: 1.5
      }
    },
    semanticDefaults: {
      brightness: 0.35,
      warmth: 0.75,
      punch: 0.7,
      decay: 0.8,
      release: 0.7
    },
    semanticMappings: {
      brightness: { param: "octaves", min: 1.5, max: 4 },
      punch: { param: "pitchDecay", min: 0.02, max: 0.1 },
      decay: { param: "envelope.decay", min: 1.5, max: 4 },
      release: { param: "envelope.release", min: 0.8, max: 2.5 }
    },
    tags: ["timpani", "kettle", "drum", "orchestral", "percussion", "tuned", "booming", "dramatic"]
  },
  /**
   * Glockenspiel (v0.9.4.1)
   *
   * Bright, bell-like metallic bars using FM synthesis.
   * High harmonicity creates inharmonic bell partials.
   * Very high register, crystalline, sparkling character.
   */
  glockenspiel: {
    name: "Glockenspiel",
    category: "orchestral",
    description: "Bright, bell-like glockenspiel. Crystalline high register.",
    type: "fmsynth",
    base: {
      harmonicity: 12,
      // High for inharmonic bell partials
      modulationIndex: 6,
      // Moderate-high for brightness
      oscillator: { type: "sine" },
      envelope: {
        attack: 1e-3,
        decay: 1.8,
        sustain: 0.05,
        release: 1
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 1e-3,
        decay: 0.3,
        // Fast mod decay for crisp attack
        sustain: 0.02,
        release: 0.2
      }
    },
    semanticDefaults: {
      brightness: 0.8,
      warmth: 0.25,
      decay: 0.65,
      release: 0.5
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 3, max: 10 },
      warmth: { param: "harmonicity", min: 8, max: 16 },
      decay: { param: "envelope.decay", min: 1, max: 3 },
      release: { param: "envelope.release", min: 0.5, max: 2 }
    },
    tags: ["glockenspiel", "bells", "orchestral", "percussion", "tuned", "bright", "crystalline", "sparkling"]
  },
  /**
   * Xylophone (v0.9.4.1)
   *
   * Wooden bars with bright, percussive attack using FM synthesis.
   * Lower harmonicity than glockenspiel for "woodier" character.
   * Short decay for rhythmic definition.
   */
  xylophone: {
    name: "Xylophone",
    category: "orchestral",
    description: "Bright xylophone with percussive attack. Wooden, rhythmic.",
    type: "fmsynth",
    base: {
      harmonicity: 4,
      // Lower for woody character (less inharmonic)
      modulationIndex: 4,
      // Moderate for presence without harshness
      oscillator: { type: "sine" },
      envelope: {
        attack: 1e-3,
        decay: 0.5,
        // Short decay for rhythmic character
        sustain: 0.02,
        release: 0.3
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 1e-3,
        decay: 0.15,
        // Very fast mod decay for sharp attack
        sustain: 0,
        release: 0.1
      }
    },
    semanticDefaults: {
      brightness: 0.65,
      warmth: 0.4,
      punch: 0.75,
      decay: 0.35
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 7 },
      warmth: { param: "harmonicity", min: 2, max: 6 },
      punch: { param: "modulationEnvelope.decay", min: 0.05, max: 0.3 },
      decay: { param: "envelope.decay", min: 0.3, max: 1 }
    },
    tags: ["xylophone", "wooden", "orchestral", "percussion", "tuned", "bright", "rhythmic", "percussive"]
  },
  /**
   * Vibraphone (v0.9.4.1)
   *
   * Warm, sustained metal bars using FM synthesis.
   * Jazz standard with warm sustain and gentle character.
   * Based on fm_vibraphone but tuned for orchestral context.
   */
  vibraphone: {
    name: "Vibraphone",
    category: "orchestral",
    description: "Warm vibraphone with sustained ring. Jazz staple.",
    type: "fmsynth",
    base: {
      harmonicity: 4,
      // Moderate for warm metal character
      modulationIndex: 3,
      // Lower for mellower tone
      oscillator: { type: "sine" },
      envelope: {
        attack: 1e-3,
        decay: 2.5,
        // Long decay for sustained ring
        sustain: 0.15,
        release: 1.5
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 1e-3,
        decay: 0.6,
        // Moderate mod decay for warm attack
        sustain: 0.05,
        release: 0.4
      }
    },
    semanticDefaults: {
      brightness: 0.5,
      warmth: 0.7,
      decay: 0.75,
      release: 0.65
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1.5, max: 6 },
      warmth: { param: "harmonicity", min: 2, max: 6 },
      decay: { param: "envelope.decay", min: 1.5, max: 4 },
      release: { param: "envelope.release", min: 0.8, max: 2.5 }
    },
    tags: ["vibraphone", "vibes", "jazz", "orchestral", "percussion", "tuned", "warm", "sustained", "mellow"]
  },
  /**
   * Marimba (v0.9.4.1)
   *
   * Mellow, wooden bar resonance using FM synthesis.
   * Lower harmonicity for fundamental-focused, woody character.
   * Warm and gentle, wider range than xylophone.
   */
  marimba: {
    name: "Marimba",
    category: "orchestral",
    description: "Mellow marimba with warm wooden resonance. Gentle and lyrical.",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      // Low for fundamental-focused woody tone
      modulationIndex: 2.5,
      // Low for mellow character
      oscillator: { type: "sine" },
      envelope: {
        attack: 1e-3,
        decay: 1.2,
        // Medium decay for resonance without ring
        sustain: 0.08,
        release: 0.7
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 1e-3,
        decay: 0.4,
        // Moderate decay for rounded attack
        sustain: 0.02,
        release: 0.25
      }
    },
    semanticDefaults: {
      brightness: 0.4,
      warmth: 0.75,
      decay: 0.55,
      release: 0.5
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1, max: 5 },
      warmth: { param: "harmonicity", min: 1, max: 4 },
      decay: { param: "envelope.decay", min: 0.7, max: 2 },
      release: { param: "envelope.release", min: 0.4, max: 1.2 }
    },
    tags: ["marimba", "wooden", "mellow", "orchestral", "percussion", "tuned", "warm", "lyrical", "gentle"]
  },
  /**
   * Tubular Bells (v0.9.4.1)
   *
   * Deep church bell sound using FM synthesis.
   * High harmonicity for inharmonic bell partials.
   * Long decay for dramatic, ceremonial character.
   */
  tubular_bells: {
    name: "Tubular Bells",
    category: "orchestral",
    description: "Deep tubular bells. Dramatic church bell sound.",
    type: "fmsynth",
    base: {
      harmonicity: 5.07,
      // Classic DX7 bell ratio (inharmonic)
      modulationIndex: 8,
      // High for rich overtones
      oscillator: { type: "sine" },
      envelope: {
        attack: 1e-3,
        decay: 5,
        // Very long decay for bell ring
        sustain: 0.1,
        release: 3
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 1e-3,
        decay: 1.5,
        // Slower mod decay preserves brightness longer
        sustain: 0.05,
        release: 1
      }
    },
    semanticDefaults: {
      brightness: 0.6,
      warmth: 0.45,
      decay: 0.85,
      release: 0.75
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 4, max: 12 },
      warmth: { param: "harmonicity", min: 3, max: 8 },
      decay: { param: "envelope.decay", min: 3, max: 8 },
      release: { param: "envelope.release", min: 2, max: 5 }
    },
    tags: ["tubular bells", "church bells", "orchestral", "percussion", "tuned", "dramatic", "ceremonial", "chime"]
  },
  /**
   * Celesta (v0.9.4.1)
   *
   * Magical, ethereal bell-piano hybrid using FM synthesis.
   * Delicate and sparkling with piano-like envelope behavior.
   * Higher register character than vibraphone.
   */
  celesta: {
    name: "Celesta",
    category: "orchestral",
    description: "Magical celesta. Ethereal, delicate bell-piano character.",
    type: "fmsynth",
    base: {
      harmonicity: 6,
      // Moderately inharmonic for bell character
      modulationIndex: 4,
      // Moderate for delicate brightness
      oscillator: { type: "sine" },
      envelope: {
        attack: 1e-3,
        decay: 1.5,
        // Medium-long decay
        sustain: 0.05,
        release: 0.8
      },
      modulation: { type: "sine" },
      modulationEnvelope: {
        attack: 1e-3,
        decay: 0.35,
        // Fast mod decay like piano hammer
        sustain: 0.01,
        release: 0.2
      }
    },
    semanticDefaults: {
      brightness: 0.65,
      warmth: 0.5,
      decay: 0.55,
      release: 0.5
    },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 8 },
      warmth: { param: "harmonicity", min: 4, max: 10 },
      decay: { param: "envelope.decay", min: 0.8, max: 2.5 },
      release: { param: "envelope.release", min: 0.4, max: 1.5 }
    },
    tags: ["celesta", "magical", "ethereal", "orchestral", "percussion", "tuned", "delicate", "sparkling", "bell-piano"]
  }
};

// src/presets/samples.ts
var TONEJS_SAMPLES_CDN = "https://nbrosowsky.github.io/tonejs-instruments/samples/";
var SAMPLES_PRESETS = {
  /**
   * Sampled Piano - Full Steinway grand piano recordings
   */
  sample_piano: {
    name: "Sampled Piano",
    category: "samples",
    description: "Recorded Steinway grand piano with natural dynamics and resonance",
    type: "sampler",
    base: {
      instrument: "piano",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["piano", "acoustic", "realistic", "samples", "grand", "classical"]
  },
  /**
   * Sampled Violin - Solo violin recordings
   */
  sample_violin: {
    name: "Sampled Violin",
    category: "samples",
    description: "Recorded solo violin with expressive legato character",
    type: "sampler",
    base: {
      instrument: "violin",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["violin", "strings", "acoustic", "realistic", "samples", "orchestral"]
  },
  /**
   * Sampled Cello - Solo cello recordings
   */
  sample_cello: {
    name: "Sampled Cello",
    category: "samples",
    description: "Recorded solo cello with rich, deep tone",
    type: "sampler",
    base: {
      instrument: "cello",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["cello", "strings", "acoustic", "realistic", "samples", "orchestral"]
  },
  /**
   * Sampled Contrabass - Double bass recordings
   */
  sample_contrabass: {
    name: "Sampled Contrabass",
    category: "samples",
    description: "Recorded double bass for orchestral foundation or jazz walking bass",
    type: "sampler",
    base: {
      instrument: "contrabass",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["contrabass", "bass", "strings", "acoustic", "realistic", "samples", "jazz", "orchestral"]
  },
  /**
   * Sampled Flute - Concert flute recordings
   */
  sample_flute: {
    name: "Sampled Flute",
    category: "samples",
    description: "Recorded concert flute with airy, lyrical tone",
    type: "sampler",
    base: {
      instrument: "flute",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["flute", "woodwinds", "acoustic", "realistic", "samples", "orchestral"]
  },
  /**
   * Sampled Clarinet - B-flat clarinet recordings
   */
  sample_clarinet: {
    name: "Sampled Clarinet",
    category: "samples",
    description: "Recorded clarinet with warm, woody tone",
    type: "sampler",
    base: {
      instrument: "clarinet",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["clarinet", "woodwinds", "acoustic", "realistic", "samples", "jazz", "orchestral"]
  },
  /**
   * Sampled Bassoon - Bassoon recordings
   */
  sample_bassoon: {
    name: "Sampled Bassoon",
    category: "samples",
    description: "Recorded bassoon with dark, reedy character",
    type: "sampler",
    base: {
      instrument: "bassoon",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["bassoon", "woodwinds", "acoustic", "realistic", "samples", "orchestral"]
  },
  /**
   * Sampled French Horn - French horn recordings
   */
  sample_french_horn: {
    name: "Sampled French Horn",
    category: "samples",
    description: "Recorded French horn with noble, warm tone",
    type: "sampler",
    base: {
      instrument: "french-horn",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["french horn", "brass", "acoustic", "realistic", "samples", "orchestral"]
  },
  /**
   * Sampled Trumpet - Solo trumpet recordings
   */
  sample_trumpet: {
    name: "Sampled Trumpet",
    category: "samples",
    description: "Recorded trumpet with bright, heroic tone. Great for jazz or orchestral",
    type: "sampler",
    base: {
      instrument: "trumpet",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["trumpet", "brass", "acoustic", "realistic", "samples", "jazz", "orchestral"]
  },
  /**
   * Sampled Trombone - Trombone recordings
   */
  sample_trombone: {
    name: "Sampled Trombone",
    category: "samples",
    description: "Recorded trombone with bold, powerful tone",
    type: "sampler",
    base: {
      instrument: "trombone",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["trombone", "brass", "acoustic", "realistic", "samples", "jazz", "orchestral"]
  },
  /**
   * Sampled Tuba - Tuba recordings
   */
  sample_tuba: {
    name: "Sampled Tuba",
    category: "samples",
    description: "Recorded tuba with deep, massive foundation",
    type: "sampler",
    base: {
      instrument: "tuba",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["tuba", "brass", "acoustic", "realistic", "samples", "orchestral"]
  },
  /**
   * Sampled Saxophone - Alto/Tenor saxophone recordings
   */
  sample_saxophone: {
    name: "Sampled Saxophone",
    category: "samples",
    description: "Recorded saxophone with expressive, soulful tone",
    type: "sampler",
    base: {
      instrument: "saxophone",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["saxophone", "woodwinds", "acoustic", "realistic", "samples", "jazz", "soul"]
  },
  /**
   * Sampled Acoustic Guitar - Nylon string guitar
   */
  sample_guitar_acoustic: {
    name: "Sampled Acoustic Guitar",
    category: "samples",
    description: "Recorded nylon string acoustic guitar with warm, intimate tone",
    type: "sampler",
    base: {
      instrument: "guitar-acoustic",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["guitar", "acoustic", "realistic", "samples", "nylon", "classical"]
  },
  /**
   * Sampled Electric Guitar - Clean electric guitar
   */
  sample_guitar_electric: {
    name: "Sampled Electric Guitar",
    category: "samples",
    description: "Recorded clean electric guitar tone",
    type: "sampler",
    base: {
      instrument: "guitar-electric",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["guitar", "electric", "realistic", "samples", "clean"]
  },
  /**
   * Sampled Nylon Guitar - Classical guitar
   */
  sample_guitar_nylon: {
    name: "Sampled Nylon Guitar",
    category: "samples",
    description: "Recorded classical nylon string guitar",
    type: "sampler",
    base: {
      instrument: "guitar-nylon",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["guitar", "acoustic", "realistic", "samples", "nylon", "classical"]
  },
  /**
   * Sampled Electric Bass - Fretted bass guitar
   */
  sample_bass_electric: {
    name: "Sampled Electric Bass",
    category: "samples",
    description: "Recorded electric bass guitar with punchy, defined tone",
    type: "sampler",
    base: {
      instrument: "bass-electric",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["bass", "electric", "realistic", "samples", "funk", "rock"]
  },
  /**
   * Sampled Harp - Concert harp recordings
   */
  sample_harp: {
    name: "Sampled Harp",
    category: "samples",
    description: "Recorded concert harp with ethereal, resonant tone",
    type: "sampler",
    base: {
      instrument: "harp",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["harp", "strings", "acoustic", "realistic", "samples", "orchestral", "ethereal"]
  },
  /**
   * Sampled Xylophone - Orchestra xylophone
   */
  sample_xylophone: {
    name: "Sampled Xylophone",
    category: "samples",
    description: "Recorded xylophone with bright, percussive attack",
    type: "sampler",
    base: {
      instrument: "xylophone",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["xylophone", "percussion", "acoustic", "realistic", "samples", "orchestral"]
  },
  /**
   * Sampled Organ - Pipe organ recordings
   */
  sample_organ: {
    name: "Sampled Organ",
    category: "samples",
    description: "Recorded pipe organ with rich, sustained tone",
    type: "sampler",
    base: {
      instrument: "organ",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["organ", "keys", "acoustic", "realistic", "samples", "church", "classical"]
  },
  /**
   * Sampled Harmonium - Indian harmonium recordings
   */
  sample_harmonium: {
    name: "Sampled Harmonium",
    category: "samples",
    description: "Recorded Indian harmonium with warm, reedy character",
    type: "sampler",
    base: {
      instrument: "harmonium",
      baseUrl: TONEJS_SAMPLES_CDN
    },
    tags: ["harmonium", "keys", "acoustic", "realistic", "samples", "world", "indian"]
  }
};

// src/presets/index.ts
var PRESET_REGISTRY = {
  ...SYNTH_PRESETS,
  ...BASS_PRESETS,
  ...PAD_PRESETS,
  ...LEAD_PRESETS,
  ...KEYS_PRESETS,
  ...PLUCK_PRESETS,
  ...FM_PRESETS,
  ...TEXTURE_PRESETS,
  ...DRUM_PRESETS,
  ...LOFI_PRESETS,
  ...CINEMATIC_PRESETS,
  ...WORLD_PRESETS,
  ...AMBIENT_PRESETS,
  ...MODERN_PRESETS,
  // v0.9.4: Orchestral presets
  ...STRINGS_PRESETS,
  ...BRASS_PRESETS,
  ...WOODWINDS_PRESETS,
  ...ORCHESTRAL_PRESETS,
  // v0.9.11: Sample-based presets
  ...SAMPLES_PRESETS
};

// src/synthesis/presets.ts
var PRESET_DEFINITIONS = {
  // ============================================================================
  // Basic Synths
  // ============================================================================
  "synth": {
    name: "Basic Synth",
    category: "synth",
    description: "Versatile triangle wave synth - good all-around sound",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.02, decay: 0.1, sustain: 0.3, release: 0.8 }
    },
    semanticDefaults: { brightness: 0.4, attack: 0.1, decay: 0.3, sustain: 0.3, release: 0.4 }
  },
  "sine": {
    name: "Sine Wave",
    category: "synth",
    description: "Pure, clean sine wave - dark and simple",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.5, release: 0.5 }
    },
    semanticDefaults: { brightness: 0.1, warmth: 0.8 }
  },
  "square": {
    name: "Square Wave",
    category: "synth",
    description: "Retro hollow square wave - 8-bit character",
    type: "polysynth",
    base: {
      oscillator: { type: "square" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.4, release: 0.3 }
    },
    semanticDefaults: { brightness: 0.6 }
  },
  "sawtooth": {
    name: "Sawtooth",
    category: "synth",
    description: "Bright, buzzy sawtooth - classic synth sound",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.2, sustain: 0.3, release: 0.4 }
    },
    semanticDefaults: { brightness: 0.8 }
  },
  // ============================================================================
  // Bass Presets
  // ============================================================================
  "synth_bass": {
    name: "Synth Bass",
    category: "bass",
    description: "Classic filtered sawtooth bass with punch",
    type: "monosynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.2, sustain: 0.4, release: 0.3 },
      filterEnvelope: {
        attack: 0.01,
        decay: 0.2,
        sustain: 0.3,
        release: 0.2,
        baseFrequency: 200,
        octaves: 2.5
      }
    },
    semanticDefaults: { brightness: 0.5, punch: 0.7, attack: 0.05 }
  },
  "sub_bass": {
    name: "Sub Bass",
    category: "bass",
    description: "Deep, clean sine sub-bass for foundation",
    type: "monosynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 0.02, decay: 0.1, sustain: 0.8, release: 0.5 }
    },
    semanticDefaults: { brightness: 0.1, warmth: 0.9, sustain: 0.8 }
  },
  "pluck_bass": {
    name: "Pluck Bass",
    category: "bass",
    description: "Short, plucky bass for staccato lines",
    type: "monosynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.01, decay: 0.3, sustain: 0.1, release: 0.2 },
      filterEnvelope: {
        attack: 0.01,
        decay: 0.15,
        sustain: 0.1,
        release: 0.1,
        baseFrequency: 300,
        octaves: 2
      }
    },
    semanticDefaults: { punch: 0.8, decay: 0.3, sustain: 0.1 }
  },
  "fm_bass": {
    name: "FM Bass",
    category: "bass",
    description: "Punchy FM bass with fast modulation decay",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.3, sustain: 0.5, release: 0.2 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.1, sustain: 0.2, release: 0.1 }
    },
    semanticDefaults: { punch: 0.9, brightness: 0.6 },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 12 }
    }
  },
  // ============================================================================
  // Pad Presets
  // ============================================================================
  "warm_pad": {
    name: "Warm Pad",
    category: "pad",
    description: "Slow-attack triangle pad - soft and warm",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.5, decay: 0.3, sustain: 0.8, release: 1.5 }
    },
    semanticDefaults: { attack: 0.5, sustain: 0.8, release: 0.6, warmth: 0.8 }
  },
  "string_pad": {
    name: "String Pad",
    category: "pad",
    description: "Orchestral string-like pad with shimmer",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.4, decay: 0.2, sustain: 0.7, release: 1.2 }
    },
    semanticDefaults: { attack: 0.4, brightness: 0.6, richness: 0.7 }
  },
  "ambient_pad": {
    name: "Ambient Pad",
    category: "pad",
    description: "Ethereal, evolving ambient texture",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 1, decay: 0.5, sustain: 0.9, release: 2 }
    },
    semanticDefaults: { attack: 0.8, sustain: 0.9, release: 0.8, warmth: 0.7 }
  },
  // ============================================================================
  // Lead Presets
  // ============================================================================
  "lead": {
    name: "Lead Synth",
    category: "lead",
    description: "Bright sawtooth lead with filter sweep",
    type: "monosynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.6, release: 0.3 },
      filterEnvelope: {
        attack: 0.01,
        decay: 0.1,
        sustain: 0.5,
        release: 0.3,
        baseFrequency: 800,
        octaves: 2
      }
    },
    semanticDefaults: { brightness: 0.7, punch: 0.6 }
  },
  "soft_lead": {
    name: "Soft Lead",
    category: "lead",
    description: "Mellow triangle lead for gentle melodies",
    type: "monosynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.05, decay: 0.2, sustain: 0.5, release: 0.5 }
    },
    semanticDefaults: { brightness: 0.3, warmth: 0.7, attack: 0.15 }
  },
  // ============================================================================
  // Keys Presets
  // ============================================================================
  "electric_piano": {
    name: "Electric Piano",
    category: "keys",
    description: "Classic Rhodes-like tone with bell attack",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 0.8, sustain: 0.2, release: 0.8 }
    },
    semanticDefaults: { punch: 0.5, decay: 0.6 }
  },
  "organ": {
    name: "Organ",
    category: "keys",
    description: "Sustained organ tone with no decay",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 0.01, sustain: 1, release: 0.1 }
    },
    semanticDefaults: { sustain: 1, release: 0.05 }
  },
  // ============================================================================
  // Pluck Presets
  // ============================================================================
  "pluck": {
    name: "Pluck",
    category: "pluck",
    description: "Basic plucked string sound",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 1e-3, decay: 0.4, sustain: 0, release: 0.2 }
    },
    semanticDefaults: { attack: 0.01, decay: 0.4, sustain: 0 }
  },
  "bell": {
    name: "Bell",
    category: "pluck",
    description: "Bright bell tone with long decay",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1, sustain: 0, release: 1 }
    },
    semanticDefaults: { attack: 0.01, decay: 0.7, sustain: 0, release: 0.6 }
  },
  "marimba": {
    name: "Marimba",
    category: "pluck",
    description: "Mallet percussion tone",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.5, sustain: 0, release: 0.3 }
    },
    semanticDefaults: { decay: 0.4, warmth: 0.6 }
  },
  // ============================================================================
  // FM Synthesis Presets (DX7-inspired)
  // ============================================================================
  "fm_epiano": {
    name: "FM Electric Piano",
    category: "fm",
    description: "DX7-style Rhodes with warm bell-like attack",
    type: "fmsynth",
    base: {
      harmonicity: 1.0007,
      modulationIndex: 4,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 2.5, sustain: 0.1, release: 1.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.4, sustain: 0.1, release: 0.3 }
    },
    semanticDefaults: { brightness: 0.5, warmth: 0.6 },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 1, max: 12 },
      warmth: { param: "harmonicity", min: 0.5, max: 2 }
    }
  },
  "fm_brass": {
    name: "FM Brass",
    category: "fm",
    description: "Brass stab where brightness tracks loudness",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 12,
      oscillator: { type: "sine" },
      envelope: { attack: 0.05, decay: 0.2, sustain: 0.7, release: 0.3 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.08, decay: 0.3, sustain: 0.6, release: 0.2 }
    },
    semanticDefaults: { punch: 0.7, brightness: 0.7 }
  },
  "fm_church_bell": {
    name: "FM Church Bell",
    category: "fm",
    description: "Large church bell with inharmonic partials",
    type: "fmsynth",
    base: {
      harmonicity: 14,
      modulationIndex: 8,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 4, sustain: 0, release: 3 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 2, sustain: 0, release: 1.5 }
    },
    semanticDefaults: { decay: 0.9, release: 0.8 }
  },
  "fm_tubular_bell": {
    name: "FM Tubular Bell",
    category: "fm",
    description: "Classic DX7 chime bell",
    type: "fmsynth",
    base: {
      harmonicity: 5.07,
      modulationIndex: 10,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0, release: 1.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 1, sustain: 0, release: 1 }
    }
    // No semanticDefaults - use base values exactly
  },
  "fm_glass": {
    name: "FM Glass",
    category: "fm",
    description: "Crystal/glass sound with delicate timbre",
    type: "fmsynth",
    base: {
      harmonicity: 7,
      modulationIndex: 3,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0, release: 1 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.3, sustain: 0, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.8, decay: 0.5 }
  },
  "fm_vibraphone": {
    name: "FM Vibraphone",
    category: "fm",
    description: "Mallet percussion with FM warmth",
    type: "fmsynth",
    base: {
      harmonicity: 4,
      modulationIndex: 3.5,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.8, sustain: 0.1, release: 1.2 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.5, sustain: 0, release: 0.3 }
    },
    semanticDefaults: { warmth: 0.7, decay: 0.6 }
  },
  "fm_organ": {
    name: "FM Organ",
    category: "fm",
    description: "FM organ with sustained brightness",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 2,
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 0.01, sustain: 1, release: 0.1 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.01, decay: 0.01, sustain: 1, release: 0.1 }
    },
    semanticDefaults: { sustain: 1, brightness: 0.4 }
  },
  "fm_bell": {
    name: "FM Bell",
    category: "fm",
    description: "Classic DX7 chime bell",
    type: "fmsynth",
    base: {
      harmonicity: 5.07,
      modulationIndex: 10,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0, release: 1.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 1, sustain: 0, release: 1 }
    }
    // No semanticDefaults - use base values exactly as designed
  },
  // ============================================================================
  // Synthwave Presets (80s inspired)
  // ============================================================================
  "synthwave_bass": {
    name: "Synthwave Bass",
    category: "bass",
    description: "Punchy 80s FM bass for driving synthwave tracks",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.3, sustain: 0.5, release: 0.2 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.1, sustain: 0.2, release: 0.1 }
    },
    semanticDefaults: { punch: 0.8, brightness: 0.6 },
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 2, max: 12 }
    }
  },
  "synthwave_lead": {
    name: "Synthwave Lead",
    category: "lead",
    description: "Bright FM lead for soaring 80s melodies",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 8,
      oscillator: { type: "sine" },
      envelope: { attack: 0.02, decay: 0.3, sustain: 0.6, release: 0.4 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.02, decay: 0.2, sustain: 0.3, release: 0.3 }
    },
    // No semanticDefaults - use base values exactly
    semanticMappings: {
      brightness: { param: "modulationIndex", min: 4, max: 16 }
    }
  },
  "synthwave_stab": {
    name: "Synthwave Stab",
    category: "fm",
    description: "Short punchy FM stab for accents and hits",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 15,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.2, sustain: 0.1, release: 0.15 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.15, sustain: 0, release: 0.1 }
    }
    // No semanticDefaults - use base values exactly
  },
  "synthwave_pad": {
    name: "Synthwave Pad",
    category: "pad",
    description: "Lush Juno-style sawtooth pad with slow attack",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 1.5, decay: 0.5, sustain: 0.8, release: 2 }
    },
    semanticDefaults: { attack: 0.7, sustain: 0.8, release: 0.7, warmth: 0.7 }
  },
  "arp_synth": {
    name: "Arpeggio Synth",
    category: "synth",
    description: "Bright, punchy sawtooth for arpeggios - short and clear",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 5e-3, decay: 0.2, sustain: 0.2, release: 0.15 }
    },
    semanticDefaults: { attack: 0.01, decay: 0.2, sustain: 0.2, release: 0.1, brightness: 0.7 }
  },
  // ============================================================================
  // Drum Presets (Individual drum sounds)
  // ============================================================================
  "kick_deep": {
    name: "Deep Kick",
    category: "drums",
    description: "Deep booming kick drum with slow pitch decay",
    type: "membrane",
    base: {
      pitchDecay: 0.08,
      octaves: 4,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.5, sustain: 0, release: 0.1 },
      pitch: "C2"
    }
  },
  "kick_909": {
    name: "909 Kick",
    category: "drums",
    description: "Classic TR-909 style kick drum",
    type: "membrane",
    base: {
      pitchDecay: 0.05,
      octaves: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.4, sustain: 0, release: 0.1 },
      pitch: "C2"
    }
  },
  "hihat_closed": {
    name: "Closed Hi-Hat",
    category: "drums",
    description: "Short, tight closed hi-hat",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.06, sustain: 0, release: 0.01 }
    }
  },
  "hihat_open": {
    name: "Open Hi-Hat",
    category: "drums",
    description: "Longer, ringing open hi-hat",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.2, sustain: 0.05, release: 0.1 }
    }
  },
  "clap_909": {
    name: "909 Clap",
    category: "drums",
    description: "Classic TR-909 style clap",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.15, sustain: 0, release: 0.05 }
    }
  },
  "snare_house": {
    name: "House Snare",
    category: "drums",
    description: "Pink noise snare for house and electronic music",
    type: "noise",
    base: {
      noise: { type: "pink" },
      envelope: { attack: 1e-3, decay: 0.2, sustain: 0, release: 0.05 }
    }
  },
  // ============================================================================
  // NEW v0.8: Lo-fi Presets
  // ============================================================================
  "lofi_keys": {
    name: "Lo-fi Keys",
    category: "lofi",
    description: "Dusty, warm keys with tape-like character",
    type: "fmsynth",
    base: {
      harmonicity: 1.005,
      // Slight detune for warmth
      modulationIndex: 2.5,
      oscillator: { type: "sine" },
      envelope: { attack: 0.01, decay: 1.2, sustain: 0.1, release: 0.8 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.01, decay: 0.3, sustain: 0.05, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.3, warmth: 0.9, punch: 0.4 }
  },
  "lofi_pad": {
    name: "Lo-fi Pad",
    category: "lofi",
    description: "Tape-saturated pad with gentle warmth",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.8, decay: 0.5, sustain: 0.6, release: 1.5 }
    },
    semanticDefaults: { brightness: 0.25, warmth: 0.95, attack: 0.6 }
  },
  "vinyl_texture": {
    name: "Vinyl Texture",
    category: "lofi",
    description: "Subtle crackle and warmth for atmosphere",
    type: "noise",
    base: {
      noise: { type: "brown" },
      envelope: { attack: 0.5, decay: 0.5, sustain: 0.3, release: 1 }
    },
    semanticDefaults: { brightness: 0.1, warmth: 0.8 }
  },
  "dusty_piano": {
    name: "Dusty Piano",
    category: "lofi",
    description: "Worn piano with vintage character",
    type: "fmsynth",
    base: {
      harmonicity: 1.01,
      // Slightly detuned
      modulationIndex: 3,
      oscillator: { type: "sine" },
      envelope: { attack: 5e-3, decay: 1.8, sustain: 0.05, release: 1 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 5e-3, decay: 0.25, sustain: 0.02, release: 0.15 }
    },
    semanticDefaults: { brightness: 0.35, warmth: 0.85, decay: 0.7 }
  },
  // ============================================================================
  // NEW v0.8: Cinematic Presets
  // ============================================================================
  "cinematic_brass": {
    name: "Cinematic Brass",
    category: "cinematic",
    description: "Massive orchestral brass for epic moments",
    type: "fmsynth",
    base: {
      harmonicity: 1,
      modulationIndex: 18,
      oscillator: { type: "sine" },
      envelope: { attack: 0.15, decay: 0.3, sustain: 0.8, release: 0.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.2, decay: 0.4, sustain: 0.7, release: 0.4 }
    },
    semanticDefaults: { punch: 0.8, brightness: 0.75, attack: 0.3 }
  },
  "tension_strings": {
    name: "Tension Strings",
    category: "cinematic",
    description: "Dark, suspenseful string texture",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.6, decay: 0.4, sustain: 0.7, release: 1.8 }
    },
    semanticDefaults: { brightness: 0.3, warmth: 0.4, attack: 0.5 }
  },
  "impact_hit": {
    name: "Impact Hit",
    category: "cinematic",
    description: "Deep cinematic impact for transitions",
    type: "membrane",
    base: {
      pitchDecay: 0.15,
      octaves: 8,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0, release: 2 },
      pitch: "C1"
    },
    semanticDefaults: { punch: 1, decay: 0.8 }
  },
  "epic_pad": {
    name: "Epic Pad",
    category: "cinematic",
    description: "Huge evolving pad for emotional moments",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 1.5, decay: 0.8, sustain: 0.9, release: 3 }
    },
    semanticDefaults: { brightness: 0.6, warmth: 0.7, attack: 0.8, release: 0.9 }
  },
  "riser": {
    name: "Riser",
    category: "cinematic",
    description: "Building tension riser sound",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 4, decay: 0.5, sustain: 0.8, release: 0.5 }
    },
    semanticDefaults: { brightness: 0.7, attack: 0.95 }
  },
  // ============================================================================
  // NEW v0.8: World Music Presets
  // ============================================================================
  "kalimba": {
    name: "Kalimba",
    category: "world",
    description: "African thumb piano with metallic tines",
    type: "fmsynth",
    base: {
      harmonicity: 5.5,
      modulationIndex: 4,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.2, sustain: 0.05, release: 0.8 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.4, sustain: 0, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.6, warmth: 0.5, decay: 0.5 }
  },
  "sitar_lead": {
    name: "Sitar Lead",
    category: "world",
    description: "Sitar-inspired lead with characteristic buzz",
    type: "fmsynth",
    base: {
      harmonicity: 3,
      modulationIndex: 7,
      oscillator: { type: "sine" },
      envelope: { attack: 0.02, decay: 0.5, sustain: 0.4, release: 0.6 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 0.01, decay: 0.3, sustain: 0.5, release: 0.4 }
    },
    semanticDefaults: { brightness: 0.7, warmth: 0.6 }
  },
  "steel_drum": {
    name: "Steel Drum",
    category: "world",
    description: "Caribbean steel pan with bright overtones",
    type: "fmsynth",
    base: {
      harmonicity: 4.5,
      modulationIndex: 5,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1, sustain: 0.1, release: 0.6 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.3, sustain: 0.05, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.75, warmth: 0.5, decay: 0.5 }
  },
  "koto": {
    name: "Koto",
    category: "world",
    description: "Japanese stringed instrument with delicate attack",
    type: "fmsynth",
    base: {
      harmonicity: 6,
      modulationIndex: 2.5,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 1.5, sustain: 0.02, release: 0.8 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1e-3, decay: 0.2, sustain: 0, release: 0.1 }
    },
    semanticDefaults: { brightness: 0.55, warmth: 0.4, decay: 0.6 }
  },
  // ============================================================================
  // NEW v0.8: Ambient Presets
  // ============================================================================
  "granular_pad": {
    name: "Granular Pad",
    category: "ambient",
    description: "Textured evolving pad with cloud-like quality",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 2, decay: 1, sustain: 0.95, release: 4 }
    },
    semanticDefaults: { brightness: 0.35, warmth: 0.8, attack: 0.9, release: 0.95 }
  },
  "drone": {
    name: "Drone",
    category: "ambient",
    description: "Deep sustained drone for atmospheric beds",
    type: "polysynth",
    base: {
      oscillator: { type: "sine" },
      envelope: { attack: 3, decay: 1, sustain: 1, release: 5 }
    },
    semanticDefaults: { brightness: 0.15, warmth: 0.9, attack: 0.95, sustain: 1, release: 1 }
  },
  "shimmer": {
    name: "Shimmer",
    category: "ambient",
    description: "Ethereal shimmering texture with high harmonics",
    type: "fmsynth",
    base: {
      harmonicity: 7.5,
      modulationIndex: 2,
      oscillator: { type: "sine" },
      envelope: { attack: 1.5, decay: 0.8, sustain: 0.7, release: 3 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 1, decay: 0.5, sustain: 0.4, release: 2 }
    },
    semanticDefaults: { brightness: 0.8, warmth: 0.5, attack: 0.7 }
  },
  "atmosphere": {
    name: "Atmosphere",
    category: "ambient",
    description: "Breathy, wind-like atmospheric texture",
    type: "noise",
    base: {
      noise: { type: "pink" },
      envelope: { attack: 2, decay: 1, sustain: 0.6, release: 3 }
    },
    semanticDefaults: { brightness: 0.4, warmth: 0.6, attack: 0.8 }
  },
  "space_pad": {
    name: "Space Pad",
    category: "ambient",
    description: "Vast, cosmic pad with deep reverb character",
    type: "fmsynth",
    base: {
      harmonicity: 2,
      modulationIndex: 1.5,
      oscillator: { type: "sine" },
      envelope: { attack: 2.5, decay: 1.5, sustain: 0.85, release: 4.5 },
      modulation: { type: "sine" },
      modulationEnvelope: { attack: 2, decay: 1, sustain: 0.6, release: 3 }
    },
    semanticDefaults: { brightness: 0.4, warmth: 0.7, attack: 0.85, release: 0.9 }
  },
  // ============================================================================
  // NEW v0.8: Modern/Trap Presets
  // ============================================================================
  "808_bass": {
    name: "808 Bass",
    category: "modern",
    description: "Classic 808 bass with long sustain",
    type: "membrane",
    base: {
      pitchDecay: 0.08,
      octaves: 6,
      oscillator: { type: "sine" },
      envelope: { attack: 1e-3, decay: 0.8, sustain: 0.3, release: 0.4 },
      pitch: "C2"
    },
    semanticDefaults: { punch: 0.9, decay: 0.6, sustain: 0.3 }
  },
  "trap_hihat": {
    name: "Trap Hi-Hat",
    category: "modern",
    description: "Crisp trap hi-hat with tight envelope",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.04, sustain: 0, release: 0.02 }
    },
    semanticDefaults: { brightness: 0.9, decay: 0.1 }
  },
  "future_bass_lead": {
    name: "Future Bass Lead",
    category: "modern",
    description: "Supersawed lead for future bass drops",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.15, sustain: 0.5, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.85, richness: 0.9, punch: 0.7 }
  },
  "wobble_bass": {
    name: "Wobble Bass",
    category: "modern",
    description: "Dubstep/EDM wobble bass foundation",
    type: "monosynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.8, release: 0.2 },
      filterEnvelope: {
        attack: 0.01,
        decay: 0.2,
        sustain: 0.5,
        release: 0.2,
        baseFrequency: 100,
        octaves: 4
      }
    },
    semanticDefaults: { brightness: 0.7, punch: 0.8, warmth: 0.5 }
  },
  "pluck_lead": {
    name: "Pluck Lead",
    category: "modern",
    description: "Bright pluck for modern pop melodies",
    type: "polysynth",
    base: {
      oscillator: { type: "triangle" },
      envelope: { attack: 1e-3, decay: 0.3, sustain: 0.1, release: 0.2 }
    },
    semanticDefaults: { brightness: 0.7, punch: 0.6, decay: 0.3 }
  },
  "supersaw": {
    name: "Supersaw",
    category: "modern",
    description: "Classic supersaw for trance and EDM leads",
    type: "polysynth",
    base: {
      oscillator: { type: "sawtooth" },
      envelope: { attack: 0.02, decay: 0.2, sustain: 0.6, release: 0.4 }
    },
    semanticDefaults: { brightness: 0.9, richness: 1, warmth: 0.4 }
  },
  "chiptune": {
    name: "Chiptune",
    category: "modern",
    description: "Retro 8-bit square wave for chiptune style",
    type: "polysynth",
    base: {
      oscillator: { type: "square" },
      envelope: { attack: 1e-3, decay: 0.1, sustain: 0.4, release: 0.1 }
    },
    semanticDefaults: { brightness: 0.8, punch: 0.5 }
  },
  // ============================================================================
  // v0.81: Noise Presets (for textures, ambience, lo-fi effects)
  // ============================================================================
  "noise": {
    name: "White Noise",
    category: "texture",
    description: "White noise for texture, risers, and ambient effects",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.5, release: 0.3 }
    },
    semanticDefaults: { brightness: 1 }
  },
  "pink_noise": {
    name: "Pink Noise",
    category: "texture",
    description: "Pink noise (1/f) - warmer, more natural sounding",
    type: "noise",
    base: {
      noise: { type: "pink" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.5, release: 0.3 }
    },
    semanticDefaults: { brightness: 0.7, warmth: 0.6 }
  },
  "brown_noise": {
    name: "Brown Noise",
    category: "texture",
    description: "Brown noise (1/f\xB2) - deepest, smoothest noise",
    type: "noise",
    base: {
      noise: { type: "brown" },
      envelope: { attack: 0.01, decay: 0.1, sustain: 0.5, release: 0.3 }
    },
    semanticDefaults: { brightness: 0.3, warmth: 0.9 }
  },
  "vinyl_crackle": {
    name: "Vinyl Crackle",
    category: "lofi",
    description: "Lo-fi vinyl crackle texture with short decay",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 1e-3, decay: 0.02, sustain: 0, release: 0.01 }
    },
    semanticDefaults: { brightness: 0.9 }
  },
  "noise_sweep": {
    name: "Noise Sweep",
    category: "texture",
    description: "White noise with longer attack for sweeps and risers",
    type: "noise",
    base: {
      noise: { type: "white" },
      envelope: { attack: 0.5, decay: 0.3, sustain: 0.3, release: 0.5 }
    },
    semanticDefaults: { brightness: 1 }
  }
};
function getPresetDefinition(name) {
  const normalizedName = name.toLowerCase();
  if (PRESET_DEFINITIONS[normalizedName]) {
    return PRESET_DEFINITIONS[normalizedName];
  }
  const registryPreset = PRESET_REGISTRY[normalizedName];
  if (registryPreset) {
    return registryPreset;
  }
  return void 0;
}

// src/synthesis/semantic-params.ts
var RANGES = {
  // Envelope times in seconds
  attack: { min: 1e-3, max: 2 },
  decay: { min: 0.05, max: 4 },
  release: { min: 0.1, max: 4 },
  // FM parameters
  harmonicity: { min: 0.5, max: 8 },
  modulationIndex: { min: 0, max: 20 },
  // Filter
  filterFrequency: { min: 100, max: 8e3 },
  filterOctaves: { min: 0.5, max: 4 },
  // Punch affects attack time inversely
  punchAttack: { bright: 1e-3, soft: 0.1 }
};
function lerp(value, min, max) {
  return min + (max - min) * Math.max(0, Math.min(1, value));
}
function semanticToEnvelope(params) {
  return {
    attack: lerp(params.attack ?? 0.1, RANGES.attack.min, RANGES.attack.max),
    decay: lerp(params.decay ?? 0.3, RANGES.decay.min, RANGES.decay.max),
    sustain: params.sustain ?? 0.5,
    release: lerp(params.release ?? 0.3, RANGES.release.min, RANGES.release.max)
  };
}
function applyPunch(envelope, punch) {
  if (punch !== void 0) {
    const punchFactor = 1 - punch;
    envelope.attack = Math.max(1e-3, envelope.attack * punchFactor);
    envelope.decay = Math.max(0.05, envelope.decay * (0.7 + 0.3 * punchFactor));
  }
}
function warmthToHarmonicity(warmth, baseHarmonicity) {
  const adjustment = lerp(warmth, 1.5, 0.7);
  return baseHarmonicity * adjustment;
}

// src/synthesis/instrument-factory.ts
function lerp2(value, min, max) {
  return min + (max - min) * Math.max(0, Math.min(1, value));
}
function applySemanticParams(base, semanticParams, mappings) {
  const result = JSON.parse(JSON.stringify(base));
  if (result.envelope) {
    const envelope = semanticToEnvelope(semanticParams);
    if (semanticParams.attack !== void 0) {
      result.envelope.attack = envelope.attack;
    }
    if (semanticParams.decay !== void 0) {
      result.envelope.decay = envelope.decay;
    }
    if (semanticParams.sustain !== void 0) {
      result.envelope.sustain = semanticParams.sustain;
    }
    if (semanticParams.release !== void 0) {
      result.envelope.release = envelope.release;
    }
    if (semanticParams.punch !== void 0) {
      applyPunch(result.envelope, semanticParams.punch);
    }
  }
  if (result.modulationIndex !== void 0 && semanticParams.brightness !== void 0) {
    if (mappings?.brightness) {
      result.modulationIndex = lerp2(
        semanticParams.brightness,
        mappings.brightness.min,
        mappings.brightness.max
      );
    } else {
      result.modulationIndex = result.modulationIndex * (0.3 + semanticParams.brightness * 1.4);
    }
  }
  if (result.harmonicity !== void 0 && semanticParams.warmth !== void 0) {
    if (mappings?.warmth) {
      result.harmonicity = lerp2(
        semanticParams.warmth,
        mappings.warmth.max,
        // Inverted: higher warmth = lower harmonicity
        mappings.warmth.min
      );
    } else {
      result.harmonicity = warmthToHarmonicity(semanticParams.warmth, result.harmonicity);
    }
  }
  if (result.filterEnvelope && semanticParams.brightness !== void 0) {
    const brightnessScale = 0.5 + semanticParams.brightness;
    result.filterEnvelope.baseFrequency = result.filterEnvelope.baseFrequency * brightnessScale;
    result.filterEnvelope.octaves = result.filterEnvelope.octaves * (0.5 + semanticParams.brightness * 0.8);
  }
  return result;
}
function applyOverrides(options, overrides) {
  const result = { ...options };
  if (overrides.oscillator?.type) {
    result.oscillator = {
      type: overrides.oscillator.type
    };
  }
  if (overrides.envelope && result.envelope) {
    result.envelope = {
      attack: overrides.envelope.attack ?? result.envelope.attack,
      decay: overrides.envelope.decay ?? result.envelope.decay,
      sustain: overrides.envelope.sustain ?? result.envelope.sustain,
      release: overrides.envelope.release ?? result.envelope.release
    };
  }
  if (overrides.filterEnvelope && result.filterEnvelope) {
    result.filterEnvelope = {
      attack: overrides.filterEnvelope.attack ?? result.filterEnvelope.attack,
      decay: overrides.filterEnvelope.decay ?? result.filterEnvelope.decay,
      sustain: overrides.filterEnvelope.sustain ?? result.filterEnvelope.sustain,
      release: overrides.filterEnvelope.release ?? result.filterEnvelope.release,
      baseFrequency: overrides.filterEnvelope.baseFrequency ?? result.filterEnvelope.baseFrequency,
      octaves: overrides.filterEnvelope.octaves ?? result.filterEnvelope.octaves
    };
  }
  if (overrides.harmonicity !== void 0) {
    result.harmonicity = overrides.harmonicity;
  }
  if (overrides.modulationIndex !== void 0) {
    result.modulationIndex = overrides.modulationIndex;
  }
  if (overrides.modulationEnvelope && result.modulationEnvelope) {
    result.modulationEnvelope = {
      attack: overrides.modulationEnvelope.attack ?? result.modulationEnvelope.attack,
      decay: overrides.modulationEnvelope.decay ?? result.modulationEnvelope.decay,
      sustain: overrides.modulationEnvelope.sustain ?? result.modulationEnvelope.sustain,
      release: overrides.modulationEnvelope.release ?? result.modulationEnvelope.release
    };
  }
  return result;
}
function createPolySynth(options) {
  return new PolySynth(Synth, {
    oscillator: options.oscillator ?? { type: "triangle" },
    envelope: options.envelope ?? { attack: 0.02, decay: 0.1, sustain: 0.3, release: 0.8 }
  });
}
function createMonoSynth(options) {
  const monoOptions = {};
  if (options.oscillator) {
    monoOptions.oscillator = options.oscillator;
  }
  if (options.envelope) {
    monoOptions.envelope = options.envelope;
  }
  if (options.filterEnvelope) {
    monoOptions.filterEnvelope = options.filterEnvelope;
  }
  return new PolySynth(MonoSynth, monoOptions);
}
function createFMSynth(options) {
  const fmOptions = {
    harmonicity: options.harmonicity ?? 2,
    modulationIndex: options.modulationIndex ?? 4,
    oscillator: options.oscillator ?? { type: "sine" },
    envelope: options.envelope ?? { attack: 0.01, decay: 0.3, sustain: 0.3, release: 0.5 },
    modulation: options.modulation ?? { type: "sine" },
    modulationEnvelope: options.modulationEnvelope ?? { attack: 0.01, decay: 0.2, sustain: 0.3, release: 0.3 }
  };
  return new PolySynth(FMSynth, fmOptions);
}
function createMembraneSynth(options) {
  return new MembraneSynth({
    pitchDecay: options.pitchDecay ?? 0.05,
    octaves: options.octaves ?? 4,
    oscillator: options.oscillator ?? { type: "sine" },
    envelope: options.envelope ?? { attack: 1e-3, decay: 0.4, sustain: 0, release: 0.1 }
  });
}
function createNoiseSynth(options) {
  return new NoiseSynth({
    noise: options.noise ?? { type: "white" },
    envelope: options.envelope ?? { attack: 1e-3, decay: 0.1, sustain: 0, release: 0.05 }
  });
}
var SAMPLE_NOTE_MAPPING = {
  // Piano - comprehensive chromatic sampling
  piano: ["A1", "C2", "E2", "A2", "C3", "E3", "A3", "C4", "E4", "A4", "C5", "E5", "A5", "C6", "E6", "A6", "C7"],
  // Violin - starts at A3 (actual available samples)
  violin: ["A3", "A4", "A5", "A6", "C4", "C5", "C6", "C7", "E4", "E5", "E6", "G4", "G5", "G6"],
  // Cello - C2 to C5 (available samples)
  cello: ["C2", "E2", "A2", "C3", "E3", "A3", "C4", "E4", "A4", "C5"],
  // Contrabass - bass range (actual available: C2, Cs3, D2, E2, E3, Fs1, Fs2, G1, Gs2, Gs3, A2, As1, B3)
  contrabass: ["C2", "D2", "E2", "A2", "E3"],
  // Trumpet (actual: C4, C6, D5, Ds4, F3, F4, F5, G4, A3, A5, As4)
  trumpet: ["A3", "C4", "F3", "F4", "G4", "D5", "F5", "A5", "C6"],
  // Trombone
  trombone: ["As2", "C3", "F3", "As3", "C4", "F4", "As4"],
  // French horn
  "french-horn": ["A1", "D2", "D3", "A3", "D4", "F4", "A4", "C5", "D5"],
  // Tuba - low brass
  tuba: ["As0", "D1", "As1", "D2", "F2", "As2", "D3"],
  // Flute
  flute: ["A4", "C4", "C5", "C6", "E4", "E5", "A5", "E6"],
  // Clarinet
  clarinet: ["As3", "D3", "D4", "D5", "F3", "F4", "F5", "As4", "As5"],
  // Bassoon
  bassoon: ["A1", "A2", "A3", "C2", "C3", "C4", "E2", "E3", "G2", "G3", "G4"],
  // Saxophone
  saxophone: ["As3", "Cs3", "Cs4", "Cs5", "E3", "E4", "E5", "G3", "G4", "G5"],
  // Acoustic guitar
  "guitar-acoustic": ["A2", "A3", "A4", "C3", "C4", "C5", "E2", "E3", "E4", "F3", "F4"],
  // Electric guitar
  "guitar-electric": ["A2", "A3", "A4", "C3", "C4", "C5", "E2", "E3", "E4", "G3", "G4"],
  // Nylon guitar
  "guitar-nylon": ["A2", "A3", "A4", "C3", "C4", "C5", "E2", "E3", "E4", "F3", "F4"],
  // Electric bass
  "bass-electric": ["As1", "As2", "Cs2", "Cs3", "E1", "E2", "G1", "G2", "Gs2"],
  // Harp
  harp: ["A2", "A4", "A6", "B1", "B3", "B5", "C3", "C5", "D2", "D4", "D6", "E3", "E5", "F2", "F4", "F6", "G3", "G5"],
  // Xylophone - upper register
  xylophone: ["C5", "C6", "C7", "C8", "E5", "E6", "E7", "G4", "G5", "G6", "G7"],
  // Organ
  organ: ["A2", "A3", "A4", "A5", "C2", "C3", "C4", "C5", "C6", "E2", "E3", "E4", "E5", "G2", "G3", "G4", "G5"],
  // Harmonium
  harmonium: ["A3", "A4", "A5", "C3", "C4", "C5", "C6", "E3", "E4", "E5", "G3", "G4", "G5"]
};
function getSampleUrls(instrument) {
  const notes = SAMPLE_NOTE_MAPPING[instrument] || SAMPLE_NOTE_MAPPING.piano;
  const urls = {};
  for (const note of notes) {
    const toneNote = note.replace(/([A-G])s(\d)/, "$1#$2");
    urls[toneNote] = `${note}.mp3`;
  }
  return urls;
}
function createSampler(options) {
  const instrument = options.instrument || "piano";
  const baseUrl = options.baseUrl || TONEJS_SAMPLES_CDN;
  const urls = getSampleUrls(instrument);
  const sampler = new Sampler({
    urls,
    baseUrl: `${baseUrl}${instrument}/`,
    release: 1,
    onload: () => {
      console.log(`Sampler loaded: ${instrument}`);
    },
    onerror: (error) => {
      console.warn(`Failed to load samples for ${instrument}:`, error.message);
    }
  });
  return sampler;
}
function createSamplerAsync(options) {
  const instrument = options.instrument || "piano";
  const baseUrl = options.baseUrl || TONEJS_SAMPLES_CDN;
  const urls = getSampleUrls(instrument);
  return new Promise((resolve, reject) => {
    const sampler = new Sampler({
      urls,
      baseUrl: `${baseUrl}${instrument}/`,
      release: 1,
      onload: () => {
        console.log(`Sampler loaded: ${instrument}`);
        resolve(sampler);
      },
      onerror: (error) => {
        console.warn(`Failed to load samples for ${instrument}:`, error.message);
        reject(error);
      }
    });
  });
}
function createInstrumentFromOptions(options) {
  let synthOptions;
  let synthType;
  let semanticMappings;
  if (options.preset) {
    const preset = getPresetDefinition(options.preset);
    if (!preset) {
      console.warn(`Unknown preset: ${options.preset}, using 'synth'`);
      return createInstrumentFromOptions({ preset: "synth" });
    }
    synthOptions = { ...preset.base };
    synthType = preset.type;
    semanticMappings = preset.semanticMappings;
    if (options.params && Object.keys(options.params).length > 0) {
      synthOptions = applySemanticParams(synthOptions, options.params, semanticMappings);
    }
  } else {
    synthType = options.type || "polysynth";
    synthOptions = {
      oscillator: { type: "triangle" },
      envelope: { attack: 0.02, decay: 0.1, sustain: 0.3, release: 0.8 }
    };
    if (options.params) {
      synthOptions = applySemanticParams(synthOptions, options.params);
    }
  }
  if (options.overrides) {
    synthOptions = applyOverrides(synthOptions, options.overrides);
  }
  switch (synthType) {
    case "fmsynth":
      return createFMSynth(synthOptions);
    case "monosynth":
      return createMonoSynth(synthOptions);
    case "membrane":
      return createMembraneSynth(synthOptions);
    case "noise":
      return createNoiseSynth(synthOptions);
    case "sampler":
      return createSampler(synthOptions);
    case "synth":
    case "polysynth":
    default:
      return createPolySynth(synthOptions);
  }
}

// src/browser/player.ts
var DRUM_POOL_SIZE = 4;
function createInstrument(def) {
  const presetName = def?.preset || "synth";
  return createInstrumentFromOptions({
    preset: presetName,
    type: def?.type,
    params: def?.params,
    overrides: def?.overrides
  });
}
function createDrumSynth(params) {
  const envelope = {
    attack: params.attack ?? 1e-3,
    decay: params.decay ?? 0.4,
    sustain: params.sustain ?? 0,
    release: params.release ?? 0.1
  };
  switch (params.type) {
    case "membrane":
      return new MembraneSynth({
        pitchDecay: params.pitchDecay ?? 0.05,
        octaves: params.octaves ?? 4,
        oscillator: { type: "sine" },
        envelope
      });
    case "noise":
      return new NoiseSynth({
        noise: { type: params.noiseType ?? "white" },
        envelope
      });
    case "metal": {
      const metalSynth = new MetalSynth({
        harmonicity: params.harmonicity ?? 5.1,
        modulationIndex: params.modulationIndex ?? 32,
        resonance: params.resonance ?? 4e3,
        envelope
      });
      if (params.frequency) {
        metalSynth.frequency.value = params.frequency;
      }
      return metalSynth;
    }
    default:
      return new MembraneSynth({ oscillator: { type: "sine" }, envelope });
  }
}
function createEffect(def) {
  const wet = def.wet ?? 0.5;
  const options = def.options || {};
  switch (def.type) {
    case "reverb":
      return new Reverb({
        decay: options.decay ?? EFFECT_DEFAULTS.reverb.decay,
        wet
      });
    case "delay":
      return new FeedbackDelay({
        delayTime: options.time ?? EFFECT_DEFAULTS.delay.time,
        feedback: options.feedback ?? EFFECT_DEFAULTS.delay.feedback,
        wet
      });
    case "chorus": {
      const chorus = new Chorus({
        frequency: options.frequency ?? EFFECT_DEFAULTS.chorus.frequency,
        delayTime: options.delayTime ?? EFFECT_DEFAULTS.chorus.delayTime,
        depth: options.depth ?? EFFECT_DEFAULTS.chorus.depth,
        wet
      });
      chorus.start();
      return chorus;
    }
    case "distortion":
      return new Distortion({
        distortion: options.amount ?? EFFECT_DEFAULTS.distortion.distortion,
        wet
      });
    case "filter":
      return new Filter({
        frequency: options.frequency ?? EFFECT_DEFAULTS.filter.frequency,
        type: options.type ?? EFFECT_DEFAULTS.filter.type,
        Q: options.Q ?? EFFECT_DEFAULTS.filter.Q
      });
    case "compressor":
      return new Compressor({
        threshold: options.threshold ?? EFFECT_DEFAULTS.compressor.threshold,
        ratio: options.ratio ?? EFFECT_DEFAULTS.compressor.ratio,
        attack: options.attack ?? EFFECT_DEFAULTS.compressor.attack,
        release: options.release ?? EFFECT_DEFAULTS.compressor.release
      });
    case "eq":
      return new EQ3({
        low: options.low ?? EFFECT_DEFAULTS.eq.low,
        mid: options.mid ?? EFFECT_DEFAULTS.eq.mid,
        high: options.high ?? EFFECT_DEFAULTS.eq.high
      });
    case "phaser": {
      const phaser = new Phaser({
        frequency: options.frequency ?? EFFECT_DEFAULTS.phaser.frequency,
        octaves: options.octaves ?? EFFECT_DEFAULTS.phaser.octaves,
        baseFrequency: options.baseFrequency ?? EFFECT_DEFAULTS.phaser.baseFrequency,
        wet
      });
      return phaser;
    }
    case "vibrato": {
      const vibrato = new Vibrato({
        frequency: options.frequency ?? EFFECT_DEFAULTS.vibrato.frequency,
        depth: options.depth ?? EFFECT_DEFAULTS.vibrato.depth,
        wet
      });
      return vibrato;
    }
    case "bitcrusher": {
      const crusher = new BitCrusher(options.bits ?? EFFECT_DEFAULTS.bitcrusher.bits);
      crusher.wet.value = wet;
      return crusher;
    }
    default:
      console.warn(`Unknown effect type: ${def.type}`);
      return null;
  }
}
function parseDrumPitch(pitch) {
  if (!pitch.startsWith("drum:")) return null;
  const match = pitch.match(/^drum:([^@]+)@(.+)$/);
  if (!match) return null;
  return {
    drumName: match[1],
    kitName: match[2]
  };
}
var Player2 = class {
  score = null;
  timeline = null;
  instruments = /* @__PURE__ */ new Map();
  drumPools = /* @__PURE__ */ new Map();
  scheduledEvents = [];
  state = "stopped";
  callbacks = {};
  progressInterval = null;
  sections = [];
  currentSectionIndex = -1;
  analyser = null;
  // Master audio chain for protection and volume control
  masterVolume;
  masterLimiter;
  masterCompressor;
  constructor() {
    this.masterCompressor = new Compressor({
      threshold: -12,
      // Start compressing at -12dB
      ratio: 4,
      // 4:1 compression ratio
      attack: 3e-3,
      // Fast attack to catch transients
      release: 0.25,
      // Moderate release
      knee: 6
      // Soft knee for smoother compression
    });
    this.masterLimiter = new Limiter(-1);
    this.masterVolume = new Volume(0);
    this.masterCompressor.connect(this.masterLimiter);
    this.masterLimiter.connect(this.masterVolume);
    this.masterVolume.toDestination();
  }
  /**
   * Set callbacks for player events
   */
  setCallbacks(callbacks) {
    this.callbacks = callbacks;
  }
  /**
   * Get the audio analyser for visualization
   */
  getAnalyser() {
    if (!this.analyser) {
      this.analyser = new Analyser("waveform", 256);
      this.masterVolume.connect(this.analyser);
    }
    return this.analyser;
  }
  /**
   * Set master volume in decibels (-60 to +6 dB range)
   * @param db Volume in decibels. 0 = unity gain, -Infinity = mute
   */
  setVolume(db) {
    const clampedDb = Math.max(-60, Math.min(6, db));
    this.masterVolume.volume.value = clampedDb;
  }
  /**
   * Get master volume in decibels
   */
  getVolume() {
    return this.masterVolume.volume.value;
  }
  /**
   * Set volume as a linear value (0-1 for convenience)
   * @param value Linear volume 0-1 (can go above 1 for boost)
   */
  setVolumeLinear(value) {
    const db = 20 * Math.log10(Math.max(1e-4, value));
    this.setVolume(db);
  }
  /**
   * Get volume as linear value (0-1)
   */
  getVolumeLinear() {
    const db = this.masterVolume.volume.value;
    return Math.pow(10, db / 20);
  }
  /**
   * Mute/unmute the output
   */
  setMuted(muted) {
    this.masterVolume.mute = muted;
  }
  /**
   * Check if muted
   */
  isMuted() {
    return this.masterVolume.mute;
  }
  /**
   * Load an EtherScore composition
   */
  async load(score) {
    this.stop();
    this.dispose();
    const cleanedScore = stripComments(score);
    this.score = cleanedScore;
    const result = compile(cleanedScore);
    this.timeline = result.timeline;
    if (result.warnings.length > 0) {
      console.warn("Compilation warnings:", result.warnings);
    }
    this.buildSectionInfo(cleanedScore);
    this.initializeInstruments(cleanedScore.instruments, this.timeline.instruments);
  }
  /**
   * Build section info for tracking current section during playback
   */
  buildSectionInfo(score) {
    this.sections = [];
    const tempo = score.settings.tempo;
    const beatsPerBar = 4;
    let currentTime = 0;
    for (const sectionName of score.arrangement) {
      const section = score.sections[sectionName];
      if (!section) continue;
      const sectionTempo = section.tempo ?? tempo;
      const sectionBeats = section.bars * beatsPerBar;
      const sectionDuration = sectionBeats / sectionTempo * 60;
      this.sections.push({
        name: sectionName,
        startTime: currentTime,
        endTime: currentTime + sectionDuration
      });
      currentTime += sectionDuration;
    }
  }
  /**
   * Initialize instruments from definitions
   */
  initializeInstruments(instrumentDefs, usedInstruments) {
    for (const name of usedInstruments) {
      const def = instrumentDefs?.[name];
      const presetName = def?.preset || "synth";
      if (presetName.startsWith("drums:")) {
        continue;
      }
      const synth = createInstrument(def);
      const presetDef = getPresetDefinition(presetName);
      const synthType = presetDef?.type || "polysynth";
      const defaultPitch = presetDef?.base?.pitch;
      const effects = [];
      if (def?.effects) {
        for (const effectDef of def.effects) {
          const effect = createEffect(effectDef);
          if (effect) {
            effects.push(effect);
          }
        }
      }
      const channel = new Channel({
        volume: def?.volume ?? 0,
        pan: def?.pan ?? 0
      }).connect(this.masterCompressor);
      if (effects.length > 0) {
        synth.connect(effects[0]);
        for (let i = 0; i < effects.length - 1; i++) {
          effects[i].connect(effects[i + 1]);
        }
        effects[effects.length - 1].connect(channel);
      } else {
        synth.connect(channel);
      }
      this.instruments.set(name, { synth, effects, channel, synthType, defaultPitch });
    }
  }
  /**
   * Get or create a drum synth pool for handling simultaneous hits
   */
  getOrCreateDrumPool(drumName, kitName) {
    const normalizedDrumName = normalizeDrumName(drumName);
    const key = `${normalizedDrumName}@${kitName}`;
    if (this.drumPools.has(key)) {
      return this.drumPools.get(key);
    }
    const kit = DRUM_KITS[kitName];
    if (!kit) {
      console.warn(`Unknown drum kit: ${kitName}`);
      return null;
    }
    const params = kit.drums[normalizedDrumName];
    if (!params) {
      console.warn(`Unknown drum: ${drumName} (normalized: ${normalizedDrumName}) in kit ${kitName}`);
      return null;
    }
    const synths = [];
    for (let i = 0; i < DRUM_POOL_SIZE; i++) {
      const synth = createDrumSynth(params);
      const volume = new Volume(params.volume ?? 0);
      synth.connect(volume);
      volume.connect(this.masterCompressor);
      synths.push({
        synth,
        volume,
        type: params.type,
        pitch: params.pitch
      });
    }
    const pool = {
      synths,
      nextIndex: 0,
      type: params.type,
      pitch: params.pitch
    };
    this.drumPools.set(key, pool);
    return pool;
  }
  /**
   * Get the next available synth from a drum pool (round-robin)
   */
  getNextDrumSynth(pool) {
    const synth = pool.synths[pool.nextIndex];
    pool.nextIndex = (pool.nextIndex + 1) % pool.synths.length;
    return synth;
  }
  /**
   * Schedule the timeline for playback
   */
  scheduleTimeline() {
    if (!this.timeline) return;
    const notes = getAllNotes(this.timeline);
    const transport = getTransport();
    transport.bpm.value = this.timeline.settings.tempo;
    for (const note of notes) {
      const drumInfo = parseDrumPitch(note.pitch);
      if (drumInfo) {
        const pool = this.getOrCreateDrumPool(drumInfo.drumName, drumInfo.kitName);
        if (!pool) continue;
        const drumSynth = this.getNextDrumSynth(pool);
        const eventId = transport.schedule((time) => {
          this.callbacks.onInstrumentPlay?.(note.instrument);
          const actualTime = time + (note.timingOffset ? note.timingOffset / 1e3 : 0);
          if (note.probability !== void 0 && Math.random() > note.probability) {
            return;
          }
          if (drumSynth.type === "membrane") {
            drumSynth.synth.triggerAttackRelease(
              drumSynth.pitch || "C2",
              note.durationSeconds,
              actualTime,
              note.velocity
            );
          } else if (drumSynth.type === "noise") {
            drumSynth.synth.triggerAttackRelease(
              note.durationSeconds,
              actualTime,
              note.velocity
            );
          } else if (drumSynth.type === "metal") {
            drumSynth.synth.triggerAttackRelease(
              note.durationSeconds,
              actualTime,
              note.velocity
            );
          }
        }, note.timeSeconds);
        this.scheduledEvents.push(eventId);
      } else {
        const instrument = this.instruments.get(note.instrument);
        if (!instrument) continue;
        const eventId = transport.schedule((time) => {
          this.callbacks.onInstrumentPlay?.(note.instrument);
          const actualTime = time + (note.timingOffset ? note.timingOffset / 1e3 : 0);
          if (note.probability !== void 0 && Math.random() > note.probability) {
            return;
          }
          const synth = instrument.synth;
          if (instrument.synthType === "noise") {
            synth.triggerAttackRelease(
              note.durationSeconds,
              actualTime,
              note.velocity
            );
          } else if (instrument.synthType === "membrane") {
            synth.triggerAttackRelease(
              instrument.defaultPitch || note.pitch,
              note.durationSeconds,
              actualTime,
              note.velocity
            );
          } else if ("triggerAttackRelease" in synth) {
            synth.triggerAttackRelease(
              note.pitch,
              note.durationSeconds,
              actualTime,
              note.velocity
            );
          }
        }, note.timeSeconds);
        this.scheduledEvents.push(eventId);
      }
    }
  }
  /**
   * Start progress tracking
   */
  startProgressTracking() {
    this.stopProgressTracking();
    this.progressInterval = setInterval(() => {
      if (!this.timeline) return;
      const currentTime = getTransport().seconds;
      const totalTime = this.timeline.totalSeconds;
      this.callbacks.onProgress?.(currentTime, totalTime);
      const newSectionIndex = this.sections.findIndex(
        (s) => currentTime >= s.startTime && currentTime < s.endTime
      );
      if (newSectionIndex !== -1 && newSectionIndex !== this.currentSectionIndex) {
        this.currentSectionIndex = newSectionIndex;
        this.callbacks.onSectionChange?.(this.sections[newSectionIndex].name);
      }
      if (currentTime >= totalTime) {
        this.stop();
      }
    }, 50);
  }
  /**
   * Stop progress tracking
   */
  stopProgressTracking() {
    if (this.progressInterval) {
      clearInterval(this.progressInterval);
      this.progressInterval = null;
    }
  }
  /**
   * Start playback
   */
  async play() {
    if (!this.timeline) {
      throw new Error("No composition loaded");
    }
    if (this.state === "playing") {
      return;
    }
    await start();
    if (this.state === "paused") {
      getTransport().start();
    } else {
      this.scheduleTimeline();
      getTransport().start();
    }
    this.state = "playing";
    this.callbacks.onStateChange?.(this.state);
    this.startProgressTracking();
  }
  /**
   * Release all active notes to prevent crackling
   */
  releaseAllNotes() {
    for (const { synth } of this.instruments.values()) {
      if ("releaseAll" in synth) {
        synth.releaseAll();
      }
    }
    for (const pool of this.drumPools.values()) {
      for (const { synth, type } of pool.synths) {
        if (type === "membrane") {
          try {
            synth.triggerRelease();
          } catch {
          }
        } else if (type === "noise") {
          try {
            synth.triggerRelease();
          } catch {
          }
        } else if (type === "metal") {
          try {
            synth.triggerRelease();
          } catch {
          }
        }
      }
    }
  }
  /**
   * Pause playback
   */
  pause() {
    if (this.state !== "playing") return;
    getTransport().pause();
    this.releaseAllNotes();
    this.state = "paused";
    this.callbacks.onStateChange?.(this.state);
    this.stopProgressTracking();
  }
  /**
   * Stop playback
   */
  stop() {
    getTransport().stop();
    getTransport().cancel();
    this.releaseAllNotes();
    this.scheduledEvents = [];
    this.state = "stopped";
    this.currentSectionIndex = -1;
    this.callbacks.onStateChange?.(this.state);
    this.stopProgressTracking();
  }
  /**
   * Seek to a position in seconds
   */
  seek(seconds) {
    const wasPlaying = this.state === "playing";
    this.stop();
    if (this.timeline) {
      this.scheduleTimeline();
      getTransport().seconds = seconds;
      if (wasPlaying) {
        getTransport().start();
        this.state = "playing";
        this.callbacks.onStateChange?.(this.state);
        this.startProgressTracking();
      }
    }
  }
  /**
   * Get current playback position in seconds
   */
  getPosition() {
    return getTransport().seconds;
  }
  /**
   * Get total duration in seconds
   */
  getDuration() {
    return this.timeline?.totalSeconds ?? 0;
  }
  /**
   * Get current state
   */
  getState() {
    return this.state;
  }
  /**
   * Get the loaded score
   */
  getScore() {
    return this.score;
  }
  /**
   * Get the compiled timeline
   */
  getTimeline() {
    return this.timeline;
  }
  /**
   * Get list of instruments in the current composition
   */
  getInstruments() {
    return this.timeline?.instruments ?? [];
  }
  /**
   * Get sections info
   */
  getSections() {
    return this.sections;
  }
  /**
   * Set playback rate (0.5 = half speed, 2.0 = double speed)
   * This adjusts the tempo proportionally while preserving pitch
   * @param rate Playback rate multiplier (0.25 to 4.0)
   */
  setPlaybackRate(rate) {
    if (!this.timeline) return;
    const clampedRate = Math.max(0.25, Math.min(4, rate));
    const baseTempo = this.timeline.settings.tempo;
    getTransport().bpm.value = baseTempo * clampedRate;
  }
  /**
   * Get current playback rate
   */
  getPlaybackRate() {
    if (!this.timeline) return 1;
    const currentBpm = getTransport().bpm.value;
    const baseTempo = this.timeline.settings.tempo;
    return currentBpm / baseTempo;
  }
  /**
   * Get the base tempo from the loaded composition
   */
  getBaseTempo() {
    return this.timeline?.settings.tempo ?? 120;
  }
  /**
   * Dispose all resources
   */
  dispose() {
    this.stop();
    for (const { synth, effects, channel } of this.instruments.values()) {
      synth.dispose();
      effects.forEach((e) => e.dispose());
      channel.dispose();
    }
    this.instruments.clear();
    for (const pool of this.drumPools.values()) {
      for (const { synth, volume } of pool.synths) {
        synth.dispose();
        volume.dispose();
      }
    }
    this.drumPools.clear();
    if (this.analyser) {
      this.analyser.dispose();
      this.analyser = null;
    }
    this.score = null;
    this.timeline = null;
    this.sections = [];
    this.currentSectionIndex = -1;
  }
  /**
   * Full cleanup including master chain (call when completely done with player)
   */
  destroyCompletely() {
    this.dispose();
    this.masterVolume.dispose();
    this.masterLimiter.dispose();
    this.masterCompressor.dispose();
  }
  /**
   * Render to WAV file
   */
  async renderToWav() {
    if (!this.timeline) {
      throw new Error("No composition loaded");
    }
    const duration = this.timeline.totalSeconds + 2;
    const buffer = await Offline(async ({ transport }) => {
      const offlineInstruments = /* @__PURE__ */ new Map();
      const offlineDrumPools = /* @__PURE__ */ new Map();
      const samplerPromises = [];
      for (const name of this.timeline.instruments) {
        const def = this.score?.instruments?.[name];
        const presetName = def?.preset || "synth";
        if (presetName.startsWith("drums:")) continue;
        const presetDef = getPresetDefinition(presetName);
        const synthType = presetDef?.type || "polysynth";
        const defaultPitch = presetDef?.base?.pitch;
        if (synthType === "sampler" && presetDef?.base) {
          const promise = createSamplerAsync(presetDef.base).then((sampler) => {
            sampler.toDestination();
            offlineInstruments.set(name, { synth: sampler, synthType, defaultPitch });
          });
          samplerPromises.push(promise);
        } else {
          const synth = createInstrument(def);
          synth.toDestination();
          offlineInstruments.set(name, { synth, synthType, defaultPitch });
        }
      }
      if (samplerPromises.length > 0) {
        console.log(`Waiting for ${samplerPromises.length} sampler(s) to load...`);
        await Promise.all(samplerPromises);
        console.log("All samplers loaded for offline rendering");
      }
      const getOfflineDrumPool = (drumName, kitName) => {
        const normalizedDrumName = normalizeDrumName(drumName);
        const key = `${normalizedDrumName}@${kitName}`;
        if (offlineDrumPools.has(key)) {
          return offlineDrumPools.get(key);
        }
        const kit = DRUM_KITS[kitName];
        if (!kit) return null;
        const params = kit.drums[normalizedDrumName];
        if (!params) return null;
        const synths = [];
        for (let i = 0; i < DRUM_POOL_SIZE; i++) {
          const synth = createDrumSynth(params);
          const volume = new Volume(params.volume ?? 0);
          synth.connect(volume);
          volume.toDestination();
          synths.push({ synth, volume, type: params.type, pitch: params.pitch });
        }
        const pool = {
          synths,
          nextIndex: 0,
          type: params.type,
          pitch: params.pitch
        };
        offlineDrumPools.set(key, pool);
        return pool;
      };
      transport.bpm.value = this.timeline.settings.tempo;
      const notes = getAllNotes(this.timeline);
      for (const note of notes) {
        const drumInfo = parseDrumPitch(note.pitch);
        if (drumInfo) {
          const pool = getOfflineDrumPool(drumInfo.drumName, drumInfo.kitName);
          if (!pool) continue;
          const drumSynth = pool.synths[pool.nextIndex];
          pool.nextIndex = (pool.nextIndex + 1) % pool.synths.length;
          transport.schedule((time) => {
            if (drumSynth.type === "membrane") {
              drumSynth.synth.triggerAttackRelease(
                drumSynth.pitch || "C2",
                note.durationSeconds,
                time,
                note.velocity
              );
            } else if (drumSynth.type === "noise") {
              drumSynth.synth.triggerAttackRelease(
                note.durationSeconds,
                time,
                note.velocity
              );
            } else if (drumSynth.type === "metal") {
              drumSynth.synth.triggerAttackRelease(
                note.durationSeconds,
                time,
                note.velocity
              );
            }
          }, note.timeSeconds);
        } else {
          const instrumentData = offlineInstruments.get(note.instrument);
          if (instrumentData) {
            const { synth, synthType, defaultPitch } = instrumentData;
            transport.schedule((time) => {
              if (synthType === "noise") {
                synth.triggerAttackRelease(
                  note.durationSeconds,
                  time,
                  note.velocity
                );
              } else if (synthType === "membrane") {
                synth.triggerAttackRelease(
                  defaultPitch || note.pitch,
                  note.durationSeconds,
                  time,
                  note.velocity
                );
              } else if ("triggerAttackRelease" in synth) {
                synth.triggerAttackRelease(note.pitch, note.durationSeconds, time, note.velocity);
              }
            }, note.timeSeconds);
          }
        }
      }
      transport.start();
    }, duration);
    return this.bufferToWav(buffer);
  }
  /**
   * Convert Tone.js buffer to WAV format
   */
  bufferToWav(buffer) {
    const audioBuffer = buffer.get();
    if (!audioBuffer) {
      throw new Error("Failed to get audio buffer");
    }
    const numChannels = audioBuffer.numberOfChannels;
    const sampleRate = audioBuffer.sampleRate;
    const length = audioBuffer.length;
    const bytesPerSample = 2;
    const blockAlign = numChannels * bytesPerSample;
    const byteRate = sampleRate * blockAlign;
    const dataSize = length * blockAlign;
    const headerSize = 44;
    const arrayBuffer = new ArrayBuffer(headerSize + dataSize);
    const view = new DataView(arrayBuffer);
    this.writeString(view, 0, "RIFF");
    view.setUint32(4, 36 + dataSize, true);
    this.writeString(view, 8, "WAVE");
    this.writeString(view, 12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, byteRate, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, 16, true);
    this.writeString(view, 36, "data");
    view.setUint32(40, dataSize, true);
    const channels = [];
    for (let i = 0; i < numChannels; i++) {
      channels.push(audioBuffer.getChannelData(i));
    }
    let offset = 44;
    for (let i = 0; i < length; i++) {
      for (let ch = 0; ch < numChannels; ch++) {
        const sample = Math.max(-1, Math.min(1, channels[ch][i]));
        const int16 = sample < 0 ? sample * 32768 : sample * 32767;
        view.setInt16(offset, int16, true);
        offset += 2;
      }
    }
    return arrayBuffer;
  }
  writeString(view, offset, str) {
    for (let i = 0; i < str.length; i++) {
      view.setUint8(offset + i, str.charCodeAt(i));
    }
  }
};
function createPlayer() {
  return new Player2();
}

// src/browser/index.ts
var VERSION = "0.50.0";
export {
  ARPEGGIATOR,
  ARTICULATION,
  AUDIO,
  CHORD_INTERVALS2 as CHORD_INTERVALS,
  DEFAULT_SETTINGS,
  DOTTED_MULTIPLIER,
  DRUM_KITS,
  DRUM_SEQUENCER,
  DURATIONS,
  DURATION_MAP,
  EFFECT_DEFAULTS,
  ENVELOPE_PRESETS,
  GROOVE_TEMPLATES,
  HUMANIZE,
  KIT_808,
  KIT_909,
  KIT_ACOUSTIC,
  KIT_LOFI,
  MIDI,
  NOTE_NAMES,
  NOTE_VALUES,
  PORTAMENTO,
  Player2 as Player,
  SCALE_INTERVALS,
  TimelineBuilder,
  VELOCITY_ENVELOPE,
  VERSION,
  analyze,
  analyzeChordFunction,
  applyGroove,
  applySwing,
  augmentPattern,
  beatsToSeconds,
  buildChord,
  calculateDensity,
  compile,
  createPlayer,
  createSimpleScore,
  expandDrumPattern,
  expandEuclidean,
  expandPattern,
  filterByInstrument,
  generateAutomationEvents,
  generateEuclidean,
  getAllNotes,
  getArticulationModifiers,
  getAutomationValue,
  getAvailableDrums,
  getAvailableGrooves,
  getAvailableKits,
  getAvailableQualities,
  getAvailableScales,
  getBarPosition,
  getChordIntervals2 as getChordIntervals,
  getChordNotes,
  getDiatonicChord,
  getDrumParams,
  getParallelKey,
  getProgression,
  getRelativeKey,
  getScaleDegree,
  getScaleIntervals,
  getScaleNotes,
  getSupportedChordQualities,
  getVoicing,
  humanizeDuration,
  humanizeTiming,
  humanizeVelocity,
  invertChord,
  invertPattern,
  isCommentKey,
  isNoteInScale,
  isRest,
  mergeTimelines,
  midiToPitch,
  offsetTimeline,
  parseAutomationPath,
  parseChord,
  parseChords,
  parseDuration,
  parseKey,
  parseNote,
  parseNotes,
  parseRest,
  parseRhythmPattern,
  parseStepPattern,
  parseTimeSignature,
  parseWithComments,
  patternToSteps,
  pitchToMidi,
  quantizeNotes,
  resolveAutomations,
  resolvePattern,
  resolveSection,
  resolveSemanticParam,
  resolveTrack,
  retrogradePattern,
  rotatePattern,
  shiftOctave,
  snapToScale,
  stripComments,
  transposeNotes,
  transposePattern,
  transposePitch,
  validateScore
};
/*! Bundled license information:

tone/build/esm/core/Tone.js:
  (**
   * Tone.js
   * @author Yotam Mann
   * @license http://opensource.org/licenses/MIT MIT License
   * @copyright 2014-2024 Yotam Mann
   *)
*/
